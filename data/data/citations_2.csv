marker,article,previous,current,next,link
"(Halliday, 1978",P96-1026,3 Linguistic Framework: Systemic Functional Linguistics,"Our analysis was carried out within the framework of Systemic-Functional Linguistics (sFL) (Halliday, 1978; Halliday, 1985) which views language as a resource for the creation of meaning.",SFL stratifies meaning into context and language.,http://www.aclweb.org/anthology/P/P96/P96-1026.pdf
"(Carroll, 1994",P96-1026,"Furthermore, we know that Macintosh documentation undergoes thorough local quality control.","It certainly conforms to the principles of good documentation established by current research on technical documentation and on the needs of end-users, e.g., (Carroll, 1994; Hammond, 1994), in that it supplies clear and concise information for the task at hand.","Finally, we have been assured by French users of the software that they consider this particular manual to be well written and to bear no unnatural trace of its origins.",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf
"(Martin, 1992)",P96-1026,"In such cases, we must appeal to another source of control over the apparently available choices.","We have looked to the construct of genre (Martin, 1992) to provide this additional control, on two grounds: (1) since genres are distinguished by their communicative purposes, we can view each of the functional sections already identified as a distinct genre; (2) genre is presented as controlling text structure and realisation.","In Martin's view, genre is defined as a staged, goal-oriented social process realised through register, the context of situation, which in turn is realised in language to achieve the goals of a text.",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf
"(McKeown, 1985",P96-1026,"It will also allow us to determine the text structures appropriate in each genre, a study we are currently undertaking.","This is consistent with other accounts of text structure for text generation in technical domains, e.g., (McKeown, 1985; Paris, 1993; Kittredge et al., 1991).","For those cases where the realisation remains under-determined by the task element type, we conducted a finer-grained analysis, by overlaying a genre partition on the undifferentiated data.",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf
"(Kosseim and Lapalme, 1994",P96-1026,9 Related Work,"The results from our linguistic analysis are consistent with other research on sublanguages in the instructions domain, in both French and English, e.g., (Kosseim and Lapalme, 1994; Paris and Scott, 1994).",Our analysis goes beyond previous work by identifying within the discourse context the means for exercising explicit control over a text generator.,http://www.aclweb.org/anthology/P/P96/P96-1026.pdf
"(Paris and Scott, 1994)",P96-1026,Our analysis goes beyond previous work by identifying within the discourse context the means for exercising explicit control over a text generator.,"An interesting difference with respect to previous descriptions is the use of the true (or direct) imperative to express an action in the procedure genre, as results from (Paris and Scott, 1994) seem to indicate that the infinitive-form of the imperative is preferred in French.","These results, however, were obtained from a corpus of instructions mostly for domestic appliances as opposed to software manuals.",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf
"(Kocourek, 1982)",P96-1026,"These results, however, were obtained from a corpus of instructions mostly for domestic appliances as opposed to software manuals.","Furthermore the use of the infinitive-form in instructions in general as observed by (Kocourek, 1982) is declining, as some of the conventions already common in English technical writing are being adopted by French technical writers, e.g., (Timbal-Duclaux, 1990).","We also note that the patterns of realisations uncovered in our analysis follow the principle of good technical writing practice known as the minimalist approach, e.g., (Carroll, 1994; Hammond, 1994).",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf
"(Biber, 1995)",P96-1026,2 Methodology,"The methodology we employed is similar to that endorsed by (Biber, 1995).",It is summarised as follows:,http://www.aclweb.org/anthology/P/P96/P96-1026.pdf
"(Vapnik, 1998)",W06-2204,"Rather than relying on redundancy of the fragments, TPLEX exploits redundancy of the learned extraction patterns.","TPLEX is a transductive algorithm (Vapnik, 1998), in that the goal is to perform extraction from a given unlabelled corpus, given a labelled corpus.","This is in contrast to the typical machine learning framework, where the goal is a set of extraction patterns (which can of course then be applied to new unlabelled text).",http://www.aclweb.org/anthology/W/W06/W06-2204.pdf
"Alche Buc et al., 2002)",W06-2204,It remains to be seen whether this approach would be effective for information extraction.,"Another possibility is to explore semi-supervised extensions to boosting (d'Alche Buc et al., 2002).","Boosting is a highly effective ensemble learning technique, and BWI uses boosting to tune the weights of the learned patterns, so if we generalize boosting to handle unlabelled data, then the learned weights may well be more effective than those calculated by TPLEX.",http://www.aclweb.org/anthology/W/W06/W06-2204.pdf
"(Freitag and Kushmerick, 2000)",W06-2204,"This strategy has previously been employed successfully (Freitag and Kushmerick, 2000; Ciravegna, 2001; Yangarber et al., 2002; Finn and Kushmerick, 2004).","TPLEX's boundary detectors are similar to those learned by BWI (Freitag and Kushmerick, 2000).","A boundary detector has two parts, a left pattern and a right pattern.",http://www.aclweb.org/anthology/W/W06/W06-2204.pdf
"(McCallum, 2002)",W06-2204,The results for ELIE were generated by the current implementation [http://smi.ucd.ie/aidan/Software.html].,"For the CRF results, we used MALLET's SimpleTagger (McCallum, 2002), with each token encoded with a set of binary features (one for each observed literal, as well as the eight token generalizations).",Our results in Fig.,http://www.aclweb.org/anthology/W/W06/W06-2204.pdf
(2012),W15-2134,2 Related work,Schwartz et al (2012) is a systematic study of how representation choices in dependency annotation schemes affect their learnability for parsing.,"The choice points investigated, much like in the current paper, relate to the issue of headedness.",http://www.aclweb.org/anthology/W/W15/W15-2134.pdf
"(Nivre et al., 2007)",W15-2134,"For all parsers and in most experiments (which explore several pipelines with different POS-tagging strategies), SD is easier to label (i.e., label accuracy scores are higher) and CoNLL is easier to structure (i.e., unlabeled attachment scores are higher).","In terms of LAS, MaltParser (Nivre et al., 2007) performs best of the 3 parsers with SD, and MSTParser (McDonald et al., 2006) performs best with CoNLL.","In Nilsson et al (2006), the authors investigate the effects of two types of input transformation on the performance of MaltParser.",http://www.aclweb.org/anthology/W/W15/W15-2134.pdf
"(Nivre et al., 2015)",W15-2134,The result is the first human-checked largescale gold standard for syntactic dependency annotation of English text.,"The first version annotated with the UD representation was released in 2015 (Nivre et al., 2015)1.",1http://universaldependencies.github.io/docs/,http://www.aclweb.org/anthology/W/W15/W15-2134.pdf
(2012),W15-2134,"Here, since the main concern is the design of a parsing representation that is meant simply as an intermediary step, all output has to be evaluated against the same gold standard.",This creates an opportunity for losses that did not exist in the experiments of Schwartz et al (2012).,7 Conclusion,http://www.aclweb.org/anthology/W/W15/W15-2134.pdf
"(Zhu et al., 2014)",S15-2109,"However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning.","Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons.","In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015).",http://www.aclweb.org/anthology/S/S15/S15-2109.pdf
"(Rumelhart et al., 1985)",S15-2109,"The parameters of subspace model in Equation 2, S and C were estimated to minimize the negative log-likelihood of the correct class.","Training employed conventional Stochastic Gradient Descent (Rumelhart et al., 1985) with mini-batch size 1 and random uniform initialization similar to (Glorot and Bengio, 2010).","After some initial experiments, it was determined that a learning rate of 0.01 and selecting the model with the best accuracy on the 20% set after 8 iterations led to the best results.",http://www.aclweb.org/anthology/S/S15/S15-2109.pdf
"(Owoputi et al., 2013)",S15-2109,"The embedding matrix E was trained in unsupervised fashion using the structured skip-gram model, described in Section 2.","We used the corpus of 52 million tweets used in (Owoputi et al., 2013) with the tokenizer described in the same work.",The words that occurred less than 40 times in the data were discarded from the vocabulary.,http://www.aclweb.org/anthology/S/S15/S15-2109.pdf
"(Goldberg and Levy, 2014)",S15-2109,The words that occurred less than 40 times in the data were discarded from the vocabulary.,"To train the model, we used a negative sampling rate of 25 words, sampled from a multinomial of unigram word probabilities over all the vocabulary (Goldberg and Levy, 2014).","Embeddings of 50, 200, 400 and 600 dimensions were trained.",http://www.aclweb.org/anthology/S/S15/S15-2109.pdf
"(Mandl and Womser-Hacker, 2005, ",E09-1091,Named Entities (NEs) play a critical role in many Natural Language Processing and Information Retrieval (IR) tasks.,"In Cross-Language Information Retrieval (CLIR) systems, they play an even more important role as the accuracy of their transliterations is shown to correlate highly with the performance of the CLIR systems (Mandl and Womser-Hacker, 2005, Xu and Weischedel, 2005).",Traditional methods for transliterations have not proven to be very effective in CLIR.,http://www.aclweb.org/anthology/E/E09/E09-1091.pdf
"(Klementiev and Roth, 2006)",E09-1091,"We demonstrate MINT's effectiveness on 4 language pairs involving 5 languages (English, Hindi, Kannada, Russian, and Tamil) from 3 different language families, and its scalability on corpora of vastly different sizes (2,000 to 200,000 articles).","We show that MINT's performance is significantly better than a state of the art method (Klementiev and Roth, 2006).","Proceedings of the 12th Conference of the European Chapter of the ACL, pages 799-807, Athens, Greece, 30 March - 3 April 2009. c2009 Association for Computational Linguistics",http://www.aclweb.org/anthology/E/E09/E09-1091.pdf
"(Klementiev and Roth, 2006",E09-1091,"The ubiquitous availability of comparable news corpora in multiple languages suggests a promising alternative to Machine Transliteration, namely, the mining of Named Entity Transliteration Equivalents (NETEs) from such corpora.","News stories are typically rich in NEs and therefore, comparable news corpora can be expected to contain NETEs (Klementiev and Roth, 2006; Tao et al., 2006).","The large quantity and the perpetual availability of news corpora in many of the world's languages, make mining of NETEs a viable alternative to traditional approaches.",http://www.aclweb.org/anthology/E/E09/E09-1091.pdf
"(Pirkola et al., 2003)",E09-1091,"The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman & Oard, 2002; Mandl & Womser-hacker, 2005; Xu &Weischedel, 2005).","In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996).","While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs.",http://www.aclweb.org/anthology/E/E09/E09-1091.pdf
"(Klementiev and Roth, 2006)",E09-1091,"In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences.","NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006), and extended for NETE mining for several languages in (Saravanan and Kumaran, 2007).","However, such methods miss vast majority of the NETEs due to their dependency on frequency signatures.",http://www.aclweb.org/anthology/E/E09/E09-1091.pdf
"(Tao et al., 2006)",E09-1091,"In addition, (Klementiev and Roth, 2006) may not scale for large corpora, as they examine every word in the target side as a potential transliteration equivalent.","NETE mining from comparable corpora using phonetic mappings was proposed in (Tao et al., 2006), but the need for language specific knowledge restricts its applicability across languages.","We proposed the idea of mining NETEs from multilingual articles with similar content in (Udupa, et al., 2008).",http://www.aclweb.org/anthology/E/E09/E09-1091.pdf
(2009),N12-1038,"Tasks requiring a uniform theme in a segment might tolerate false positives, while tasks requiring complete ideas or complete themes might accept false negatives.","Georgescul et al (2009) note that while WindowDiff technically penalizes false positives and false negatives equally, false positives are in fact more likely; a false positive error occurs anywhere were there are more computed boundaries than boundaries in the reference, while a false negative error can only occur when a boundary is missed.","Consider figure 1, only 3 of the 8 windows contain a boundary; only those 3 windows may have false negatives (a missed boundary), while all other windows may contain false positives (too many boundaries).",http://www.aclweb.org/anthology/N/N12/N12-1038.pdf
"(Lafferty et al., 2001)",P06-1009,This paper presents an alternative discriminative method for word alignment.,"We use a conditional random field (CRF) sequence model, which allows for globally optimal training and decoding (Lafferty et al., 2001).",The inference algo,http://www.aclweb.org/anthology/P/P06/P06-1009.pdf
"(Malouf, 2002",P06-1009,"While the log-likelihood cannot be maximised for the parameters, A, in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters.","We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003).",Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters.,http://www.aclweb.org/anthology/P/P06/P06-1009.pdf
"(Och and Ney, 2003)",P06-1009,"Furthermore, the model allows regularisation using a prior over the parameters, a very effective and simple method for limiting over-fitting.","We use a similar graphical structure to the directed hidden Markov model (HMM) from GIZA++ (Och and Ney, 2003).","This models one-to-many alignments, where each target word is aligned with zero or more source words.",http://www.aclweb.org/anthology/P/P06/P06-1009.pdf
(2003),P06-1009,"In order to produce many-to-many alignments we combine the outputs of two models, one for each translation direction.",We use the refined method from Och and Ney (2003) which starts from the intersection of the two models' predictions and 'grows' the predicted alignments to neighbouring alignments which only appear in the output of one of the models.,4 Experiments,http://www.aclweb.org/anthology/P/P06/P06-1009.pdf
"(Mihalcea and Pedersen, 2003)",P06-1009,"We used the original 37 sentence trial set for feature used these as our training and test sets, respectively.","For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003).","For this task we have used the same test data as the competition entrants, and therefore can directly compare our results.",http://www.aclweb.org/anthology/P/P06/P06-1009.pdf
(2005),P06-1009,"Unlike the unsupervised entrants in the 2003 task, we require word-aligned training data, and therefore must cannibalise the test set for this purpose.",We follow Taskar et al (2005) by using the first 100 test sentences for training and the remaining 347 for testing.,"This means that our results should not be directly compared to those entrants, other than in an approximate manner.",http://www.aclweb.org/anthology/P/P06/P06-1009.pdf
(1966),J98-3009,"251-263) is actually an exercise in metaevaluation, for the object of her evaluation is not any (group of) MT systems, but rather the criteria in terms of which MT systems are commonly evaluated.","After reviewing those that were employed by the Automatic Language Processing Advisory Committee (1966) and those that were designed in the early 1990s on behalf of ARPA, a major U.S. funding agency, the author concludes that translation quality cannot be defined in the abstract, mainly because there is no such thing as ""the correct translation,"" and pleads for an evaluation that explicitly takes into account the specific purpose for which a translation has been made.","Now that I have gone through the individual papers, the obvious question is how much they contribute to the overall goal of the book, i.e., to help build a bridge between MT and translation theory.",http://www.aclweb.org/anthology/J/J98/J98-3009.pdf
(2008),P15-2128,The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language.,"Banea et al (2008) follow this approach, translating an annotated corpus via MT.",Balamurali et al (2012) use linked Wordnets to,http://www.aclweb.org/anthology/P/P15/P15-2128.pdf
"(Hall et al., 2009)",P15-2128,bels.,"We performed the experiments with the weka toolkit (Hall et al., 2009), using a filter to convert strings into word vectors, and two learning algorithms: SVMs and bagging with Fast Decision Tree Learner as base algorithm.",Figure 4 represents the experiments conducted with the EN test set.,http://www.aclweb.org/anthology/P/P15/P15-2128.pdf
(2014),P15-2128,"In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012).","However, Balahur and Turchi (2014) conclude that MT systems can be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English.",All this work was performed at sentence or document level.,http://www.aclweb.org/anthology/P/P15/P15-2128.pdf
"(Koehn et al., 2007)",P15-2128,"That is, the text between the relevant segment boundaries is not reordered nor mixed with the text outside these boundaries.3 Thus the text in the target language segment comes only from the corresponding source language segment.","We use the Moses statistical MT (SMT) toolkit (Koehn et al., 2007) to perform the translation.","In Moses, these reordering constraints are implemented with the zone and wall tags, as indicated in Figure 3.",http://www.aclweb.org/anthology/P/P15/P15-2128.pdf
"(Koehn, 2010)",P15-2128,The LM was an interpolation of LMs trained with the target part of the parallel corpora and with the rest of the Booking and Trip Advisor data (last 2 rows of Table 2).,"We used Moses Experiment Management System (Koehn, 2010) with all default options to build the SMT system.7 Because the common crawl corpus contained English sentences in the Spanish side, we applied an LM-based filter to select only sentence pairs in which the Spanish side was better scored by the Spanish LM than with the English LM, and conversely for the English side.",We conducted supervised sentiment classification experiments for settings a and b of use case I (see Section 2).,http://www.aclweb.org/anthology/P/P15/P15-2128.pdf
"(Moens and Steedman, 1988",W97-0318,1 Introduction,"The ability to distinguish states, e.g., ""Mark seems happy,"" from events, e.g., ""Renee ran down the street,"" is a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse (Moens and Steedman, 1988; Dorr, 1992; Klavans, 1994).","Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause.",http://www.aclweb.org/anthology/W/W97/W97-0318.pdf
"(Moens and Steedman, 1988",W97-0318,"Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause.","Aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation (Moens and Steedman, 1988; Passonneau, 1988; Dorr, 1992; Klavans, 1994).","Researchers have used empirical analysis of corpora to develop linguistically-based numerical indicators that aid in aspectual classification (Klavans and Chodorow, 1992; Siegel and McKeown, 1996).",http://www.aclweb.org/anthology/W/W97/W97-0318.pdf
"(Pereira, Tishby, and Lee, 1993",W97-0318,Table 4: Breakdown of verb occurrences.,"and for the automatic identification of semantically related groups of words (Pereira, Tishby, and Lee, 1993; Hatzivassiloglou and McKeown, 1993).","For more detail on the machine learning experiments described here, see Siegel (1997).",http://www.aclweb.org/anthology/W/W97/W97-0318.pdf
"(Duda and Hart, 1973)",W97-0318,2This test was suggested by Judith Klavans (personal communication).,"3Similar baselines for comparison have been used for many classification problems (Duda and Hart, 1973), e.g., part-of-speech tagging (Church, 1988; Allen, 1995).",159,http://www.aclweb.org/anthology/W/W97/W97-0318.pdf
"(Halliday and Hasan, 1976)",P11-2022,1 Introduction,"A well-written document is coherent (Halliday and Hasan, 1976)- it structures information so that each new piece of information is interpretable given the preceding context.","Models that distinguish coherent from incoherent documents are widely used in generation, summarization and text evaluation.",http://www.aclweb.org/anthology/P/P11/P11-2022.pdf
"(McIntyre and Lapata, 2010)",P11-2022,"Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks.","In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010).","It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models.",http://www.aclweb.org/anthology/P/P11/P11-2022.pdf
(2005),P11-2022,We follow the -COREFERENCE setting from Barzilay and Lapata (2005) and perform heuristic coreference resolution by linking mentions which share a head noun.,"Although some versions of the grid use an automatic coreference resolver, this often fails to improve results; in Barzilay and Lapata (2005), coreference improves results in only one of their target domains, and actually hurts for readability prediction.","Their results, moreover, rely on running coreference on the document in its original order; in a summarization task, the correct order is not known, which will cause even more resolver errors.",http://www.aclweb.org/anthology/P/P11/P11-2022.pdf
"(Barzilay and Lapata, 2005)",P11-2022,"We test our model on two experimental tasks, both testing its ability to distinguish between correct and incorrect orderings for wSi articles.","In document discrimination (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences, scoring the system correct if it prefers the original ordering4.","We also evaluate on the more difficult task of sentence insertion (Chen et al., 2007; Elsner and Charniak, 2008).",http://www.aclweb.org/anthology/P/P11/P11-2022.pdf
"(Elsner and Charniak, 2010)",P11-2022,"This enables the model to pick up premodifiers in phrases like ""a Bush spokesman"", which do not head NPs in the Penn Treebank.","Finding these is also necessary to maximize coreference recall (Elsner and Charniak, 2010).",We give nonhead mentions the role X.,http://www.aclweb.org/anthology/P/P11/P11-2022.pdf
"(Andrew and Gao, 2007)",P11-2022,The standard entity grid estimates that such an entity will be the subject of the next sentence with a probability of about,"7We train the regressor using OWLQN (Andrew and Gao, 2007), modified and distributed by Mark Johnson as part of the Charniak-Johnson parse reranker (Charniak and Johnson, 2005).",127,http://www.aclweb.org/anthology/P/P11/P11-2022.pdf
(2008),P11-2022,This model is significantly better than the standard grid on discrimination (84% versus 80%) and has a higher mean score on insertion (24% versus 21%)8.,"The best WSJ results in previous work are those of Elsner and Charniak (2008), who combine the entity grid with models based on pronoun coreference and discourse-new NP detection.",We report their scores in the table.,http://www.aclweb.org/anthology/P/P11/P11-2022.pdf
(2005),P11-2022,"To construct a grid, we must first decide which textual units are to be considered ""entities"", and how the different mentions of an entity are to be linked.",We follow the -COREFERENCE setting from Barzilay and Lapata (2005) and perform heuristic coreference resolution by linking mentions which share a head noun.,"Although some versions of the grid use an automatic coreference resolver, this often fails to improve results; in Barzilay and Lapata (2005), coreference improves results in only one of their target domains, and actually hurts for readability prediction.",http://www.aclweb.org/anthology/P/P11/P11-2022.pdf
[1992],J96-1003,Spelling correction is an important application for error-tolerant recognition.,There has been substantial work on spelling correction (see the excellent review by Kukich [1992]).,"All methods essentially enumerate plausible candidates that resemble the incorrect word, and use additional heuristics to rank the results.'",http://www.aclweb.org/anthology/J/J96/J96-1003.pdf
(1992),J96-1003,"For edit distance thresholds 1, 2, and 3, we selected 1,000 words at random from each word list and perturbed them by random insertions, deletions, replacements, and transpositions, so that each misspelled word had the required edit distance from the correct form.","Kukich (1992), citing a number of studies, reports that typically 80% of misspelled words contain a single error of one of the unit operations, although",84,http://www.aclweb.org/anthology/J/J96/J96-1003.pdf
"(Cortes and Vapnik, 1995)",D09-1160,"However, 'explicit' feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier.","Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995).","The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008).",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf
(2005),D09-1160,"E1-regularized log-linear models (E1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007).","However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification.","In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy.",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf
(2003),D09-1160,"1) is O(|xd|), which is linear with respect to the number of active features in xd within the expanded feature space Fd.","Heuristic Kernel Expansion (SVM-HKE) To make the weight vector sparse, Kudo and Matsumoto (2003) proposed a heuristic method that filters out less useful features whose absolute weight values are less than a pre-defined threshold .2 They reported that increased threshold value  resulted in a dramatically sparse feature space Fd, which had the side-effects of accuracy degradation and classifier speed-up.",3 Proposed Method,http://www.aclweb.org/anthology/D/D09/D09-1160.pdf
"(Kudo and Matsumoto, 2003)",D09-1160,"Without fstries, little speed-up of SVM-HKE classifiers versus the SVM-KE classifiers (in Table 3) was obtained due to the mild reduction in the average number of active features |xd |in the classification.","This result conforms to the results reported in (Kudo and Matsumoto, 2003).","The parsing speed reached 14,937 sentences per second with accuracy of 90.91% (SVM-HKE, d = 3, Q = 0.002).",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf
"(Ganchev and Dredze, 2008)",D09-1160,"To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs.","When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage.","We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al (2009)).",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf
"(Kudo and Matsumoto, 2002",D09-1160,"Due to space limitations, we omit the details of the parsing algorithm.","We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1).","Note that features listed in the 'Between bunsetsus' row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector.",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf
"(Kurohashi and Nagao, 2003)",D09-1160,This task is therefore a better measure of our method than simple sequential labeling such as POS tagging or named-entity recognition.,"For evaluation, we used Kyoto Text Corpus Version 4.0 (Kurohashi and Nagao, 2003), Mainichi news articles in 1995 that have been manually annotated with dependency relations.6 The training, development, and test sets included 24,283, 4833, and 9284 sentences, and 234,685, 47,571, and 89,874 bunsetsus, respectively.","The training samples generated from the training set included 150,064 positive and 146,712 negative samples.",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf
"(Sakaki et al., 2010)",D13-1114,More work has been done in the latter category: analysis of social phenomena in a non-English context.,"A well-known study evaluated usage of Twitter in the aftermath of the 2010 earthquake in Japan (Sakaki et al., 2010).","Another Japanese-oriented study evaluated the impact of television on tweeted content (Akioka et al., 2010).",http://www.aclweb.org/anthology/D/D13/D13-1114.pdf
"(Mislove et al., 2011)",D13-1114,"(Rao et al., 2010; Pennacchiotti and Popescu, 2011; Zamal et al., 2012), the dominant way of obtaining datasets consisting of Twitter users with highconfidence gender-labels is to use gender-name associations.","The use of name-gender associations are problematic when non-English content is considered because databases of anglophone name-gender associations are no longer useful (Mislove et al., 2011).","We instead used Amazon Mechanical Turk workers to identify the gender of the person shown in the profile picture associated with a user's account (Liu and Ruths, 2013).",http://www.aclweb.org/anthology/D/D13/D13-1114.pdf
"(Zamal et al., 2012)",D13-1114,"We followed prior work in this regard, particularly since our intent here is to evaluate the relevance of existing gender inference machinery on other languages.","For the present study, we adopted an SVM-based classifier, described in (Zamal et al., 2012), that incorporated nearly all features used in prior work and showed comparable (and sometimes better) accuracy than other methods.",Parameter values and kernel choices for the SVM are discussed in the source paper.,http://www.aclweb.org/anthology/D/D13/D13-1114.pdf
"(Liu and Ruths, 2013)",D13-1114,Each dataset consisted of approximately 1000 users who tweeted primarily in a given language.,"We used Amazon Mechanical Turk to manually label each user with their gender, using a language-agnostic labeling strategy (Liu and Ruths, 2013).","For classification, we employed a performant support vector machine-based (SVM) technique that has been used in a range of studies, e.g.",http://www.aclweb.org/anthology/D/D13/D13-1114.pdf
"(Moilanen and Pulman, 2008",W15-2911,Dragut et al (2012) examined inconsistency across lexicons.,"Negation and its scope has been studied extensively (Moilanen and Pulman, 2008; Pang and Lee, 2004; Choi and Cardie, 2009).","Polar words can even carry an opposite sentiment in a new domain (Blitzer et al., 2007; Andreevskaia and Bergler, 2006; Schwartz et al., 2013; Wilson et al., 2005).",http://www.aclweb.org/anthology/W/W15/W15-2911.pdf
(2006),W15-2911,"Taboada et al (2011) presented a polarity lexicon with negation words and intensifiers, which they refer to as contextual valence shifters (Polanyi and Zaenen, 2006).","Research by Kennedy and Inkpen (2006) dealt with negation and intensity by creating a discrete modifier scale, namely, the occurrence of good might be either good, not good, intensified good, or diminished good.",A similar approach was taken by Steinberger et al (2012).,http://www.aclweb.org/anthology/W/W15/W15-2911.pdf
"(Go et al., 2009)",W15-2911,"We compute the LMI over a corpus of positive, respectively negative tweets, in order to obtain positive (LMIpos) and negative (LMIneg) bigram scores.","We combine the following freely available data, leading to a large corpus of positive and negative tweets: - 1.6 million automatically labeled tweets from the Sentiment140 data set (Go et al., 2009), collected by searching for positive and negative emoticons;",1An online demo illustrating the score values and distributional term similarities in this Twitter space can be found at the LT website http://maggie.lt.informatik.,http://www.aclweb.org/anthology/W/W15/W15-2911.pdf
"(Hagen et al., 2000)",W02-1024,Keyword-based search engines have been one of the most highly utilized internet tools in recent years.,"Nevertheless, search performance remains unsatisfactory at most e-commerce sites (Hagen et al., 2000).","Librarians and search professionals have traditionally favored Boolean keyword search systems, which, when successful, return a small set of relevant hits.",http://www.aclweb.org/anthology/W/W02/W02-1024.pdf
"(Bierner, 2001)",W02-1024,"User questions are compared against those in the database, and links to webpages for the closest matches are returned.","Similar to our approach, Electric Knowledge transforms a natural language question into a series of increasingly more general keyword queries (Bierner, 2001).","However, their query formulation process utilizes hard-crafted regular expressions, while we adopt a more general machine learning approach for transformation rule application.",http://www.aclweb.org/anthology/W/W02/W02-1024.pdf
"(Watkins, 1989",W02-1024,4.1 Q Learning,"We adopted the Q learning paradigm (Watkins, 1989; Mitchell, 1997) to model our problem as a set of possible states, S, and a set of actions, A, which can be performed to alter the current state.","While in state s E S and performing action a E A, the learner receives a reward r(s, a), and advances to state s0 = (s, a).",http://www.aclweb.org/anthology/W/W02/W02-1024.pdf
"(Misra and Walker, 2013)",W14-2715,The annotations for stance were gathered using Amazon's Mechanical Turk service with an interface that allowed annotators to see complete discussions.,"Quotes provide additional context that were used by human annotators in a separate task for annotating agreement and disagreement (Misra and Walker, 2013).",Responses can be labeled as either PRO or CON toward the topic.,http://www.aclweb.org/anthology/W/W14/W14-2715.pdf
"(Bach et al., 2013)",W14-2715,"Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion.","We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data.","We evaluate our model on data from the 4FORUMS online debate site (Walker et al., 2012b).",http://www.aclweb.org/anthology/W/W14/W14-2715.pdf
[0],W14-2715,"Like other statistical relational learning methods, dependencies in the domain are captured by constructing rules with weights that can be learned from data.","But unlike other statistical relational learning methods, PSL relaxes boolean truth values for atoms in the domain to soft truth values in the interval [0,1].","In this setting, finding the mostprobable explanation (MPE), a joint assignment of truth values to all random variable ground atoms, can be done efficiently.",http://www.aclweb.org/anthology/W/W14/W14-2715.pdf
[0],W14-2715,"For disagreement/agreement annotations in the interval [-5, 5], we consider values [-5,0] as evidence for the disagreesAuth relation and values [1, 5] as evidence for the agreesAuth relation.","We discard observations with annotations in the interval [0,1] because it indicates a very weak signal of agreement, which is already rare on debate sites.",We populate disagreesPost and agreesPost in the same way as described above.,http://www.aclweb.org/anthology/W/W14/W14-2715.pdf
"(Somasundaran and Wiebe, 2009)",W14-2715,We predict that this difference makes it more difficult to achieve accuracies as high for 4FORUMS discussions as can be achieved for the congressional debates corpus.,"Work by (Somasundaran and Wiebe, 2009) on idealogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics.","Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker's stance in a corpus of congressional floor debates.",http://www.aclweb.org/anthology/W/W14/W14-2715.pdf
"(Bach et al., 2013)",W14-2715,We identify both linguistic features of the posts and features that capture the underlying relationships between posts and users.,"We use probabilistic soft logic (PSL) (Bach et al., 2013) to model post stance by leveraging both these local linguistic features as well as the observed network structure of the posts to reason over the dataset.","We evaluate our approach on 4FORUMS (Walker et al., 2012b), a collection of discussions from an online debate site on issues ranging from gun control to gay marriage.",http://www.aclweb.org/anthology/W/W14/W14-2715.pdf
"(Mohammad and Pedersen, 2004)",W04-0839,"We believe this is due to the low training data per task ratio, which usually means that the weak indicators (P and P ) are likely to be overwhelmed by idiosyncrasies of the data.","(Mohammad and Pedersen, 2004) show results to the same conclusions for SENSEVAL-1 and SENSEVAL-2 data that have similar low training data per task, while, the line, hard, serve and interest data which have much larger training data per task are shown to benefit from a larger context.",Duluth-ELSS (a sister system of SyntaLex) achieves an accuracy of 61.7%.,http://www.aclweb.org/anthology/W/W04/W04-0839.pdf
"(Collins, 2001a",P06-2052,2.2 Algorithm to calculate similarity,"Collins et al (Collins, 2001a; Collins, 2001b) proposed an efficient method to calculate Tree Kernel by using C(n1, n2) as follows."," If the productions at n1 and n2 are different C(n1, n2) = 0  If the productions at n1 and n2 are the same, and n1 and n2 are pre-terminals, then C(n1, n2) = 1  Else if the productions at n1 and n2 are the same and n1 and n2 are not pre-terminals,",http://www.aclweb.org/anthology/P/P06/P06-2052.pdf
"(Sagae and Tsujii, 2008",S14-2080,"We explore two kinds of basic models: One is transition-based, and the other is tree approximation.","Transition-based models are widely used for dependency tree parsing, and they can be adapted to graph parsing (Sagae and Tsujii, 2008; Titov et al., 2009).","Here we implement 5 transitionbased models for dependency graph parsing, each of which is based on different transition system.",http://www.aclweb.org/anthology/S/S14/S14-2080.pdf
"(Sagae, 2007)",S14-2080,3.3 Sentence Reversal,"Reversing the order the words of a given sentence is a simple way to yield heterogeneous parsing models, thus improving parsing accuracy of the model ensemble (Sagae, 2007).","In our experiments, one transition system produces two models, one trained on the normal corpus, and the other on the corpus of reversed sentences.",http://www.aclweb.org/anthology/S/S14/S14-2080.pdf
"(Fowler and Penn, 2010",S14-2080,"In our work, we use a practical idea to indirectly profile the graph-based parsing techniques for dependency graph parsing.","Inspired by the PCFG approximation idea (Fowler and Penn, 2010; Zhang and Krieger, 2011) for deep parsing, we study tree approximation approaches for graph spanning.",This tree approximation technique can be applied to both transition-based and graph-based parsers.,http://www.aclweb.org/anthology/S/S14/S14-2080.pdf
[2],S14-2080,"We have 19 heterogeneous basic models (10 transition-based models, 9 tree approximation models), and use a simple voter to combine their outputs.",noun ARG1 verb ARG1 verb ARG2 Mrs Ward was relieved root noun ARG1-R root verb ARG2 verb ARG1 adj ARG1;[2]verb ARG1 verb ARG2 Mrs Ward was relieved root noun ARG1-R,462,http://www.aclweb.org/anthology/S/S14/S14-2080.pdf
"(1985, 1986)",J88-3004,We have left the automatic creation of this taxonomy from advisor experiences to future research.,"Initial work in recognizing and indexing abstract configurations of planning relations is discussed in Dolan and Dyer (1985, 1986).",9.3 OTHER CLASSES OF MISCONCEPTIONS,http://www.aclweb.org/anthology/J/J88/J88-3004.pdf
"(Christiansen et al., 1998",Q14-1008,"tion of its effectiveness in adult speech processing (Cutler et al., 1986).","Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and ""algebraic"" (as opposed to ""statistical"") approaches (Christiansen et al., 1998; Yang, 2004; Lignos and Yang, 2010; Lignos, 2011; Lignos, 2012).","Bayesian models of word segmentation (Brent, 1999; Goldwater, 2007), however, have until recently completely ignored stress.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
"(Brent, 1999",Q14-1008,"Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and ""algebraic"" (as opposed to ""statistical"") approaches (Christiansen et al., 1998; Yang, 2004; Lignos and Yang, 2010; Lignos, 2011; Lignos, 2012).","Bayesian models of word segmentation (Brent, 1999; Goldwater, 2007), however, have until recently completely ignored stress.","The sole exception in this respect is Doyle and Levy (2013) who added stress cues to the Bigram model (Goldwater et al., 2009), demonstrating that this leads to an improvement in segmentation performance.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
"(Jusczyk et al., 1999a)",Q14-1008,"Overall, we find that stress cues add roughly 6% token f-score to a model that does not account for phonotactics and 4% to a model that already incorporates phonotactics.","Relatedly and in line with the finding that stress cues are used by infants before phonotactic cues (Jusczyk et al., 1999a), we observe that phonotactic cues require more input than stress cues to be used efficiently.",A closer look at the knowledge acquired by our models shows that the Unique Stress Constraint of Yang (2004) can be acquired jointly with segmenting the input instead,http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
(2004),Q14-1008,"Relatedly and in line with the finding that stress cues are used by infants before phonotactic cues (Jusczyk et al., 1999a), we observe that phonotactic cues require more input than stress cues to be used efficiently.",A closer look at the knowledge acquired by our models shows that the Unique Stress Constraint of Yang (2004) can be acquired jointly with segmenting the input instead,93,http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
(1987),Q14-1008,"Lexical stress is the ""accentuation of syllables within words"" (Cutler, 2005) and has long been argued to play an important role in adult word recognition.","Following Cutler and Carter (1987)'s observation that stressed syllables tend to occur at the beginnings of words in English, Jusczyk et al (1993) investigated whether infants acquiring English take advantage of this fact.","Their study demonstrated that this is indeed the case for 9 month olds, although they found no indication of using stressed syllables as cues for word boundaries in 6 month olds.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
(2004),Q14-1008,"They only reported a word-token f-score of 44% (roughly, segmentation accuracy: see Section 4), which is considerably below the performance of subsequent models, making a direct comparison complicated.",Yang (2004) introduced a simple incremental algorithm that relies on stress by embodying a Unique Stress Constraint (USC) that allows at most a single stressed syllable per word.,"On pre-syllabified child directed speech, he reported a word token fscore of 85.6% for a non-statistical algorithm that exploits the USC.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
"(2010, 2011, 2012)",Q14-1008,"While the USC has been argued to be near-to-universal and follows from the ""culminative function of stress"" (Fromkin, 2001; Cutler, 2005), the high score Yang reported crucially depends on every word token carrying stress, including function words.","More recently, Lignos (2010, 2011, 2012) further explored Yang's original algorithm, taking into account that function words should not be assumed to possess lexical stress cues.","While his scores are in line with those reported by Yang, the importance of stress for this learner were more modest, providing a gain of around 2.5% (Lignos, 2011).",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
(2013),Q14-1008,This allows the model to acquire knowledge about the stress patterns of its input by assigning different probabilities to the different stress-templates.,"However, Doyle and Levy (2013) do not directly examine the probabilities assigned to the stress-templates; they only report that their model does slightly prefer stress-initial words over the baseline model by calculating the fraction of stress-initial word types in the output segmentations of their models.","They also demonstrate that stress cues do indeed aid segmentation, although their reported gain of 1% in token f-score is even smaller than that reported by Lignos (2011).",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
(2009),Q14-1008,"We give an intuitive description of the mathematical background of Adaptor Grammars in 3.1, referring the reader to Johnson et al (2007) for technical details.","The models we examine are derived from the collocational model of Johnson and Goldwater (2009) by varying three parameters, resulting in 6 models: two baselines that do not take advantage of stress cues and either do or do not use phonotactics, as described in Section 3.2; and four stress models that differ with respect to the use of phonotactics, and as to whether they embody the Unique Stress Constraint introduced by Yang (2004).",We describe these models in section 3.3.,http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
"(Jusczyk et al., 1993)",Q14-1008,"In order for stress cues to be helpful, the model must have some way of associating the position of stress with word-boundaries.","Intuitively, the reason stress helps infants in segmenting English is that a stressed syllable is a reliable indicator of the beginning of a word (Jusczyk et al., 1993).","More generally, if there is a (reasonably) reliable relationship between the position of stressed syllables and beginnings (or",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
(2013),Q14-1008,This (partly) captures the tendency of English for stress-initial words and thus provide an additional cue for identifying words; and it is exactly the kind of preference infant learners of English seem to acquire (Jusczyk,"3This is, in essence, also the strategy chosen by Doyle and Levy (2013).",96,http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
"(Yang, 2004)",Q14-1008,This yields the colloc3-phon-stress model.,"We can also add the Unique Stress Constraint (USC) (Yang, 2004) by excluding all variants of rule (18) that generate two or more stressed syllables.","For example, while the lexical generator for the colloc3-nophon-stress model will include the rule Word  SSyll SSyll, the lexical generator embodying the USC lacks this rule.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
"(Bernstein-Ratner, 1987",Q14-1008,"As function words account for roughly 50% of the tokens but only roughly 5% of the types in our corpora, this means that the type and token distribution of stress patterns differs dramatically in all our corpora, as can be seen from Table 2.","We also added stress information to the BrentBernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent, 1999), following the procedure just outlined.",This corpus is a de-facto standard for evaluat,http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
(2013),Q14-1008,"For example, languages such as French lack lexical stress; it would be interesting to know whether in such a case, phonotactic (or other) cues are more important.","Relatedly, recent work such as Borschinger et al (2013) has found that artificially created data often masks the complexity exhibited by real speech.","This suggests that future work should use data directly derived from the acoustic signal to account for contextual effects, rather than using dictionary look-up or other heuristics.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
(1998),Q14-1008,4.1 Corpora and corpus creation,"Following Christiansen et al (1998) and Doyle and Levy (2013), we use the Korman corpus (Korman, 1984) as one of our corpora.","It comprises childdirected speech for very young infants, aged between 6 and 16 weeks and, like all other corpora used in this paper, is available through the CHILDES database (MacWhinney, 2000).",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
"(Demuth et al., 2006",Q14-1008,"ing models of Bayesian word segmentation (Brent, 1999; Goldwater, 2007; Goldwater et al., 2009; Johnson and Goldwater, 2009), comprising in total 9790 utterances.","As our third corpus, we use the Alex portion of the Providence corpus (Demuth et al., 2006; Borschinger et al., 2012).",A major benefit of the Providence corpus is that the video-recordings from which the transcripts were produced are available through CHILDES alongside the transcripts.,http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
(2009),Q14-1008,"Alternatively, one could limit the models ability to capture word-to-word dependencies by removing rules (1) to (3).",This results 2We follow Johnson and Goldwater (2009) in limiting the length of possible words to four syllables to speed up runtime.,"In pilot experiments, this choice did not have a noticeable effect on segmentation performance.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
(1998),Q14-1008,"Our version of the Korman corpus contains, in total, 11413 utterances.","Unlike Christiansen et al (1998), Yang (2004), and Doyle and Levy (2013), we follow Lignos and Yang (2010) in making the more realistic assumption that the 94 mono-syllabic function words listed by Selkirk (1984) never surface with lexical stress.","As function words account for roughly 50% of the tokens but only roughly 5% of the types in our corpora, this means that the type and token distribution of stress patterns differs dramatically in all our corpora, as can be seen from Table 2.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf
[1949],J11-2002,"If a language has suffixing and/or prefixing-sometimes called concatenative morphology-it obviously follows that text words in that language can be segmented into a sequence of morphological elements: a stem and a number of suffixes after the stem and/or prefixes before the stem.1 Morphology is one of the oldest linguistic subdisciplines, and this brief presentation by necessity omits many intricacies and greatly simplifies a vast scholarship.","(For standard, in-depth, introductions to this fascinating field, see, e.g., Nida [1949], Jensen [1990], Spencer and Zwicky [1998], or Haspelmath [2002].)","In language technology applications, a morphological component forms a bridge between texts and structured information about the vocabulary of a language.",http://www.aclweb.org/anthology/J/J11/J11-2002.pdf
(1965b),J11-2002,"The initial suffix candidate, -oj, has a high functional load and consequently a high text frequency: It is the most ambiguous of the Russian adjective suffixes, appearing in four different slots in the adjective paradigm, and is also homonymous with a noun suffix.","In Andreev (1965b) the method is tested extensively on Russian, which is the subject of several papers in the volume, and a number of other languages: Albanian (Persikov 1965), Armenian (Melkumjan 1965), Bulgarian (Fedulova 1965), Czech (Ozigova 1965), English (Malahovskij 1965), Estonian (Hol'm 1965), French (Kordi 1965), German (Fitialova 1965), Hausa (Fihman 1965a), Hungarian (Andreev 1965a), Latvian (Jakubajtis 1965), Serbo-Croatian (Panina 1965), Swahili (Fihman 1965b), Ukrainian (Eliseeva 1965), and Vietnamese (Jakuseva 1965).","As an aside, we may note that only after the turn of the millennium are we again seeing this variety of languages in ULM work.",http://www.aclweb.org/anthology/J/J11/J11-2002.pdf
"a, 2005b, ",J11-2002,"Harris 1955, 1968, 1970 C T English Segmentation Andreev 1965a, Andreev 1967, C T Vietnamese to Segmentation Chapter 2, Persikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ozigova 1965; Malahovskij 1965; Hol'm 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuseva 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71-93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; Brent, Murthy, C T English/Child- Segmentation and Lundberg 1995; Snover 2002; English/Polish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1997; Kazakov and C T French/English Segmentation Manandhar 1998, 2001 Jacquemin 1997 C T English Segmentation Cromm 1997 C T German Segmentation Gaussier 1999 C T French/English (I) Lexicon+ Paradigms Dejean 1998a, 1998b C T Turkish/English/ Affix Lists Korean/French/ Swahili/ Vietnamese (I) Medina Urrea 2000, 2003, 2006b C T Spanish Affix List Schone and Jurafsky 2000, 2001a; C T English Segmentation Schone 2001 Goldsmith 2000, 2001, 2006; Belkin C T English (I) Lexicon+ Paradigms and Goldsmith 2002; Goldsmith, Higgins, and Soglasnova 2001; Hu et al.","2005b; Xanthos, Hu, and Goldsmith 2006 Baroni 2000, 2003 C T Child-English/ Affix List English Cho and Han 2002 C T Korean Segmentation Sharma, Kalita, and Das 2002, 2003; C T Assamese Lexicon+ Paradigms Sharma and Das 2002 Baroni, Matiasek, and Trost 2002 C/NC T English/German (I) Related word pairs Bati 2002 C/NC T Amharic Lexicon+ Paradigms Creutz 2003, 2006; Creutz and C T Finnish/Turkish/ Segmentation Lagus 2002, 2004, 2005a, 2005b, English 2005c, 2007; Creutz, Lagus, and Virpioja 2005; Hirsimaki et al.",2003; Creutz et al.,http://www.aclweb.org/anthology/J/J11/J11-2002.pdf
"a, 2002b",J11-2002,2005; Hu et al.,"2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al.","2004 Oliver 2004, Chapter 4-5 C T Catalan Paradigms Bordag 2005a, 2005b, 2007, 2008 C T English/German Segmentation Hammarstrom 2005, 2006a, 2006b, C - Maori to Warlpiri Same-stem 2007b, 2009a, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation ur Rehman and Hussain 2005 C T Finnish/Turkish/English Segmentation Jordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101-139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al.",http://www.aclweb.org/anthology/J/J11/J11-2002.pdf
(2009),J11-2002,"Xanthos (2007), on the other hand, starts out only by assuming that there exists a distinction between root and pattern graphemes and subsequently learns which graphemes are which.",See Goldsmith and Xanthos (2009) for an excellent survey on how to do this (something which falls under learning phonological categories rather than morphology learning).,"Basically, it is possible only because there are systematic combination constraints between different phonemes (approximated by graphemes); for example, vowels and consonants alternate in a very non-random manner.",http://www.aclweb.org/anthology/J/J11/J11-2002.pdf
(2007),J11-2002,The recent increased interest in Bayesian generative models in general in NLP may possibly serve as a catalyst.,"In the group-and-abstract paradigm, working with feature sets of a word, as in De Pauw and Wagacha (2007), is an ingenious generalization that holds numerous advantages over string edit distances.","Feature set comparisons are naturally defined over arbitrary collections, whereas string edit distances work on pairs of strings.",http://www.aclweb.org/anthology/J/J11/J11-2002.pdf
"Berry and Jessup, 1999)",P00-1072,"Now, we suggest 21n other words, the projection into the reduced space is chosen such that the representations in the original space are changed as little as possible when measured by the sum of the squares of the differences.","One can prove that Ak is the best approximation to A for any unitray invariant norm(Michael W. Berry and Jessup, 1999) three variant probability estimation methods in dimension-reduced space.",First is estimating p(y Ix) by computing distance between given word x and predicting word y in reduced space.,http://www.aclweb.org/anthology/P/P00/P00-1072.pdf
"(Dagan et al., 1999)",P00-1072,(k = 3) (dim = 2) (6 = 0.1) (0 = 0) P(swig I coffee) 0 0.09 0 0.1627 0.0756 0.11 P(sip I coffee) 1 1 1 0.2425 0.5536 1 P(drink I coffee) 0 0.18 0.17 0.2359 0.1932 0.28 P(devour I coffee) 0 0.18 0.11 0.1271 0.0726 0.11 P(eat I coffee) 0 0.18 0.11 0.1159 0.0526 0 P(swallow I coffee) 0 0.18 0.11 0.1159 0.0526 0 2.3.3 Method 3: Dimension-reduced similarity-based method,"The similarity-based method(Dagan et al., 1999) and dimension reduction technique can be merged into one model 3 .",Reduced dimension can be better representation space than the original space for finding similarities between words.,http://www.aclweb.org/anthology/P/P00/P00-1072.pdf
"(Dagan et al., 1999)",P00-1072,"Each method is presented with a noun and two verbs, deciding which verb is more likely to have the noun as a direct object.","Data preparation method and error counting scheme are almost similar to that of similarity-based methods (Dagan et al., 1999) (Lee and Pereira, 1999).","Performance is measured by the error rate, defined as error rate = T-1(0 of incorrect choices) where T is the size of test set.",http://www.aclweb.org/anthology/P/P00/P00-1072.pdf
"(Lee and Pereira, 1999)",P00-1072,"Pd(Yilxi) frq(xi, yi) > 0 cx(xi) P(yi) frq(xi,yi) = 0","For similarity-based method, the parameter tuning is important to improve performance but we use the simplified unweighted average equation as in (Lee and Pereira, 1999) 6 .","Since this equation is the same as our estimation method in Section 2.3.3, we can say that the comparison is fair.",http://www.aclweb.org/anthology/P/P00/P00-1072.pdf
"Donnell et al., 2001)",W11-2820,Adaptive interfaces change the style and content of interaction according to the context of use.,"In particular, adaptive hypertext (O'Donnell et al., 2001) adapts the content and form of natural language text.",Systems like this introduce the need for a good model of the context and how it influences language.,http://www.aclweb.org/anthology/W/W11/W11-2820.pdf
"(Brickley and Miller, 2010)",W11-2820,The user's social context.,"In the VRE, the link between the social network and digital artifacts is established formally, by the integration of the FOAF social networking vocabulary (Brickley and Miller, 2010) with our provenance ontologies.","FOAF characterises an individual and their social network by defining a vocabulary describing people, the links between them and the things they create and do.",http://www.aclweb.org/anthology/W/W11/W11-2820.pdf
(2010),W11-2820,"In order to generate the text, we have implemented a RESTful service that invokes a Text Generator service based on the RDF ID of the resource being described, passed as a parameter by the Web interface.","This service generates text containing a description of the resource using a deep model of the syntactic structure of sentences and their combinations, inspired by the work of Hielkema (2010).",VRE User Interface User Core Services Social Networking Metadata Repository Policy Policy Manager Text Interface,http://www.aclweb.org/anthology/W/W11/W11-2820.pdf
(2010),W11-2820,"Using such a strategy, the Policy Manager would be able to determine how to prioritise conflicting policies applying to a particular resource.","To determine if two policies may conflict, we plan to use a conflict detection mechanism similar to the one proposed by Sensoy et al (2010).","Moreover regarding usability, we need to implement a system that would allow users to easily create SPIN rules representing their policies, possibly using a NLG interface.",http://www.aclweb.org/anthology/W/W11/W11-2820.pdf
"(Clark and Lavie, 2010)",W11-2143,"Decoding was carried out in Joshua (Li et al., 2009), an open-source framework for parsing-based MT.","We managed our experiments with LoonyBin (Clark and Lavie, 2010), an open-source tool for defining, modifying, and running complex experimental pipelines.",We describe our system-building process in more detail in Section 2.,http://www.aclweb.org/anthology/W/W11/W11-2143.pdf
"(Stolcke, 2002)",W11-2143,The final prepared corpus was made up of approximately 1.8 billion words of running text.,"We built a 5-gram language model from it with the SRI language modeling toolkit (Stolcke, 2002).","To match the treatment given to the training data, the language model was also built in mixed case.",http://www.aclweb.org/anthology/W/W11/W11-2143.pdf
(2010),W11-2143,"Our BLEU score improvements of 1.2 to 1.9 points are statistically significant according to the paired bootstrap resampling method (Koehn, 2004) with n = 1000 and p < 0.01.",They are also larger than the 0.7- to 1.1-point gains reported by Pino et al (2010) when the full Giga-FrEn was added.,"The 2011 system also shows a significant reduction in the out-of-vocabulary (OOV) rate on both test sets: 38% and 47% fewer OOV types, and 44% and 45% fewer OOV tokens, when compared to the 2010 system.",http://www.aclweb.org/anthology/W/W11/W11-2143.pdf
"(Hanneman et al., 2010)",W11-2143,This is our fourth yearly submission to the WMT shared translation task.,"In design and construction, the system is similar to our submission from last year's workshop (Hanneman et al., 2010), with changes in the methods we employed for training data selection and SCFG filtering.","Continuing WMT's general trend, we worked with more data than in previous years, basing our 2011 system on 13.9 million sentences of parallel French-English training data and an English language model of 1.8 billion words.",http://www.aclweb.org/anthology/W/W11/W11-2143.pdf
"(Petrov and Klein, 2007)",W11-2143,"Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the grow-diag-final-and heuristic (Koehn et al., 2005).","For generating parse trees, we used the French and English grammars of the Berkeley statistical parser (Petrov and Klein, 2007).","Except for minor bug fixes, our method for extracting and scoring a translation grammar remains the same as in our WMT 2010 submission.",http://www.aclweb.org/anthology/W/W11/W11-2143.pdf
"(McCallum, 2002)",P10-2063,"The classifier is used only to get solutions for the openclass words, although we wish to give the classifier all the words for the sentence.","The cross-product of the stem name and (open-class) reduced core POS tags, plus the CLOSED tag, yields 24 labels for a CRF classifier in Mallet (McCallum, 2002).",4 Experiments and Evaluation,http://www.aclweb.org/anthology/P/P10/P10-2063.pdf
"(Diab et al., 2007",P10-2063,"1SAMA-v3.1 is an updated version of BAMA, with many significant differences in analysis.","In contrast, other approaches have used a pipelined approach, with separate models to first do tokenization and then part-of-speech tagging (Diab et al., 2007; Diab, 2009).","While these approaches have somewhat lower performance than the joint approach, they have the advantage that they do not rely on the presence of a full-blown morphological analyzer, which may not always be available or appropriate as the data shifts to different genres or Arabic dialects.",http://www.aclweb.org/anthology/P/P10/P10-2063.pdf
"(Roth et al., 2008)",P10-2063,"However, both (Habash and Rambow, 2005; Diab et al., 2007) assume gold tokenization for evaluation of POS results, which we do not.","The ""MorphPOS"" task in (Roth et al., 2008), 96.4%, is somewhat similar to ours in that it scores on a ""core tag"", but unlike for us there is only one such tag for a source token (easier) but it distinguishes between NOUN and ADJ (harder).",We would like to do a direct comparison by simply runing the above systems on the exact same data and evaluating them the same way.,http://www.aclweb.org/anthology/P/P10/P10-2063.pdf
"(Moore and Quirk, 2008)",W12-3159,"Recent efforts extended MERT to work on lattices (Macherey et al., 2008) and hypergraphs (Kumar et al., 2009).","Random restarts and random walks (Moore and Quirk, 2008) are commonly used to combat the fact the search space is highly non-convex, often with multiple minima.","Several problems still remain with MERT, three of which are addressed by this work.",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf
"(Chang and Collins, 2011)",W12-3159,"First, the Nbest error surface explored by MERT is generally not the same as the true error surface, which means that the error rate at an optimum1 of the N-best error surface is not guaranteed to be any close to an optimum of the true error surface.","Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an error-prone decoder differs from the one of an exact decoder (Chang and Collins, 2011).","MERT calculates an envelope from candidate translations and assumes all translations on the envelope are reachable by the decoder, but these translations may become unreachable due to search errors.",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf
"(Helder and Mead, 1965)",W12-3159,"which takes source sentences and a set of weights as inputs, and outputs the evaluation score (e.g., BLEU score) computed on the decoded sentences.","Since it is impractical to calculate derivatives of this function, we use derivative-free optimization methods such as the downhill simplex method (Helder and Mead, 1965) and Powell's method (Powell, 1964), which generally handle such difficult search conditions relatively well.",This approach confers several benefits over MERT.,http://www.aclweb.org/anthology/W/W12/W12-3159.pdf
"(Moore and Lee, 1994)",W12-3159,"In this paper, we make direct search reasonably fast thanks to two speedup techniques.","First, we use a model selection acceleration technique called racing (Moore and Lee, 1994) in conjunction with randomization tests (Riezler and Maxwell, 2005) to avoid decoding the entire development set at each function evaluation.",This approach discards the current model whenever performance on the translated subset of the development data is deemed significantly worse in comparison to the current best model.,http://www.aclweb.org/anthology/W/W12/W12-3159.pdf
"(Maron and Moore, 1994",W12-3159,"Prior to considering the SMT case, we review one of these methods in the case of leave-one-out cross validation (LOOCV).","Racing for model selection (Maron and Moore, 1994; Moore and Lee, 1994) works as follows: we are given a collection of N,,t models and Nd data points, and we must find the  model that minimizes the mean e j = 1 i ej(i),",Nd,http://www.aclweb.org/anthology/W/W12/W12-3159.pdf
(1994),W12-3159,"The models are evaluated concurrently, and at any given step k E [1, Nd], each model Mj is associated with two pieces of information: the current estimate of its mean error rate, and the estimate of its variance.","As evaluations progress, we eliminate any model that is significantly worse than any other model.3 We also note that the Racing technique first randomizes the order of the data points to ensure that prefixes of the dataset are gen3The details of these statistical tests are not so important here since we use different ones in the case of SMT, but we briefly summarize them as follows: Maron and Moore (1994) use a non-parametric method (Hoeffding bounds (Hoeffding, 1963)) for confidence estimation, and places confidence intervals on the mean value of the random variable representing ej(i).",A model is discarded if its confidence interval no longer overlaps with the confidence interval of the current best model.,http://www.aclweb.org/anthology/W/W12/W12-3159.pdf
"(Och et al., 1999)",W12-3159,5.2 Lattice-based decoding,"We use another technique to speed up direct search by storing and re-using search graphs, which consist of lattices in the case of phrase-based decoding (Och et al., 1999) and hypergraphs in the case of hierarchical decoding (Chiang, 2005).","The successive expansion of translation options in order to construct the search graph is generally done from scratch, but this can be wasteful when the same sentences are translated multiple times, as it is the case with direct search.",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf
"(Koehn et al., 2007)",W12-3159,6.1 Setup,"For our experiments, we use a phrase-based translation system similar to Moses (Koehn et al., 2007).","Our decoder uses many of the same features as Moses, including four phrasal and lexicalized translation scores, phrase penalty, word penalty, a language model score, linear distortion, and six lexicalized reordering scores.",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf
"(Riezler and Maxwell, 2005)",W12-3159,"In SMT, it is common to use either bootstrap resampling (Efron and Tibshirani, 1993; Och, 2003) or randomization tests (Noreen, 1989).","In this paper, we use the randomization test for discarding unpromising models, since this statistical test was shown to be less likely to cause type-I errors6 than bootstrap methods (Riezler and Maxwell, 2005).","Since both kinds of statistical tests involve a time-consuming sampling step, it 4Since Racing only discards suboptimal models, the current best model M* is one for which we have decoded the entire development set.",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf
"(Ren et al., 2014)",D14-1114,"(Tan et al., 2012) encode intent in language models, aware of long-lasting interests.","(Ren et al., 2014) uses an unsupervised heterogeneous clustering.","(Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics.",http://www.aclweb.org/anthology/D/D14/D14-1114.pdf
"(Yamamoto et al., 2012)",D14-1114,"Quite an amount of work leverage query logs (Jiang et al., 2013), including query reformulations (Radlinski et al., 2010), click-through data (Li et al., 2008).","There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013).",The new trend of integrating knowledge graph will be discussed next.,http://www.aclweb.org/anthology/D/D14/D14-1114.pdf
"(Ren et al., 2014)",D14-1114,"(Pantel et al., 2012) models latent intent to mine entity type distributions.","(Ren et al., 2014) utilizes knowledge graph resources in a hetrogeneous view.","(Lin et al., 2012) also pays attention to refiners, but restricted to limited domains, while our method is more general.",http://www.aclweb.org/anthology/D/D14/D14-1114.pdf
"(Yin and Shah, 2010)",D14-1114,"(Ren et al., 2014) uses an unsupervised heterogeneous clustering.","(Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics.","We do not split queries into clusters or subtopics relevant to the original query to indicate a intent, but link them in an graph with intent feature similarity, weakly or strongly, in a holistical view.",http://www.aclweb.org/anthology/D/D14/D14-1114.pdf
"(Indyk and Motwani, 1998)",D14-1114,"To put it in more details, we use jaccard similarity for name shinglings and cosine similarity for domain and topic vector.","As query log induced intent topic graph is of considerable large size, the pair-wise similarity is computationally prohibitive, hence we use Local Sensitive Hash (Indyk and Motwani, 1998) for each similarity metric so as to compute ISim just in candidate set.","We use random hyperplane based hash family proposed in (Charikar, 2002) and set the hash code dimension and hash table numbers empirically to ensure the number of nodes falling into each bucket is relatively stable.",http://www.aclweb.org/anthology/D/D14/D14-1114.pdf
"(Charikar, 2002)",D14-1114,"As query log induced intent topic graph is of considerable large size, the pair-wise similarity is computationally prohibitive, hence we use Local Sensitive Hash (Indyk and Motwani, 1998) for each similarity metric so as to compute ISim just in candidate set.","We use random hyperplane based hash family proposed in (Charikar, 2002) and set the hash code dimension and hash table numbers empirically to ensure the number of nodes falling into each bucket is relatively stable.",3.2 Merging nodes,http://www.aclweb.org/anthology/D/D14/D14-1114.pdf
"(Ferragina and Scaiella, 2012)",D14-1114,We then move on to map queries to Freebase and empirically filter sessions that are less entity-centric.,"We use an annotation tool especially for short text (Ferragina and Scaiella, 2012) called Tagme3 to recognize entities and observe only 16% of all the queries are exactly an entity itself, which means most of queries do have refiner words to convey information need.","To ensure the precision of recognized entities, we set a significant threshold and bottom line threshold , queries should have at least one recognized entity with a likelihood above significant level, and those below bottom line are ignored.",http://www.aclweb.org/anthology/D/D14/D14-1114.pdf
[1979],J99-2001,"By ""local focus,"" we refer to the person, object, property, or concept that a sentence is most centrally about within the discourse context in which it occurs.","The appropriate movement and marking of local focus, and the appropriate choice of the form of a noun phrase (NP) based on local focus information, are considered to contribute to the local coherence exhibited by discourse (Sidner [1979], Grosz, Joshi, and Weinstein [1983, 1995], Carter [1987], and others).","In addition, local focus information is one source of information that is used by readers and hearers for interpreting pronouns.",http://www.aclweb.org/anthology/J/J99/J99-2001.pdf
"(Rabiner and Juang, 1993)",P05-1046,"where P, is the common word distribution, and  is","4While it may be surprising that disallowing reestimation of the transition function is helpful here, the same has been observed in acoustic modeling (Rabiner and Juang, 1993).",375,http://www.aclweb.org/anthology/P/P05/P05-1046.pdf
(2002),P05-1046,There has also been some previous work on unsupervised learning of field segmentation models in particular domains.,Pasula et al (2002) performs limited unsupervised segmentation of bibliographic citations as a small part of a larger probabilistic model of identity uncertainty.,"However, their system does not explicitly learn a field segmentation model for the citations, and encodes a large amount of hand-supplied information about name forms, abbreviation schemes, and so on.",http://www.aclweb.org/anthology/P/P05/P05-1046.pdf
(2001),P05-1046,Because the structure of the HMMs they learn is similar to ours it seems that their system could benefit from the techniques of this paper.,"Finally, Blei and Moreno (2001) use an HMM augmented by an aspect model to automatically segment documents, similar in goal to the system of Hearst (1997), but using techniques more similar to the present work.",7 Conclusions,http://www.aclweb.org/anthology/P/P05/P05-1046.pdf
"(Baker et al., 1998)",W15-1842,"We use hfst-pmatch (Linden et al., 2013), a pattern-matching tool mimicking and extending Xerox fst (Karttunen, 2011), for demonstrating how to develop a semantic frame extractor.","We select a FrameNet (Baker et al., 1998) frame and write shallowly syntactic pattern-matching rules based on part-of-speech information and morphology from either a morphological automaton or tagged text.",1 Introduction,http://www.aclweb.org/anthology/W/W15/W15-1842.pdf
"(Linden et al., 2013)",W15-1842,Abstract,"We use hfst-pmatch (Linden et al., 2013), a pattern-matching tool mimicking and extending Xerox fst (Karttunen, 2011), for demonstrating how to develop a semantic frame extractor.","We select a FrameNet (Baker et al., 1998) frame and write shallowly syntactic pattern-matching rules based on part-of-speech information and morphology from either a morphological automaton or tagged text.",http://www.aclweb.org/anthology/W/W15/W15-1842.pdf
"(Steen et al., 2010)",W15-1402,"In this paper, we present a set of experiments aimed at improving on previous work on the task of supervised word-level detection of linguistic metaphor in running text.","The use of supervised machine learning techniques for metaphor identification has increased manyfold in the recent years (see section 10, Related Work, for a review and references), partially due to the availability of largescale annotated resources for training and evaluating the algorithms, such as the VU Amsterdam corpus (Steen et al., 2010), datasets built as part of a U.S. government-funded initiative to advance the state-of-art in metaphor identification and interpretation (Mohler et al., 2013; Strzalkowski et al., 2013), and recent annotation efforts with other kinds of data (Beigman Klebanov and Flor, 2013; Jang et al., 2014).","Some of these data are publicly available (Steen et al., 2010), allowing for benchmarking and for measuring incremental improvements, which is the approach taken in this paper.",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf
(2014),W15-1402,"Content tokens are nouns, adjectives, adverbs, and verbs.","We start with a baseline set of features and training regime from Beigman Klebanov et al (2014), and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations.","The usage of concreteness features was previously discussed in the literature; to our knowledge, these features have not yet been evaluated for their impact in a comprehensive system for word-level metaphor detection, apart from the concreteness features as used in Beigman Klebanov et al (2014), which we use as a baseline.",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf
"(Blanchard et al., 2013)",W15-1402,"In this study, each content-word token in a text is an instance that is classified as either a metaphor or not a metaphor.","We use the logistic regression classifier as implemented in the SKLL package (Blanchard et al., 2013), which is based on scikitlearn (Pedregosa et al., 2011), with F1 optimization (""metaphor"" class).","Performance will be evaluated using Precision, Recall, and F-1 score, for the positive (""metaphor"") class.",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf
"(Yang et al., 2014",W15-1402,5 Experiment 1: Re-weighting of Examples,"Given that the category distribution is generally heavily skewed towards the non-metaphor category (see Table 1), we experimented with cost-sensitive machine learning techniques to try to correct for the imbalanced class distribution (Yang et al., 2014; Muller et al., 2014).","The first technique uses AutoWeight (as implemented in the auto flag in scikitlearn toolkit), where we assign weights that are inversely proportional to the class frequencies.2 Table 2 shows the results.",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf
"(Dunn, 2014",W15-1402,"The small improvement is perhaps not surprising, since the baseline model itself already contains a version of the concreteness features.","Given the relevant literature that has put forward concreteness and difference in concreteness as important predictors of metaphoricity (Dunn, 2014; Tsvetkov et al., 2014; Gandy et al., 2013; Assaf et al., 2013; Turney et al., 2011), it is instructive to evaluate the overall contribution of the concreteness features over the UPT baseline (no concreteness features), across the different weighting regimes.",Table 9 provides this information.,http://www.aclweb.org/anthology/W/W15/W15-1402.pdf
(2013),W15-1402,"The field of automated identification of metaphor has grown dramatically over the last few years, and there exists a plurality of approaches to the task.",Shutova and Sun (2013) and Shutova et al (2013) explored unsupervised clustering-based approaches.,"Features used in supervised learning approaches include selectional preferences violation, outlier detection, semantic analysis using topical signatures and ontologies, as well as n-gram features, among others (Tsvetkov et al., 2014; Schulder and Hovy, 2014; Beigman Klebanov et al., 2014; Mohler et al., 2013; Dunn, 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Strzalkowski et al., 2013; Bethard et al., 2009; Pasanek and Sculley, 2008).",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf
(2014),W15-1402,"We start with a baseline set of features and training regime from Beigman Klebanov et al (2014), and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations.","The usage of concreteness features was previously discussed in the literature; to our knowledge, these features have not yet been evaluated for their impact in a comprehensive system for word-level metaphor detection, apart from the concreteness features as used in Beigman Klebanov et al (2014), which we use as a baseline.",2 Data,http://www.aclweb.org/anthology/W/W15/W15-1402.pdf
(2014),W15-1402,"The data is annotated according to the MIPVU procedure (Steen et al., 2010) with the interannotator reliability of  > 0.8.","In order to allow for direct comparison with prior work, we used the same subset of these data as Beigman Klebanov et al (2014), in the same crossvalidation setting.","The total of 90 fragments are used in cross-validation: 10-fold on News, 9-fold on Conversation, 11 on Fiction, and 12 on Academic.",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf
(2014),W15-1402,"Performance will be evaluated using Precision, Recall, and F-1 score, for the positive (""metaphor"") class.","As a baseline, we use the best performing feature set from Beigman Klebanov et al (2014), who investigated supervised word-level identification of metaphors.","We investigate the effect of reweighting of examples, as well as the effectiveness of features related to the notion of concreteness.",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf
(2014),W15-1402,4 Baseline System,"As a baseline, we use the best feature set from Beigman Klebanov et al (2014).","Specifically, the baseline contains the following families of features:",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf
(2013),W15-1402,6 Experiment 2: Re-representing concreteness information,"In this paper, we use mean concreteness scores for words as published in the large-scale norming study by Brysbaert et al (2013).","The dataset has a reasonable coverage for our data; thus, 78% of tokens in Set A have a concreteness rating.",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf
"(Ahrenberg et al., 2002)",W06-2717,"For this purpose we have developed a ""TreeAligner"".","This program is a graphical user interface to insert (or correct) alignments between pairs of syntax trees.4 The TreeAligner can be seen in the line of tools such as I*Link (Ahrenberg et al., 2002) or Cairo (Smith and Jahr, 2000) but it is especially tailored to visualize and align full syntax trees.",The TreeAligner requires three input files.,http://www.aclweb.org/anthology/W/W06/W06-2717.pdf
"(Erk and Pado, 2004)",W06-2717,"For example, we are currently experimenting with the annotation of semantic frames on top of the treebanks.","We use the SALSA tool developed at Saarbrucken University (Erk and Pado, 2004) which also assumes TIGER-XML input.",,http://www.aclweb.org/anthology/W/W06/W06-2717.pdf
"(Burstein, 2003",W13-1708,"Forms of text used for assessment include mathematical responses, short answers, essays and spoken responses among others (Williamson et al., 2010).","Standardized tests like GRE and GMAT too use such systems to complement human scorers while evaluating student essays automatically (Burstein, 2003; Rudner et al., 2005).","Zhang (2008) discusses proficiency classification for the Examination for the Certificate of Proficiency in English (ECPE) in detail, by comparing procedures based on four types of measurement models.",http://www.aclweb.org/anthology/W/W13/W13-1708.pdf
(2011),W13-1708,"Although error-rate is a strong indicator of a learner's proficiency in a language, considering other factors like lexical indices or syntactic and morphological complexity would help in providing multiple views about the same data.","Providing a non-error driven model, Crossley et al (2011) studied the impact of various lexical indices in predicting the learner proficiency level.","Using a corpus of 100 writing samples by L2 learners of English classified in to three levels (beginner, intermediate, advanced), they built a classification system that analyses language proficiency using the Coh-metrix5 lexical indices.",http://www.aclweb.org/anthology/W/W13/W13-1708.pdf
(2000),W13-1708,Cascade generalization is the process of sequentially using a set of small classifiers to perform an overall classification task.,Gama and Brazdil (2000) showed that a cascade can outperform other ensemble methods like stacking or boosting.,Kaynak and Alpaydin (2000) proposed a method to sequentially cascade classifiers and showed that this improves the accuracy without increasing the computational complexity and cost.,http://www.aclweb.org/anthology/W/W13/W13-1708.pdf
"(Hall et al., 2009)",W13-1708,"Hence, we further investigated the problem as a collection of multi-stage twoclass cascades instead of a single stage three class classification.","For all our classification experiments, we used the WEKA (Hall et al., 2009) toolkit.",We report the overall classification accuracy as our evaluation metric.,http://www.aclweb.org/anthology/W/W13/W13-1708.pdf
"(Seewald, 2002)",W13-1708,"We trained three classification models on the entire feature set, using the same train-test sets as explained before and trained an ensemble model with three classifiers.","We used the StackingC implementation of WEKA (Seewald, 2002) to combine the models, with a linear regression model as our meta classifier.",Table 4 shows the classification accuracies for the individual classifiers as well as the ensemble on a 10-fold CV of the training set and on the held out test set.,http://www.aclweb.org/anthology/W/W13/W13-1708.pdf
"(Chiang, 2007)",W13-0804,1 Introduction,"Cube pruning (Chiang, 2007) is a widely used search strategy in state-of-the-art hierarchical decoders.","Some alternatives and extensions to the classical algorithm as proposed by David Chiang have been presented in the literature since, e.g.",http://www.aclweb.org/anthology/W/W13/W13-0804.pdf
"(Chiang, 2007)",W13-0804,Hierarchical phrase-based translation (HPBT) was first proposed by Chiang (2005).,"Chiang also introduced the cube pruning algorithm for hierarchical search (Chiang, 2007).",It is basically an adaptation of one of the k-best parsing algorithms by Huang and Chiang (2005).,http://www.aclweb.org/anthology/W/W13/W13-0804.pdf
(2008),W13-0804,It is basically an adaptation of one of the k-best parsing algorithms by Huang and Chiang (2005).,Good descriptions of the cube pruning implementation in the Joshua decoder have been provided by Li and Khudanpur (2008) and Li et al (2009b).,Xu and Koehn (2012) implemented hierarchical search with the cube growing algorithm in Moses and compared its performance to Moses' cube pruning implementation.,http://www.aclweb.org/anthology/W/W13/W13-0804.pdf
"(Och, 2003)",W13-0804,"The language models are 4-grams with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) which have been trained with the SRILM toolkit (Stolcke, 2002).","During decoding, a maximum length constraint of ten is applied to all non-terminals except the initial symbol S. Model weights are optimized with MERT (Och, 2003) on 100-best lists.",The optimized weights are obtained (separately for deep and for shallow-1 grammars) with a k-best generation size of 1000 for Chinese-*English and of 500 for Arabic-*English and kept for all setups.,http://www.aclweb.org/anthology/W/W13/W13-0804.pdf
(1967),W00-1220,"In resent years, Western researchers have published a large volume of papers, which present many points of view.","The most important are Vendler(1967), Bache(1982), and Smith (1985) .","They approximately classify the situation as four types: state, activity, accomplishment, and achievement.",http://www.aclweb.org/anthology/W/W00/W00-1220.pdf
[3],W00-1220,(2) Separate classification of the verb situation from classification of the sentence situation.,"Chen[3] points that some verbs belong to more than one category, and gives a method to distinguish these cases.","To make the ideal more clear, we use two steps to complete the sentence situation recognition.",http://www.aclweb.org/anthology/W/W00/W00-1220.pdf
[11],W00-1220,2.3 Implementation of the algorithm,"According to table 2.1, a classification algorithm was designed, and we use two resources to implement our algorithm: The Contemporary Chinese Cihai [11] (which we will refer to as the Cihai below) dictionary and the Machine Tractable Dictionary of Contemporary Chinese Predicate Verbs [12}(which we will refer to as the predicate dictionary below).","The Cihai dictionary includes 12,000 entries and 700,000 collocation instances, predicate dictionary includes about 3000 verbs with their semantic information, case relations and detailed collocation information.",http://www.aclweb.org/anthology/W/W00/W00-1220.pdf
(2010),S15-2120,2010).,Davidov et al (2010) examined hashtags that indicated sarcasm to identify if such labelled tweets can be a reliable source of sarcasm.,They concluded that user-labelled sarcastic tweets can be noisy and constitute the hardest form of sarcasm.,http://www.aclweb.org/anthology/S/S15/S15-2120.pdf
(2013),S15-2120,They concluded that user-labelled sarcastic tweets can be noisy and constitute the hardest form of sarcasm.,Riloff et al (2013) identify sarcasm that arises from the contrast between a positive sentiment referring to a negative situation.,"Reyes et al (2012) involved in their work features that make use of contextual imbalance, natural language concepts, syntactical and morphological aspects of a tweet.",http://www.aclweb.org/anthology/S/S15/S15-2120.pdf
"(He et al., 2010a)",N13-3003,The confidence metric ensures that only Author did this work during his post doctoral research at CNGL.,"those MT outputs that are guaranteed to require less post-editing effort than the best corresponding TM match are presented to the post-editor (He et al., 2010a).","The MT is integrated seamlessly, and established localisation cost estimation models based on TM technologies still apply as upper bounds.",http://www.aclweb.org/anthology/N/N13/N13-3003.pdf
"(Specia, 2011)",N13-3003,"MT confidence estimation and its relation to existing TM scoring methods, together with how to make the most effective use of both technologies, is an active area of research.","(Specia, 2011) and (Specia et al., 2009, 2010) propose a confidence estimator that relates specifically to the post-editing effort of translators.","This research uses regression on both the automatic scores assigned to the MT and scores assigned by posteditors and aims to model post-editors' judgements of the translation quality between good and bad, or among three levels of post-editing effort.",http://www.aclweb.org/anthology/N/N13/N13-3003.pdf
"(Koehn and Haddow, 2009)",N13-3003,"Related research by (Simard and Isabelle., 2009) focuses on combining TM information into an SMT system for improving the performance of the MT when a close match already exists within the TM.","(Koehn and Haddow, 2009) presents a post-editing environment using information from the phrasebased SMT system Moses.2 (Guerberof, 2009) compares the post-editing effort required for TM and MT output, respectively.","(Tatsumi, 2009) studies the correlation between automatic evaluation scores and post-editing effort.",http://www.aclweb.org/anthology/N/N13/N13-3003.pdf
"(Lin et al., 2007)",N13-3003,"System-independent features for each translation output are fed as input to the SVM classifier (Cortes and Vapnik, 1995).","The SVM classifier outputs class labels and the class labels are converted into confidence scores using the techniques given in (Lin et al., 2007).",Relying on system independent black-box features has allowed us to build,http://www.aclweb.org/anthology/N/N13/N13-3003.pdf
"(Germann, 2003)",D10-1091,Translating a sentence amounts to finding the best scoring translation hypothesis in the search space.,"Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004).","Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics.",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf
"(Dreyer et al., 2007",D10-1091,1.3 Contribution and organization,"To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS.",We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure.,http://www.aclweb.org/anthology/D/D10/D10-1091.pdf
"Koehn, 2007)",D10-1091,934,"Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009).",We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3).,http://www.aclweb.org/anthology/D/D10/D10-1091.pdf
"(Banerjee and Lavie, 2005)",D10-1091,"This amounts to evaluating translation quality in terms of unigram precision and recall, which are highly correlated with human judgements (Lavie et al., ).","This measure is closely related to the BLEU-1 evaluation metric and the Meteor (Banerjee and Lavie, 2005) metric (when it is evaluated without considering near-matches and the distortion penalty).",We also believe that hypotheses that maximize the unigram precision and recall at the sentence level yield corpus level BLEU-4 scores close the maximal achievable.,http://www.aclweb.org/anthology/D/D10/D10-1091.pdf
"(Germann et al., 2001)",D10-1091,"To solve the combinatorial problem introduced in the previous section, we propose to cast it into an Integer Linear Programming (ILP) problem, for which many generic solvers exist.","ILP has already been used in SMT to find the optimal translation for word-based (Germann et al., 2001) and to study the complexity of learning phrase alignments (De Nero and Klein, 2008) models.","Following the latter reference, we introduce the following variables: fi,j (resp.",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf
"(Kumar and Byrne, 2005)",D10-1091,"l < m  1, ai,j,k,lai0,j0,l+1,l0  |i0  j  1 |< d","Implementing the ""local"" or MJ-d (Kumar and Byrne, 2005) reordering strategy is also straightforward, and implies using the following constraints:","Xai0,j0,k0,l0  k0k",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf
"(Dreyer et al., 2007)",D10-1091,"This is the main motivation of (Tillmann and Zhang, 2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering model.","Oracle decoding has also been used to assess the limitations induced by various reordering constraints in (Dreyer et al., 2007).","To this end, the authors propose to use a beam-search based oracle decoder, which computes lower bounds of the best achievable BLEU-4 using dynamic programming techniques over finite-state (for so-called local and IBM constraints) or hierarchically structured (for ITG constraints) sets of hypotheses.",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf
"(Auli et al., 2009)",D10-1091,"To the best of our knowledge, there are only a few works that try to study the expressive power of phrase-based machine translation systems or to provide tools for analyzing potential causes of failure.","The approach described in (Auli et al., 2009) is very similar to ours: in this study, the authors propose to find and analyze the limits of machine translation systems by studying the reference reachability.",A reference is reachable for a given system if it can be exactly generated by this system.,http://www.aclweb.org/anthology/D/D10/D10-1091.pdf
"(Achterberg, 2007)",D10-1091,"Even though ILP problems are NP-hard in general, there exist several off-theshelf ILP solvers able to efficiently find an optimal solution or decide that the problem is infeasible.","In our experiments, we used the free solver SCIP (Achterberg, 2007).",An optimal solution was found for all problems we considered.,http://www.aclweb.org/anthology/D/D10/D10-1091.pdf
(2008),D13-1151,"Subsequent work, such as the StahelDonoho Estimator (Stahel, 1981; Donoho, 1982), PCout (Filzmoser et al., 2008), LOF (Breunig and Kriegel, 2000) and ABOD (Kriegel et al., 2008) have generalized univariate methods to highdimensional data points.","In his comprehensive review of outlier detection methods in textual data, Guthrie (2008) compares a variety of vectorization methods along with a variety of generic outlier methods.","The vectorization methods employ a variety of lexical and syntactic stylistic features, while the outlier detection methods use a variety of similarity/distance measures such as cosine and Euclidean distance.",http://www.aclweb.org/anthology/D/D13/D13-1151.pdf
"(Stamatatos, 2009",D13-1151,"The vectorization methods employ a variety of lexical and syntactic stylistic features, while the outlier detection methods use a variety of similarity/distance measures such as cosine and Euclidean distance.","Similar methods have also been used in the field of intrinsic plagiarism detection, which involves segmenting a text and then identifying outlier segments (Stamatatos, 2009; Stein et al., 2010).",3 Proximity Methods,http://www.aclweb.org/anthology/D/D13/D13-1151.pdf
(1992),N07-1039,"Each document in each corpus was preprocessed into individual sentences, lower-cased, and tokenized.","We used an implementation of Brill's (1992) part-of-speech tagger to find adjectives and modifiers; for parsing, we used the Stanford dependency parser (Klein and Manning, 2003).",6 Evaluating Extraction,http://www.aclweb.org/anthology/N/N07/N07-1039.pdf
"(Turney, 2002",N07-1039,"sions, such as where an attitude is expressed via a noun or a verb.","In this regard, we will be examining extension of existing methods for automatically building lexicons of positive/negative words (Turney, 2002; Esuli and Sebastiani, 2005) to the more complex task of estimating also attitude type and force.",,http://www.aclweb.org/anthology/N/N07/N07-1039.pdf
(1995),N07-1039,"We generally look for rules that contain attitude type, orientation, thing type, and a product name, when these rules occur more frequently than expected.",The idea is similar to Agrawal and Srikant's (1995) notion of generalized association rules.,"We treat each appraisal expression as a transaction, with the attributes of attitude type, orientation, polarity, force, and thing type, as well as the document attributes product name, product type, and document classification (based on the number of stars the reviewer gave the product).",http://www.aclweb.org/anthology/N/N07/N07-1039.pdf
"(Whitelaw et al., 2005)",N07-1039,311,"For adjectival attitudes, we used the lexicon developed we developed in our previous work (Whitelaw et al., 2005) on appraisal.",We reviewed the entire lexicon to determine its accuracy and made numerous improvements.,http://www.aclweb.org/anthology/N/N07/N07-1039.pdf
"(Wang et al., 2003)",N07-1039,"We treat each appraisal expression as a transaction, with the attributes of attitude type, orientation, polarity, force, and thing type, as well as the document attributes product name, product type, and document classification (based on the number of stars the reviewer gave the product).","We use CLOSET+ (Wang et al., 2003) to find all of the frequent closed itemsets in the data, with a support greater than or equal to 20 occurrences.","Let (b, a1, a2,... an) or (b, A) denote the contents of an itemset, and c ((b, A)) denote the support for this itemset.",http://www.aclweb.org/anthology/N/N07/N07-1039.pdf
"(Carroll et al., 1998",W12-2202,"The present work, however, deals with lexical simplification and is centred around a corpus analysis, a preparatory stage for the development of a separate lexical module in the future.","Earlier work already establishes the importance of lexical changes for text simplification (Carroll et al., 1998; Caseli et al., 2009; De Belder et al., 2010).","Upon examining a parallel corpus consisting of original and manually simplified newspaper articles in Spanish, we have found that by far the most common type of changes applied by human editors are precisely lexical changes, accounting for 17.48% of all annotated operations (Bott and Saggion, 2012a).",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf
"(Bott and Saggion, 2012a)",W12-2202,"Earlier work already establishes the importance of lexical changes for text simplification (Carroll et al., 1998; Caseli et al., 2009; De Belder et al., 2010).","Upon examining a parallel corpus consisting of original and manually simplified newspaper articles in Spanish, we have found that by far the most common type of changes applied by human editors are precisely lexical changes, accounting for 17.48% of all annotated operations (Bott and Saggion, 2012a).",Words perceived as more complicated are replaced,http://www.aclweb.org/anthology/W/W12/W12-2202.pdf
"(Carroll et al., 1998)",W12-2202,"The earliest simplification systems employed a rule-based approach and focused on syntactic structure of the text (Chandrasekar et al., 1996).","The PSET project (Carroll et al., 1998) dealt with simplification of news articles in English for aphasic readers.","Together with syntactic analysis and transformations similar to those of Chandrasekar et al (1996), they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency from the Oxford Psycholinguistic Database (Quinlan, 1992).",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf
(2009),W12-2202,"The above approach to lexical simplification has been repeated in a number of works (Lal and Ruger, 2002; Burstein et al., 2007).","Bautista et al (2009) also rely on a dictionary of synonyms, but their criterion for choosing the most appropriate one is wordlength rather than frequency.","Caseli et al (2009) analyse lexical operations on a parallel corpus of original and manually simplified texts in Portuguese, using lists of simple words and discourse markers as resources.",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf
"(Cunningham et al., 2002)",W12-2202,"aligning algorithm based on Hidden Markov Models (Bott and Saggion, 2011) has been applied to obtain sentence-level alignments.","The automatic alignments have then been manually corrected through a graphical editing tool within the GATE framework (Cunningham et al., 2002).","A total of 570 sentences have been aligned (246 in original and 324 in simple texts), with the following correlations between them: one to one, one to many or many to one, as well as cases where there is no correlation (cases of content reduction through summarisation or information expansion through the introduction of definitions).",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf
(1996),W12-2202,"The PSET project (Carroll et al., 1998) dealt with simplification of news articles in English for aphasic readers.","Together with syntactic analysis and transformations similar to those of Chandrasekar et al (1996), they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency from the Oxford Psycholinguistic Database (Quinlan, 1992).","Therefore, the most frequent of a set of synonyms for every content word of the input text was chosen to appear in its simplified version.",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf
"(Hajic et al., 2004",W06-2938,This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques.,"The treebanks (Hajic et al., 2004; Chen et al., 2003; Bohmova et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair.",An initial model is built to identify dependency relations between adjacent word pairs using a decision list learning algorithm.,http://www.aclweb.org/anthology/W/W06/W06-2938.pdf
"(Bangalore et al., 2000)",P00-1059,"This number is subtracted from and then divided by the length of the sentence, yielding a number between 0 and 1, with 1 the best score.","For a more detailed discussion of the issue of metrics in evaluation, of the metrics we have used, and of an experiment relating human judgments to these metrics, see (Bangalore et al., 2000).",String accuracy measures the performance of the entire FERGUS system.,http://www.aclweb.org/anthology/P/P00/P00-1059.pdf
"(Hamaker et al., 1998)",W00-1004,An alternative approach is seen in some schemes used for labeling corpora for purposes of training and evaluating speech recognizers.,"A quote from the most recent Switchboard labeling standard (Hamaker et al., 1998) gives the flavor: 20.","Hesitation Sounds: Use ""uh"" or ""oh"" for hesitations consisting of a vowel sound, and ""urn"" or ""hm"" for hesitations with a nasal sound, depending upon which transcription the actual sound is closest to.",http://www.aclweb.org/anthology/W/W00/W00-1004.pdf
"(Lander, 1996)",W00-1004,"<other speaker responds> urn ok, I see your point.""","21: yes/no sounds: Use ""uh-huh"" or ""um-hum"" (yes) and ""huh-uh"" or ""hum-um"" (no) for anything remotely resembling these sounds of assent or denial"" Another scheme (Lander, 1996) lists several ""miscellaneous words"", including:",30,http://www.aclweb.org/anthology/W/W00/W00-1004.pdf
"(Barry and Fourcin, 1992)",W00-1004,"2. can represent all observed grunts, and 3. unambiguously represents all meaningful differences in sound.","While it is not possible to devise a single transcription scheme which is perfect for all purposes (Barry and Fourcin, 1992), it is clear that the current schemes all have room for improvement.",3 Proposal,http://www.aclweb.org/anthology/W/W00/W00-1004.pdf
(2012),D13-1021,We introduce a noise symbol to handle partial noise in the many-to-many alignment model.,"Htun et al (2012) extended the many-to-many alignment for the sample-wise transliteration mining, but its noise model only handles the sample-wise noise and cannot distinguish partial noise.",We model partial noise in the CRP-based joint substring model.,http://www.aclweb.org/anthology/D/D13/D13-1021.pdf
"(Finch and Sumita, 2010)",D13-1021,"BASELINE 104,563 899,080 1,372,993 PROPOSED 104,561 893,366 1,317,256","phrases in statistical machine transliteration and improved transliteration performance (Finch and Sumita, 2010).","We extracted them by: 1) generate many-to-many word alignment, in which all possible word alignment links in many-to-many correspondences (e.g., 0-0 0-1 0-2 1-0 1-1 1-2 for  , c o m), 2) run phrase extraction and scoring same as a standard Moses training.",http://www.aclweb.org/anthology/D/D13/D13-1021.pdf
"(Brown et al., 1994)",P06-2061,1 Introduction,"A desired feature of computer-assisted translation (CAT) systems is the integration of the human speech into the system, as skilled human translators are faster at dictating than typing the translations (Brown et al., 1994).","Additionally, incorporation of a statistical prediction engine, i.e.",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf
"(Foster et al., 1997",P06-2061,"a statistical interactive machine translation system, to the CAT system is another useful feature.","A statistical prediction engine provides the completions to what a human translator types (Foster et al., 1997; Och et al., 2003).","Then, one possible procedure for skilled human translators is to provide the oral translation of a given source text and then to post-edit the recognized text.",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf
"(Och et al., 2003)",P06-2061,"Then, one possible procedure for skilled human translators is to provide the oral translation of a given source text and then to post-edit the recognized text.","In the post-editing step, a prediction engine helps to decrease the amount of human interaction (Och et al., 2003).","In a CAT system with integrated speech, two sources of information are available to recognize the speech input: the target language speech and the given source language text.",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf
"(Och et al., 2003)",P06-2061,"Thus, the integration of MT models to ASR word graphs can be considered as an N-best rescoring but with very large value for N. Another advantage of working with ASR word graphs is the capability to pass on the word graphs for further processing.","For instance, the resulting word graph can be used in the prediction engine of a CAT system (Och et al., 2003).","The remaining part is structured as follows: in Section 2, a general model for an automatic text dictation system in the computer-assisted translation framework will be described.",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf
"(Vogel et al., 1996)",P06-2061,"better MT system, and generating a larger N-best list from the ASR word graphs.","We rescore the ASR N-best lists with the standard HMM (Vogel et al., 1996) and IBM (Brown et al., 1993) MT models.","The development and evaluation sets Nbest lists sizes are sufficiently large to achieve almost the best possible results, on average 1738 hypotheses per each source sentence are extracted from the ASR word graphs.",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf
"(Khadivi et al., 2005)",P06-2061,4.2 N-best Rescoring,"To rescore the N-best lists, we use the method of (Khadivi et al., 2005).","But the results shown here are different from that work due to a better optimization of the overall ASR system, using a",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf
"(Kanthak et al., 2005)",P06-2061,"a sequence of nodes with one incoming arc and one outgoing arc, the words of source language text are placed consecutively in the arcs of the acceptor, 2. an acceptor containing possible permutations.","To limit the permutations, we used an approach as in (Kanthak et al., 2005).",Each of these two acceptors results in different constraints for the generation of the hypotheses.,http://www.aclweb.org/anthology/P/P06/P06-2061.pdf
(2013),W13-2241,"In the future, we plan to further investigate these models by devising more advanced kernels and feature selection methods.","Specifically, we want to employ our feature set in a multi-task kernel setting, similar to the one proposed by Cohn and Specia (2013).","These kernels have the power to model inter-annotator variance and noise, which can lead to better results in the prediction of post-editing time.",http://www.aclweb.org/anthology/W/W13/W13-2241.pdf
"(Shah et al., 2013)",W13-2241,"For task 1.1, we performed this feature selection over all 160 features mentioned in Section 2.","For task 1.3, we used a subset of the 80 most general BB features as in (Shah et al., 2013), for which we had all the necessary resources available for the extraction.","We selected the top 25 features for both models, based on empirical results found by Shah et al (2013) for a number of datasets, and then retrained the GP using only the selected features.",http://www.aclweb.org/anthology/W/W13/W13-2241.pdf
"(Settles and Craven, 2008)",W13-2241,"We optimised the Gaussian Process hyperparameters every 20 new instances, for both tasks.","As a measure of informativeness we used Information Density (ID) (Settles and Craven, 2008).",This measure leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is:,http://www.aclweb.org/anthology/W/W13/W13-2241.pdf
(2008),W13-2241,"In a real annotation setting, it is important to decide when to stop adding new instances to the training set.","In this work, we used the confidence method proposed by Vlachos (2008).",This is an method that measures the model's confidence on a held-out non-annotated dataset every time a new instance is added to the training set and stops the AL procedure when this confidence starts to drop.,http://www.aclweb.org/anthology/W/W13/W13-2241.pdf
"(Cohn and Specia, 2013",W13-2241,The kernel function encodes the covariance (similarity) between each input pair.,"While a variety of kernel functions are available, here we followed previous work on QE using GP (Cohn and Specia, 2013; Shah et al., 2013) and employed a squared exponential (SE) kernel with automatic relevance determination (ARD): where F is the number of features, 2f is the covariance magnitude and li > 0 are the feature length scales.","The resulting model hyperparameters (SE variance 2f, noise variance 2n and SE length scales li) were learned from data by maximising the model likelihood.",http://www.aclweb.org/anthology/W/W13/W13-2241.pdf
(2013),W13-2241,"To avoid poor hyperparameter values due to this, we performed a two-step procedure where we first optimise a model with all the SE length scales tied to the same value (which is equivalent to an isotropic model) and we used the resulting values as starting point for the ARD optimisation.","To perform feature selection, we followed the approach used in Shah et al (2013) and ranked the features according to their learned length scales (from the lowest to the highest).",The length scales of a feature can be interpreted as the relevance of such feature for the model.,http://www.aclweb.org/anthology/W/W13/W13-2241.pdf
(2000),N04-4019,Fig.,1 shows a sample Speech Graffiti dialog User interactions with Speech Graffiti (independent of other speech interfaces) have previously been assessed in Rosenfeld et al (2000).,"Here we consider a head-to-head comparison: given the chance to interact with both types of interfaces, which would people choose?",http://www.aclweb.org/anthology/N/N04/N04-4019.pdf
"Guindon & Shuldberg, 1987",N04-4019,Several studies have previously found that users are able to interact successfully using constrained or subset languages (e.g.,"Guindon & Shuldberg, 1987; Ringle & Halstead-Nussloch, 1989; Sidner & Forlines, 2002).","As far as we know, no studies have been done comparing constrained, ""universal"" languages and natural language interfaces directly as we have done in this study.",http://www.aclweb.org/anthology/N/N04/N04-4019.pdf
(2001),N04-4019,"As far as we know, no studies have been done comparing constrained, ""universal"" languages and natural language interfaces directly as we have done in this study.",General information about the Speech Graffiti project and its motivation can be found in Rosenfeld et al (2001).,2 Method,http://www.aclweb.org/anthology/N/N04/N04-4019.pdf
(2011),W15-4923,"Conceptually, this is loosely related to semantically focused metrics (e.g.","MEANT, Lo and Wu (2011)), as we go beyond a ""flat"" n-gram matching but evaluate a meaningful entity, in our case a preposition-noun-verb triple.",3 Methodology,http://www.aclweb.org/anthology/W/W15/W15-4923.pdf
(2009),W15-4923,Weller et al (2014) use noun class information as tree labels in syntactic SMT to model selectional preferences of prepositions.,"The presented work is similar to that of Agirre et al (2009), but is applied to a fully statistical MT system.","The main difference is that Agirre et al (2009) use linguistic information to select appropriate translation rules, whereas we generate prepositions in a post-processing step.",http://www.aclweb.org/anthology/W/W15/W15-4923.pdf
(2008),W15-4923,3 Methodology,"Our approach is integrated into an English-German morphology-aware SMT system which first translates into a lemmatized representation with a component to generate fully inflected forms in a second step, an approach similar to the work by Toutanova et al (2008) and Fraser et al (2012).","The inflection requires the modeling of the grammatical case of noun phrases (among other features), which corresponds to determining the syntactic function1.",http://www.aclweb.org/anthology/W/W15/W15-4923.pdf
"(Milne et al., 2007)",W14-3611,Synonyms from WordNet are used to expand the question in order to extract the most semantically relevant passages to the question.,"(Milne et al., 2007) proposed a system called ""KORU"" for query expansion using Wikipedia's most relevant articles to user's query.",The system allows the user to refine the set of Wikipedia pages to be used for expansion.,http://www.aclweb.org/anthology/W/W14/W14-3611.pdf
"(Weston et al., 2011",P15-1061,We use stochastic gradient descent (SGD) to minimize the loss function with respect to .,"Like some other ranking approaches that only update two classes/examples at every training round (Weston et al., 2011; Gao et al., 2014), we can efficiently train the network for tasks which have a very large number of classes.",This is an advantage over softmax classifiers.,http://www.aclweb.org/anthology/P/P15/P15-1061.pdf
"(Zhang, 2004",P15-1061,5 Related Work,"Over the years, various approaches have been proposed for relation classification (Zhang, 2004; Qian et al., 2009; Hendrickx et al., 2010; Rink and Harabagiu, 2010).",Most of them treat it as a multiclass classification problem and apply a variety of machine learning techniques to the task in order to achieve a high accuracy.,http://www.aclweb.org/anthology/P/P15/P15-1061.pdf
(2011),P15-1061,Zeng et al (2014) propose the use of word position embeddings (position features) which help the CNN by keeping track of how close words are to the target nouns.,These features are similar to the position features proposed by Collobert et al (2011) for the Semantic Role Labeling task.,In this work we also experiment with the word position embeddings (WPE) proposed by Zeng et al (2014).,http://www.aclweb.org/anthology/P/P15/P15-1061.pdf
(2014),P15-1061,"For tasks where the number of classes is large, we can fix a number of negative classes to be considered at each example and select the one with the largest score to perform a gradient step.",This approach is similar to the one used by Weston et al (2014) to select negative examples.,We use the backpropagation algorithm to compute gradients of the network.,http://www.aclweb.org/anthology/P/P15/P15-1061.pdf
(2014),P15-1061,"CR-CNN outperforms CNN+Softmax in both precision and recall, and improves the F1 by 1.6.",The third line in Table 4 shows the result reported by Zeng et al (2014) when only word embeddings and WPEs are used as input to the network (similar to our CNN+Softmax).,We believe that the word embeddings employed by them is the main reason their result is much worse than that of CNN+Softmax.,http://www.aclweb.org/anthology/P/P15/P15-1061.pdf
"(Zechner et al., 2007)",W08-0912,"Entropy in this context can be seen as a measure for how predictable the language in the expected spoken response is: Some tests, such as SET-10 (Bernstein 1999), are focused mostly on the lower entropy aspects of language, using tasks such as ""reading"" or ""repetition"", where the expected sequence of words is highly predictable.","Other assessments, such as the TOEFL Practice Online Speaking test, on the other hand, focus on more spontaneous, high-entropy responses (Zechner et al., 2007).","In this paper, we describe a spoken language test with heterogeneous task types, ranging from read speech to tasks that require candidates to give their opinions on an issue, whose goal is to assess communicative competence (Bachman, 1990; Bachman & Palmer, 1996); we call this test THT (Test with Heterogeneous Tasks).",http://www.aclweb.org/anthology/W/W08/W08-0912.pdf
"(North, 2000)",W08-0912,"In Bernstein et al (2000), an experiment is performed to investigate the performance of the SET-10 test in predicting speakers' oral proficiency.","It is shown that the SET-10 test scores can predict different levels on the Oral Interaction Scale of the Council of Europe's Framework (North, 2000) for describing oral proficiency of second/foreign language speakers with reasonable accuracy.",This paper further reports on studies done to correlate the SET-10 automated scores with the human scores from two other tests of oral English communication skills.,http://www.aclweb.org/anthology/W/W08/W08-0912.pdf
"(Zechner et al., 2007)",W08-0912,"For this work, we are using a state-of-the-art gender-independent Hidden Markov Model speech recognizer whose acoustic model was trained on about 30 hours of non-native speech and whose language model was built on several hundred hours of both native and non-native speech.","The nonnative data came from the TOEFL Practice Online system, a web-based practice program for prospective takers of the Test Of English as a Foreign Language (TOEFL) (Zechner et al., 2007).","This data is somewhat different from the THT, as there are only high-entropy tasks in TOEFL Speaking and as the speakers are generally more proficient.",http://www.aclweb.org/anthology/W/W08/W08-0912.pdf
"(Tomokiyo and Waibel, 2001",W08-0912,4.4 Unsupervised speaker adaptation,"We used unsupervised maximum likelihood linear regression (MLLR) AM adaptation on top of the previous adaptation and optimization steps (Tomokiyo and Waibel, 2001; Wang et al., 2003).","In this step, all words whose confidence score was higher than a pre-set threshold were collected and their acoustic information was used to adapt the acoustic model.",http://www.aclweb.org/anthology/W/W08/W08-0912.pdf
"(Papineni et al., 2002)",W14-4719,"Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics.","Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations.","Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy.",http://www.aclweb.org/anthology/W/W14/W14-4719.pdf
(2012),W14-4719,3 Comparison of multiword expression association approaches,"To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider four alternative approaches to defining the lexical similarities si,pred and si,j, all of which employ a standard context vector model of the individual words/tokens in the multiword expression arguments between the reference and machine translations, as descibed by Lo et al (2012) and Tumuluru et al (2012).",3.1 Bag of words (geometric mean),http://www.aclweb.org/anthology/W/W14/W14-4719.pdf
(2006),W14-4719,3.2 Maximum alignment (precision-recall average),"In the first maximum alignment based approach we will consider, the definitions of sz,pred and sz, are inspired by Mihalcea et al (2006) who normalize phrasal similarities according to the phrase length.","1 Si,pred = 2 1precei,pred,fi,pred + recei,pred,fi,pred 1 =2 1precei,j,fi,j + recei,j ,fi,j",http://www.aclweb.org/anthology/W/W14/W14-4719.pdf
"(Callison-Burch et al., 2008, 2010, 2011, 2012",W14-4719,output.,"We computed sentence-level correlations following the benchmark assessment procedure used by WMT and NIST MetricsMaTr (Callison-Burch et al., 2008, 2010, 2011, 2012; and Bojar, 2013), which use T correlation coefficient, to evaluate the correlation of evaluation metrics against human judgment on ranking the translation adequacy of the three output.",A higher value for T indicates more similarity to the human adequacy rankings by the evaluation metrics.,http://www.aclweb.org/anthology/W/W14/W14-4719.pdf
(2011a),W14-4719,A higher value for T indicates more similarity to the human adequacy rankings by the evaluation metrics.,"The range of possible values of T correlation coefficient is 1], where 1 mean (2011a).","state-of-the-art systems' Machacek Kendall's systems' Kendall's Kendall's [-1, s the",http://www.aclweb.org/anthology/W/W14/W14-4719.pdf
(2012),W14-4719,"The ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave tokens unaligned instead of aligning them anyway as the other strategies tend to do.","Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al (2012) showed that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure.",5 Conclusion,http://www.aclweb.org/anthology/W/W14/W14-4719.pdf
(2009),W14-4719,"The rule probability (or more accurately, rule weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is defined by MEANT's lexical similarity measure on English Gigaword context vectors.","To calculate the ( ) inside probability (or more accurately, inside score) of a pair of segments, P A   e/f|G , we use the algorithm described in Saers et al (2009).","Given this, si,pred and si, now represent the length normalized BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and machine translations.",http://www.aclweb.org/anthology/W/W14/W14-4719.pdf
"(Ido Dagan et al., 2006)",W10-1609,1 Introduction,"The objective of the Recognizing Textual Entailment Challenge is determining whether the meaning of the Hypothesis (H) can be inferred from a text (T) (Ido Dagan et al., 2006).",This challenge has been organized by NIST in recent years.,http://www.aclweb.org/anthology/W/W10/W10-1609.pdf
"(Witten & Frank, 2005)",W10-1609,"The training set SPARTE-Balanced was created by taking all true cases and randomly taking false cases, and then we build a balanced training set containing 1352 pairs, with 676 true and 676 false pairs.","We used four classifiers to learn every development set: (1) Support Vector Machine, (2) Ada Boost, (3) Multilayer Perceptron (MLP) and (4) Decision Tree using the open source WEKA Data Mining Software (Witten & Frank, 2005).",In all the tables results we show only the accuracy of the best classifier.,http://www.aclweb.org/anthology/W/W10/W10-1609.pdf
"(Baroni and Lenci, 2010)",P13-2128,Abstract,"Syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces (Baroni and Lenci, 2010).","However, they are also sparse, with resulting reliability and coverage problems.",http://www.aclweb.org/anthology/P/P13/P13-2128.pdf
(2010),P13-2128,"The advantage of syntactic models is that they incorporate a richer, structured notion of context.",This makes them more versatile; the Distributional Memory framework by Baroni and Lenci (2010) is applicable to a wide range of tasks.,"It is also able - at least in principle - to capture more fine-grained types of semantic similarity such as predicateargument plausibility (Erk et al., 2010).",http://www.aclweb.org/anthology/P/P13/P13-2128.pdf
"(Voorhees, 1994",P13-2128,"Many of the methods were first applied in Language Modeling to deal with unseen n-grams (Chen and Goodman, 1999; Dagan et al., 1999).","Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003).","In lexical semantics, smoothing is often achieved by backing",http://www.aclweb.org/anthology/P/P13/P13-2128.pdf
"(Zesch et al., 2007)",P13-2128,The first task is predicting semantic similarity.,"We lemmatized and POS-tagged the German GUR350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict",2Downloadable from: http://goo.gl/bFokI,http://www.aclweb.org/anthology/P/P13/P13-2128.pdf
"(Zeller et al., 2013)",P13-2128,"For German, there are several resources with derivational information.","We use version 1.3 of DERIVBASE (Zeller et al., 2013),1 a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 nonsingleton derivational families.",It has a precision of 84% and a recall of 71%.,http://www.aclweb.org/anthology/P/P13/P13-2128.pdf
"(Pado and Utt, 2012)",P13-2128,The syntactic distributional model that we use represents target words by pairs of dependency relations and context words.,"More specifically, we use the W x LW matricization of DM.DE, the German version (Pado and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010).","DM.DE was created on the basis of the 884M-token SDEWAC web corpus (Faa et al., 2010), lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010).",http://www.aclweb.org/anthology/P/P13/P13-2128.pdf
(2007),P13-2128,"We measure quality of the semantic similarity task as the Pearson correlation between the model predictions and the human judgments for covered items (Zesch et al., 2007).","For synonym choice, we follow the method established by Mohammad et al (2007), measuring accuracy over covered items, with partial credit for ties.",Results for Semantic Similarity.,http://www.aclweb.org/anthology/P/P13/P13-2128.pdf
"(Mikolov et al., 2013)",W15-2610,2.3 Skip-gram,"Like the SENNA model, the Skip-gram model (Mikolov et al., 2013) is trained to differentiate between the correct central word of a phrase and a random replacement, which they refer to as negative sampling.","Unlike SENNA, however, the Skipgram model tries to make this prediction using only a single one of the surrounding words at a time and ignores the ordering of those words, i.e.",http://www.aclweb.org/anthology/W/W15/W15-2610.pdf
(2003b),W15-2610,"While an unlexicalized parser that uses syntactic categories based solely on the symbols found in the Penn Treebank will generally perform poorly, a number of results show that refining these categories can substantially improve performance.","Klein and Manning (2003b), for example, show that the performance of an unlexicalised model can be substantially improved by splitting the existing symbols down into finer categories.",Their subcategorizations were developed by hand based on linguistic intuitions and a careful error analysis.,http://www.aclweb.org/anthology/W/W15/W15-2610.pdf
(1997),W15-2610,"Surprisingly, the Skip-gram model retrained on biomedical data (SG-bio) fared worse than the original (SG-news), due probably in large part to the fact that the original training data was almost 100 times larger than our 1.2B word corpus.","The ngram contexts achieved the best F-Scores fairly consistently for all parsers, vindicating our appeal to the psycholinguistic research of Cartwright and Brent (1997), Mintz (2003) and Redington et al (1998).","Turning now to each parser individually, the baseline performance of the Berkeley Parser proved difficult to exceed, with only the 2gram distributional contexts giving any improvement.",http://www.aclweb.org/anthology/W/W15/W15-2610.pdf
"(Das and Petrov, 2011",W15-2610,"Thus, these clusters engender a form of distributional similarity comparable to that used in our KNN algorithm.","KNN algorithms are also commonly used in Graph-Based Semi-Supervised Learning approaches (Das and Petrov, 2011; Altun et al., 2006; Subramanya et al., 2010), with the knearest-neighbour sets determining the edges that structure the graph.",POS tags are then propagated through the graph from labelled to unlabelled data.,http://www.aclweb.org/anthology/W/W15/W15-2610.pdf
"(Petrov et al., 2006)",W15-2610,Their subcategorizations were developed by hand based on linguistic intuitions and a careful error analysis.,"The Berkeley Parser4 (Petrov et al., 2006), in contrast, is based on a method for automatically finding useful subcategorizations during training by splitting and merging the original nodes.","The model is an unlexicalized generative PCFG, but the granularity of the terminal and nonterminal categories found in training give it a much greater sensitivity to the syntactic behaviour of words and phrases than is possible using standard POS tags.",http://www.aclweb.org/anthology/W/W15/W15-2610.pdf
(2008),W15-2610,POS tags are then propagated through the graph from labelled to unlabelled data.,"Although similarity in these cases is commonly being assessed between token sequences, as opposed to word types, the features used are similar to the ngram templates used here and the bigram distributions used by Koo et al (2008).",A major difference in our approach is that it does not require retraining the parser or constructing a full model on the unlabelled data.,http://www.aclweb.org/anthology/W/W15/W15-2610.pdf
"(De La Clergerie et al., 2009)",W13-0302,"In this section, we address the issue of defining the boundaries of E_M segments, using a syntactic parser.","We use a large-coverage syntactic parser for French, FRMG (FRench MetaGrammar) (De La Clergerie et al., 2009).",The main syntactic contexts in which semantic clues can occur are as follows:,http://www.aclweb.org/anthology/W/W13/W13-0302.pdf
"(Pustejovsky et al., 2002c)",W02-0312,"'protein', rather than 'Amino Acid, Peptid, or Protein).","This has motivated the development of semantic re-rendering algorithms, designed to enrich the ontological specificity of a domain-specific semantic tag set, based on inductive techniques described in (Pustejovsky et al., 2002c).",3.4 Rerendering Semantic Ontologies,http://www.aclweb.org/anthology/W/W02/W02-0312.pdf
"(Toutanova et al., 2005)",W05-0623,The model is trained to re-rank a set of N likely labelings according to the local model.,"We find the exact top N consistent' most likely local model labelings using a simple dynamic program described in (Toutanova et al., 2005).",'A labeling is consistent if satisfies the constraint that argument phrases do not overlap.,http://www.aclweb.org/anthology/W/W05/W05-0623.pdf
"(Toutanova et al., 2005)",W05-0623,"The percentage of perfectly labeled propositions for the three sets is 55.11% (development), 56.52% (test), and 37.06% (Brown test).","The improvement achieved by the joint model relative to the local model is about 2 points absolute in F-Measure, similar to the improvement when gold-standard syntactic parses are used (Toutanova et al., 2005).","The relative error reduction is much lower for automatic parses, possibly due to a lower upper bound on performance.",http://www.aclweb.org/anthology/W/W05/W05-0623.pdf
"(Toutanova et al., 2005)",W05-0623,Figure 2: An example tree with semantic role annotations.,"Most of the features we use are described in more detail in (Toutanova et al., 2005).",Here we briefly describe these features and introduce several new joint features (denoted by *).,http://www.aclweb.org/anthology/W/W05/W05-0623.pdf
"(Collins, 2000)",W05-0623,"Since the space of possible joint labelings is exponential in the number of parse tree nodes, a model cannot exhaustively consider these labelings unless it makes strong independence assumptions.","To overcome this problem, we adopt a discriminative re-ranking approach reminiscent of (Collins, 2000).","We use a local model, which labels arguments independently, to generate a smaller number of likely joint labelings.",http://www.aclweb.org/anthology/W/W05/W05-0623.pdf
"(McDonald, 2006)",D15-1154,"However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a).","A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high.","On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004).",http://www.aclweb.org/anthology/D/D15/D15-1154.pdf
"(Nivre and Scholz, 2004)",D15-1154,"A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high.","On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004).",Previous studies explored the trade-off between computational costs and parsing performance.,http://www.aclweb.org/anthology/D/D15/D15-1154.pdf
(2006),D15-1154,different languages.,The system of McDonald et al (2006) achieved the best average parsing performance over 13 languages (excluding English) in CoNLL-2006 shared tasks.,"Its average UAS and LAS are 87.03% and 80.83%, respectively, while our average UAS and LAS excluding English are 87.79% and 81.29%.",http://www.aclweb.org/anthology/D/D15/D15-1154.pdf
"(Eisner, 1996",D15-1154,Here y(x) denotes the set of possible dependency trees for sentence x.,"In this paper, we adopt the second-order sibling factorization (Eisner, 1996; McDonald and",1322,http://www.aclweb.org/anthology/D/D15/D15-1154.pdf
"(Lee, 2001",W15-2518,"Second, the commonly used notion of a domain neglects the fact that topic and genre are two distinct properties of text (Stein and Meyer Zu Eissen, 2006).","While this distinction has long been acknowledged in text classification literature (Lee, 2001; Dewdney et al., 2001; Lee and Myaeng, 2002), most work on domain adaptation in SMT uses in-domain and out-of-domain data that differs on both the topic and the genre level (e.g., Europarl political proceedings (Koehn, 2005) versus EMEA medical text (Tiedemann, 2009)), making it unclear whether the proposed solutions address topic or genre differences.","In this work, we follow text classification literature for definitions of the concepts topic and genre.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
"(Foster and Kuhn, 2007)",W15-2518,"Most existing domain adaptation approaches can be grouped into two categories, depending on where in the SMT pipeline they adapt the system.","First, mixture modeling approaches learn models from different subcorpora and interpolate these linearly (Foster and Kuhn, 2007) or log-linearly (Koehn and Schroeder, 2007).","Sennrich (2012) enhances the approach by interpolating up to ten models, and Bertoldi and Federico (2009) use in-domain monolingual data to automatically generate in-domain bilingual data.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
"(Moore and Lewis, 2010",W15-2518,"Second, instance weighting methods prioritize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013).","In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011).","In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
"(Eidelman et al., 2012",W15-2518,"In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations.","In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003).","Surprisingly, genre (or style) adaptation has only been addressed to a limited extent (Bisazza and Federico, 2012; Wang et al., 2012), with methods requiring the availability of clearly separable in-domain and out-of-domain training corpora.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
(1997),W15-2518,"Karlgren and Cutting (1994) were among the first to use simple document statistics, such as common word frequencies, first-person pronoun count, and average sentence length.","Kessler et al (1997) categorize four types of genre-revealing cues: structural cues (e.g., part-of-speech (POS) tag counts), lexical cues (specific words), character-level cues (e.g., punctuation marks), and derivative cues (ratios and variation measures based on other types of cues).","Dewdney et al (2001) compare a large number of document features and show that these outperform bag-of-words approaches, which are traditionally used in topic-based text classifica",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
(2013),W15-2518,3 Translation model genre adaptation,"For the task of genre adaptation to the genres newswire (NW) and UG comments or weblogs, we use a flexible translation model adaptation approach based on phrase pair weighting using a vector space model (VSM) inspired by Chen et al (2013).","The reason we choose an instanceweighting method rather than a mixture modeling approach is twofold: First, mixture modeling approaches intrinsically depend on subcorpus boundaries, which resemble provenance or require manual labeling.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
(2013),W15-2518,"The reason we choose an instanceweighting method rather than a mixture modeling approach is twofold: First, mixture modeling approaches intrinsically depend on subcorpus boundaries, which resemble provenance or require manual labeling.","Second, Irvine et al (2013) have shown that including relevant training data in a mixture modeling approach solves many coverage errors, but also introduces substantial amounts of new scoring errors.","With phrase-pair weighting we aim to optimize phrase translation selection while keeping our training data fixed, and we can thus compare the impact of several methodological variants on genre adaptation for SMT.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
(2013),W15-2518,different subcorpora.,"Since our aim is to adapt to multiple genres in a test corpus, we follow Chen et al (2013) and manually group our training data into subcorpora that reflect various genres (see Table 3).","While this definition of the vector space can approximate genres at different levels of granularity, manual subcorpus labels are labor intensive to generate, particularly in the scenario where provenance information is not available, and may not generalize well to new translation tasks.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
"(Petrenz and Webber, 2012)",W15-2518,"seven features are most discriminative between the genres NW and UG, and are used in the genrespecific VSM approaches.","features potentially generalize across languages (Petrenz and Webber, 2012), we compute the document-level feature values wi(d) on the source as well as the target sides of our bitext, and we examine whether both are equally suitable for translation model genre adaptation.",3.4 Genre adaptation with LDA,http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
"(Blei et al., 2003)",W15-2518,3.4 Genre adaptation with LDA,"Another type of feature that does not depend on provenance information is Latent Dirichlet allocation (LDA) (Blei et al., 2003), an unsupervised word-based approach that infers a preset number of latent dimensions in a corpus and represents documents as distributions over those dimensions.","Despite its recent successes in topic adaptation for SMT, we expect such a bag-of-words approach to be insufficient to model genre accurately.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
(2012),W15-2518,"2010), with varying numbers of latent dimensions (5, 10, 20, and 50).","Of these, LDA with 10 dimensions yields the best translation performance, which is consistent with findings in a related topic adaptation approach by Eidelman et al (2012).",The LDA features in this VSM variant are inferred from the source side of the training data.,http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
(2013),W15-2518,"Table 3 lists the corpus statistics of the training data, split by manual subcorpus labels as used for the subcorpus VSM variant (see Section 3.2).","While our manually grouped subcorpora approximate those used by Chen et al (2013), exact agreement was impossible to obtain, illustrating that it is not trivial to manually generate optimal subcorpus labels.","We tokenize all Arabic data using MADA (Habash and Rambow, 2005), ATB scheme.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
"(Noreen, 1989)",W15-2518,"Translation quality of all experiments is measured with case-insensitive BLEU (Papineni et al., 2002) using the closest-reference brevity penalty.","We use approximate randomization (Noreen, 1989) for significance testing (Riezler and Maxwell, 2005).","Statistically significant differences are marked by A and  for the p < 0.05 and the p < 0.01 level, respectively.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
"(Koehn et al., 2007)",W15-2518,"Note that Gen&Topic contains one reference translation per sentence, while NIST has four sets of reference translations.","We perform our experiments using an inhouse phrase-based SMT system similar to Moses (Koehn et al., 2007).","All runs use lexicalized reordering, distinguishing between monotone, swap, and discontinuous reordering, with respect to the previous and next phrase (Koehn et al., 2005).",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf
(2001),W10-1824,"Through the development of an annotation scheme and the use of spoken and written corpora, we aim to determine different functions of demonstratives and to examine their distributional properties.",Our corpus study adopts similar features of annotation used in Botley and McEnery (2001) and provides some linguistic hypotheses on grammatical functions of Korean demonstratives to be further explored.,1 Introduction,http://www.aclweb.org/anthology/W/W10/W10-1824.pdf
"(Chang, 1980",W10-1824,"Thus, distinct usage of ce and ku is associated with how the speaker allocates the deictic center and contextual space, i.e., the speakercentered space vs. the speaker- and the hearer- centered space.","In contrast with deictic usage, previous studies (Chang, 1980; Chang, 1984) assumed that anaphoric demonstratives show only a two-way distinction between proximal forms i and distal forms ku.","However, it is still controversial as to whether the boundaries between anaphora and deixis are clear cut.",http://www.aclweb.org/anthology/W/W10/W10-1824.pdf
(1979),W10-2109,"First, the contradictory nature of the possible meanings has been explained in terms of pragmatic factors concerning the relevant presuppositions of the sentences.","According to Wason and Reich (1979) (as explained in more detail below), sentences such as (2) are actually nonsensical, but people coerce them into a sensible reading by reversing the interpretation.",One of our goals in this work is to explore whether computational linguistic techniques-specifically automatic corpus analysis drawing on lexical resources-can help to elucidate the factors influencing interpretation of such sentences across a collection of actual usages.,http://www.aclweb.org/anthology/W/W10/W10-2109.pdf
(2008),W10-2109,"The majority judgements of these annotators are the same as those obtained from AMT on the development data, giving us confidence in the reliability of the AMT judgements.",These findings are consistent with those of Snow et al (2008) in showing that AMT judgements can be as reliable as those of expert judges.,"Finally, we remove a small number of items from the testing dataset which were difficult to paraphrase due to ellipsis of the verb participating in the target construction, or an extra negation in the verb phrase.",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf
(1996),W10-2109,"Although not intended as proposing a cognitive model, the work here can be seen as connected to these lines of research, in investigating the extent to which lexical factors can be used as proxies to more ""hidden"" features that underlie the appropriate interpretation of a pragmatically complex construction.","Moreover, as in the approach of Jurafsky (1996), the phenomenon we investigate here may be best considered within a constructional analysis (e.g., Langacker, 1987), in which both the syntactic construction and the particular lexical items contribute to the determination of the meaning of a usage.","We suggest that a clause of the form No Xis too Y to Z might be the (identical) surface expression of two underlying constructions-one with the ""every"" interpretation and one with the ""no"" interpretation-which place differing constraints on the semantics of the verb.",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf
"Burton et al., 2009)",W10-2109,"While we do not adopt the view of some that usages of the target construction having the ""no"" interpretation are errors, it could be the case that such usages are more frequent in less formal text.","In the future we also intend to extract usages of the target construction from datasets of less formal text, such as blogs (e.g., Burton et al., 2009).",Constructions other than No X is too Y to Z exhibit a similar ambiguity.,http://www.aclweb.org/anthology/W/W10/W10-2109.pdf
(2004),W10-2109,We are particularly interested in further exploring the hypothesis that it is the semantics of the component verb that gives rise to the meaning of the target construction.,"Recall Pullum's (2004) observation that the verb in the ""no"" interpretation involves explicitly not acting.","Using this intuition, we have informally observed that it is largely possible to (manually) predict the interpretation of the target construction knowing only the component verb.",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf
(1979),W10-2109,"Note that such a constructional analysis of this phenomenon assumes that both interpretations of these sentences are linguistically valid, given the appropriate lexical instantiation.","This stands in contrast to the analysis of Wason and Reich (1979), which presumes that people are applying some higher-level reasoning to ""correct"" an ill-formed statement in the case of the ""no"" in",67,http://www.aclweb.org/anthology/W/W10/W10-2109.pdf
"(Burnard, 2000)",W10-2109,62,"get construction-we use two corpora: the British National Corpus (Burnard, 2000), an approximately one hundred million word corpus of latetwentieth century British English, and The New York Times Annotated Corpus (Sandhaus, 2008), approximately one billion words of non-newswire text from the New York Times from the years 1987-2006.","We extract all sentences in these corpora containing the sequence of strings no, is too, and to separated by one or more words.",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf
"(Fellbaum, 1998)",W10-2109,"To test our hypothesis that the interaction of the semantics of the noun, adjective, and verb in the target construction contributes to its pragmatic interpretation, we represent each instance in our dataset as a vector of features that capture aspects of the semantics of its component words.","WordNet To tap into general lexical semantic properties of the words in the construction, we use features that draw on the semantic classes of words in WordNet (Fellbaum, 1998).","These binary features each represent a synset in WordNet, and are turned on or off for the component words (the noun, adjective, and verb) in each instance of the target construction.",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf
"(Bird et al., 2009)",W10-2109,"A synset feature is on for a word if the synset occurs on the path from all senses of the word to the root, and off otherwise.","We use WordNet version 3.0 accessed using NLTK version 2.0 (Bird et al., 2009).","Polarity Because of the observation that the verb in the target construction, in particular, has some property of negativity in the ""no"" interpretation, we also use features representing the semantic polarity of the noun, adjective, and verb in each instance.",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf
"Brill, 1992]",W99-0707,"We also present the results of [Argamon et at., 1998], [Ramshaw and Marcus, 1995] and [Cardie and Pierce, 1998] in Table 4.","The latter two use a transformation-based error-driven learning method [Brill, 1992].","In [Ramshaw and Marcus, 1995], the method is used for NP chunking, and in [Cardie and Pierce, 1998] the approach is indirectly used to evaluate corpus-extracted NP chunking rules.",http://www.aclweb.org/anthology/W/W99/W99-0707.pdf
(2007),D08-1058,"Instead of using only the limited Chinese knowledge, this study aims to improve Chinese sentiment analysis by making full use of bilingual knowledge in an unsupervised way, including both Chinese resources and English resources.","Generally speaking, there are two unsupervised scenarios for ""borrowing"" English resources for sentiment analysis in other languages: one is to generate resources in a new language by leveraging on the resources available in English via cross-lingual projections, and then perform sentiment analysis in the English language based on the generated resources, which has been investigated by Mihalcea et al (2007); the other is to translate the texts in a new language into English texts, and then perform sentiment analysis in the English language, which has not yet been investigated.","In this study, we first translate Chinese reviews into English reviews by using machine translation services, and then identify the sentiment polarity of English reviews by directly leveraging English resources.",http://www.aclweb.org/anthology/D/D08/D08-1058.pdf
(2008),D08-1058,"Blitzer et al (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products.",Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting.,"Research work focusing on Chinese sentiment analysis includes (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007; Wang et al., 2007).",http://www.aclweb.org/anthology/D/D08/D08-1058.pdf
"(Bangalore et al., 2007",W10-3805,"In contrast to a phrase-based model, a discriminative model has the power to integrate much richer contextual information into the training model.","Contextual information is extremely useful in making lexical selections of higher quality, as illustrated by the models for Global Lexical Selection (Bangalore et al., 2007; Venkatapathy and",34,http://www.aclweb.org/anthology/W/W10/W10-3805.pdf
"(Papineni et al., 1998",W10-3805,"In this section, we present approaches that are directly related to our approach.","In Direct Translation Model (DTM) proposed for statistical machine translation by (Papineni et al., 1998; Och and Ney, 2002), the authors present a discriminative set-up for natural language understanding (and MT).",They use a slightly modified equation (in comparison to IBM models) as shown in equation 1.,http://www.aclweb.org/anthology/W/W10/W10-3805.pdf
"(Hassan et al., 2009)",W10-3805,"e = arg max Pr(e|f) e M (3) = arg max mm(f, e) e m=1","The DTM2 model differs from other phrasebased SMT models in that it avoids the redundancy present in other systems by extracting from a word aligned parallel corpora a set of minimal phrases such that no two phrases overlap with each other (Hassan et al., 2009).","The decoding strategy in DTM2 (Ittycheriah and Roukos, 2007) is similar to a phrase-based decoder except that the score of a particular translation block is obtained from the maximum entropy model using the set of feature functions.",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf
"(Chen and Goodman, 1999)",W10-3805,We trained a language model of order 5 built on the entire EUROPARL corpus using the SRILM package.,"The method uses improved Kneser-Ney smoothing algorithm (Chen and Goodman, 1999) to compute sequence probabilities.",5 Dataset,http://www.aclweb.org/anthology/W/W10/W10-3805.pdf
"(Koehn et al., 2007)",W10-3805,We used maximum entropy model in our experiments.,"We show that the performance of our approach (using simple lexical features) is comparable to that of the state-of-art statistical MT system (Koehn et al., 2007).","When additional syntactic features (POS tags in this paper) are used, there is a boost in the performance which is likely to improve when richer syntactic features are incorporated in the model.",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf
"(Ittycheriah and Roukos, 2007)",W10-3805,"The DTM2 model differs from other phrasebased SMT models in that it avoids the redundancy present in other systems by extracting from a word aligned parallel corpora a set of minimal phrases such that no two phrases overlap with each other (Hassan et al., 2009).","The decoding strategy in DTM2 (Ittycheriah and Roukos, 2007) is similar to a phrase-based decoder except that the score of a particular translation block is obtained from the maximum entropy model using the set of feature functions.","In our approach, instead of providing the complete scoring function ourselves, we compute the parameters needed by a phrase based decoder, which in turn uses these parameters appropriately.",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf
"(Bangalore et al., 2007",W10-3805,These classifiers predict if a particular target block should be present given the source word and its context.,"This model is similar to the global lexical selection (GLS) model described in (Bangalore et al., 2007; Venkatapathy and Bangalore, 2009) except that in GLS, the predicted target blocks are not associated with any particular source word unlike the case here.","For the set of experiments in this paper, we used a context of size 6, containing three words to the left and three words to the right.",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf
"(Haffner, 2006)",W10-3805,"Similarly, we call the reordering model, a 'context dependent block distortion model'.","For training, we use the maximum entropy software library Llama presented in (Haffner, 2006).",3.1.1 Context Dependent Block Translation Model,http://www.aclweb.org/anthology/W/W10/W10-3805.pdf
"(Sacks et al., 1974)",W09-3953,357,"tween an initiating speech act and a responding one-the analog of adjacency pairs (Sacks et al., 1974).","The most closely related effort is (Galley et al., 2004), which aims to automatically identify adjacency pairs in the ICSI Meeting corpus, a large corpus of 75 meetings, using a small tagset.",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf
"(Stolcke et al., 2000)",W09-3953,"Their maximum entropy ranking approach achieved 90% accuracy on the 4-way classification into agreement, disagreement, backchannel and other.","Using the switchboard corpus, (Stolcke et al., 2000) achieved good dialogue act labeling accuracy (71% on manual transcriptions) for a set of 42 dialogue act types, and constructed probabilistic models of dialogue act sequencing in order to test the hypothesis that dialogue act sequence information could boost speech recognition performance.",There has been far less work on developing manual and automatic dialogue act annotation schemes for email.,http://www.aclweb.org/anthology/W/W09/W09-3953.pdf
"(Cochran, 1950)",W09-3953,"We have previously shown (Passonneau and Litman, 1997) that intention-based segmentation can be done reliably by multiple annotators.","For twenty narratives each segmented by the same seven annotators, using Cochran's Q (Cochran, 1950), we found the probabilities associated with the null hypothesis that the observed distributions could have arisen by chance to be at or below p=0.1 x106.",Partitioning Q by number of annotators gave significant results for all values of A ranging over the number of annotators apart from A = 2.,http://www.aclweb.org/anthology/W/W09/W09-3953.pdf
"(Sacks et al., 1974)",W09-3953,"DFU Links, or simply Links, correspond to adjacency pairs, but need not be adjacent.","A forward link (Flink) is the analog of a ""first pair-part"" of an adjacency pair (Sacks et al., 1974), and is similarly restricted to specific speech act types.",All Request-Information and Request-Action DFUs are assigned Flinks.,http://www.aclweb.org/anthology/W/W09/W09-3953.pdf
"(Taskar et al., 2003",W09-3953,"Taskar, 2007) such as link prediction.","However, Structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2005) is also a compelling method which has the potential to handle the interdependence between labeling and sequencing, due to its ability to handle dependencies among features and prediction results within the structure.","sequence labeling (Tsochantaridis et al., 2005).",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf
"(Traum and Heeman, 1996",W09-3953,"A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses.","The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as the contrast between human-human and human-computer dialogues (Doran et al., 2001).","While there is a large degree of overlap across schemes, the set of DA types will differ due to differences in the nature of the communicative goals; thus information-seeking versus task-oriented dialogues differ in the set of speech acts and their relative frequencies.",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf
"(Yeh and Harnly, 2006)",W09-3953,"Most of the emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely social emails.","We used a version of the corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006).",The annotator of the majority of the Loqui corpus also annotated the Enron corpus.,http://www.aclweb.org/anthology/W/W09/W09-3953.pdf
(1982),J88-3005,"The major problem with these systems is due to their inability to reason about the misconception itself, they are completely at a loss when faced with a misconception absent from their a priori listing.","The work of Sleeman (1982) on inferring defective algebra rules (mal-rules) is based on the observation that the a priori listing of misconceptions is a difficult, if not impossible, task.",Sleeman proposes on-line inference of mal-rules based on the answer the student has given to a particular problem.,http://www.aclweb.org/anthology/J/J88/J88-3005.pdf
(1982),J88-3005,The highlighting and similarity metric used by ROMPER will be discussed below.,This method for correcting misconceptions suggests a model of natural language generation that is similar to that put forth by McKeown (1982) but which differs from McKeown's model in several ways.,Both McKeown and this work concentrate on determining the content and textual shape of a response.,http://www.aclweb.org/anthology/J/J88/J88-3005.pdf
(1977),J88-3005,A second major problem with a similarity metric based on distance in the generalization hierarchy is that it is context invariant; contextual information has no way of affecting the assessments.,"As shown by Tversky (1977) and others, human judgments of object similarity have been found to shift both when the set of objects under discussion are altered (e.g., a violin and an electric guitar may be judged quite similar when in a group with a clarinet and an oboe, and may be judged quite different when the other members of the group are Computational Linguistics, Volume 14, Number 3, September 1988 57 Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions a cello and an electric bass), and when the salience of attributes are altered (e.g., in a group containing a red triangle, a blue triangle, and a red square, the red triangle might be judged similar to the blue triangle when attribute shape is stressed, but may be judged similar to the red square when attribute color is stressed).",One metric that avoids these problems was introduced by Tversky (1977).,http://www.aclweb.org/anthology/J/J88/J88-3005.pdf
(2012),P13-1028,verbs become leaves instead of governing the sentences.,"Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed.","The main contribution of this paper is a considerable improvement of unsupervised parsing quality by estimating the Pstop probabilities externally using a very large corpus, and employing this prior knowledge in the standard inference of DMV.",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf
(2004),P13-1028,Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years.,It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al (2008).,Headden III et al (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing.,http://www.aclweb.org/anthology/P/P13/P13-1028.pdf
"(Marecek and Zabokrtsky, 2012)",P13-1028,3.1 Recognition of reducible sequences,"We introduced a simple procedure for recognition of reducible sequences in (Marecek and Zabokrtsky, 2012): The particular sequence of words is removed from the sentence and if the remainder of the sentence exists elsewhere in the corpus, the sequence is considered reducible.",We provide an example in Figure 2.,http://www.aclweb.org/anthology/P/P13/P13-1028.pdf
"(Gilks et al., 1996)",P13-1028,5 Inference,"We employ the Gibbs sampling algorithm (Gilks et al., 1996).","Unlike in (Marecek and Zabokrtsky, 2012), where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming.",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf
"(Marecek and Zabokrtsky, 2012)",P13-1028,"We employ the Gibbs sampling algorithm (Gilks et al., 1996).","Unlike in (Marecek and Zabokrtsky, 2012), where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming.",The algorithm works as follows:,http://www.aclweb.org/anthology/P/P13/P13-1028.pdf
"(Klein and Manning, 2004)",P13-1028,4 Model,"We use the standard generative Dependency Model with Valence (Klein and Manning, 2004).","The generative story is the following: First, the head of the sentence is generated.",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf
"(Chu and Liu, 1965)",P13-1028,"After each iteration is finished (all the trees in the corpus are re-sampled), we increment the counter of all directed pairs of nodes which are connected by a dependency edge in the current trees.","After the last iteration, we use these collected counts as weights and compute maximum directed spanning trees using the Chu-Liu/Edmonds' algorithm (Chu and Liu, 1965).","Therefore, the resulting trees consist of edges maximizing the sum of individual counts:",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf
"(Buchholz and Marsi, 2006)",P13-1028,We use two types of resources in our experiments.,"The first type are CoNLL treebanks from the year 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007), which we use for inference and for evaluation.","As is the standard practice in unsupervised parsing evaluation, we removed all punctuation marks from the trees.",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf
"(Majlis and Zabokrtsky, 2012)",P13-1028,"In case a punctuation node was not a leaf, its children are attached to the parent of the removed node.","For estimating the STOP probabilities (Section 3), we use the Wikipedia articles from W2C corpus (Majlis and Zabokrtsky, 2012), which provide sufficient amount of data for our purposes.",Statistics across languages are shown in Table 1.,http://www.aclweb.org/anthology/P/P13/P13-1028.pdf
(2006),N07-1064,"Allen and Hogan (2000) sketch the outline of such an automated post-editing (APE) system, which would automatically learn post-editing rules from a tri-parallel corpus of source, raw MT and post-edited text.","Elming (2006) suggests using tranformation-based learning to automatically acquire error-correcting rules from such data; however, the proposed method only applies to lexical choice errors.","Knight and Chander (1994) also argue in favor of using a separate APE module, which is then portable across multiple MT systems and language pairs, and suggest that the post-editing task could be performed using statistical machine translation techniques.",http://www.aclweb.org/anthology/N/N07/N07-1064.pdf
"(Brown et al., 1993)",N07-1064,no new translations are produced at the decoding step.,"To improve raw output from decoding, Portage relies on a rescoring strategy: given a list of n-best translations from the decoder, the system reorders this list, this time using a more elaborate loglinear model, incorporating more feature functions, in addition to those of the decoding model: these typically include IBM-1 and IBM-2 model probabilities (Brown et al., 1993) and an IBM-1-based feature function designed to detect whether any word in one language appears to have been left without satisfactory translation in the other language; all of these feature functions can be used in both language directions, i.e.",source-to-target and target-to-source.,http://www.aclweb.org/anthology/N/N07/N07-1064.pdf
(2004),N07-1064,"Decoding is the central phase in SMT, involving a search for the hypotheses t that have highest probabilities of being translations of the current source sentence s according to a model for P(t1s).","Portage implements a dynamic programming beam search decoding algorithm similar to that of Koehn (2004), in which translation hypotheses are constructed by combining in various ways the target-language part of phrase pairs whose sourcelanguage part matches the input.",These phrase pairs come from large phrase tables constructed by collecting matching pairs of contiguous text segments from word-aligned bilingual corpora.,http://www.aclweb.org/anthology/N/N07/N07-1064.pdf
"(Foster et al., 2006)",N07-1064,"Portage's model for P(t1s) is a log-linear combination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature.","The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P(s|t) are smoothed using the Good-Turing technique (Foster et al., 2006).","The distortion model is also very similar to Koehn's, with the exception of a final cost to account for sentence endings.",http://www.aclweb.org/anthology/N/N07/N07-1064.pdf
"(Brown et al., 1993)",E06-1005,"As in statistical machine translation, we make modelling assumptions.","We use the IBM Model 1 (Brown et al., 1993) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996)) to estimate the alignment model.",The lexicon probability of a sentence pair is modelled as a product of single-word based probabilities of the aligned words.,http://www.aclweb.org/anthology/E/E06/E06-1005.pdf
"(TC-STAR, 2005)",E06-1005,"Here, the involved MT systems had used about 60K sentence pairs (420K running words) for training.","Finally, we also computed consensus translation from some of the submissions to the TC-STAR 2005 evaluation campaign (TC-STAR, 2005).",The TC-STAR participants had submitted translations of manually transcribed speeches from the European Parliament Plenary Sessions (EPPS).,http://www.aclweb.org/anthology/E/E06/E06-1005.pdf
"Kaplan et al., 1989)",E91-1050,"The first and last of these are to be performed by parsing and generation with natural language grammars, but, while proposals have been made to combine some of the three stages (e.g.","Kaplan et al., 1989), there are advantages in treating the intermediate, transfer, stage independently.","As an example, consider the FSs shown below:6",http://www.aclweb.org/anthology/E/E91/E91-1050.pdf
"(Shieber, 1986)",E91-1050,Conclusion,"We have presented what is to our knowledge the first formalization and implementation of a type of rule and control regime intended for use in situations where it is desired to produce the effect of transforming one feature structure into another.9 The formalism described above has been implemented as part of ISSCO's ELUM, an enhanced PATR-II style (Shieber, 1986) unification grammar environment, based on the UD system presented by Johnson and Rosner (1989).","ELU incorporates a parser and generator, and is primarily intended for use as a tool for research in machine translation.",http://www.aclweb.org/anthology/E/E91/E91-1050.pdf
"(Freund and Schapire, 1997)",W01-0726,2 System Architecture,"AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate.","In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions.",http://www.aclweb.org/anthology/W/W01/W01-0726.pdf
"(Schapire and Singer, 1999)",W01-0726,"AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate.","In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions.","This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such as: POS tagging, PP-attachment disambiguation, text categorization, and word sense disambiguation.",http://www.aclweb.org/anthology/W/W01/W01-0726.pdf
"(Schapire and Singer, 1999)",W01-0726,"This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such as: POS tagging, PP-attachment disambiguation, text categorization, and word sense disambiguation.","In our particular setting, weak rules are extended to arbitrarily deep decision trees following the suggestion of (Schapire and Singer, 1999) and the definition presented in (Carreras and Marquez, 2001).","These more complex weak rules allow the algorithm to work in a higher dimensional feature space that contains conjunctions of simple features, and this fact has turned out to be crucial for improving results in the present domain.",http://www.aclweb.org/anthology/W/W01/W01-0726.pdf
(2001),W05-0618,the costs of selecting a pair of labels for two related objects3.,"Chekuri et al (2001) proposed an integer linear programming (ILP) formulation of the metric labeling problem, with both assignment cost and separation costs being modeled as binary variables of the linear cost function.","Recently, Roth & Yih (2004) applied an ILP model to the task of the simultaneous assignment of semantic roles to the entities mentioned in a sentence and recognition of the relations holding between them.",http://www.aclweb.org/anthology/W/W05/W05-0618.pdf
(1991),W15-0302,Early discussion about interpreting epistemic phrases as hedges originated in the analysis I think.,"Particularly, Thompson and Mulac (1991) consider this epistemic phrase has achieved a hedging state through a process of grammaticalization.","Their view is that I think is roughly similar to maybe when used to express the degree of speaker commitment, thus comprising a grammatical sub-category of adverbs.",http://www.aclweb.org/anthology/W/W15/W15-0302.pdf
"(Scheibman, 2001)",W15-0302,"Scheibman (2001), Karkkainen (2010) and Wierzbicka (2006) showed that first person epistemic phrases used to express personal stance are highly frequent in various registers in contemporary English.","They also describe particular properties of these phrases such as: a) representing the speaker's attitude with respect to the subsequent piece of discourse in contrast to when third person is used, there the piece of discourse is seen as a description (Scheibman, 2001), b) it comprises explicitly subjective claims in contrast to impersonal expressions where the hedging source is obscure (Hyland, 1998) , c) used to express knowledge states, as a boundary marker for turn-taking in conversation, a speaker's perspective marker, and as a way to align the speaker's with the listener's stance (Karkkainen, 2010).","Besides, Wierzbicka (2006) suggests that this category of phrases merits recognition as a major grammatical and semantic class in modern English.",http://www.aclweb.org/anthology/W/W15/W15-0302.pdf
"Strong, Lee, Wang, 1997, ",N03-2033,Table 1.,"Information Quality Dimensions (Source: Strong, Lee, Wang, 1997, p.39)",Empirical attempts to assess quality have primarily focused on counting hyperlinks in a networked environment.,http://www.aclweb.org/anthology/N/N03/N03-2033.pdf
(2010),W11-2507,1 Background,The categorical distributional compositional model of meaning of Coecke et al (2010) combines the modularity of formal semantic models with the empirical nature of vector space models of lexical semantics.,"The meaning of a sentence is defined to be the application of its grammatical structure- represented in a type-logical model-to the kronecker product of the meanings of its words, as computed in a distributional model.",http://www.aclweb.org/anthology/W/W11/W11-2507.pdf
(2008),W11-2507,This degree decreases for the pair 'the child met the house' and 'the child satisfied the house'.,"Dataset The dataset is built using the same guidelines as Mitchell and Lapata (2008), using transitive verbs obtained from CELEX1 paired with subjects and objects.",We first picked 10 transitive verbs from the most frequent verbs of the BNC.,http://www.aclweb.org/anthology/W/W11/W11-2507.pdf
(2008),W11-2507,"The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011).",Parameters We used the parameters described by Mitchell and Lapata (2008) for the noun and verb vectors.,All vectors were built from a lemmatised version of the BNC.,http://www.aclweb.org/anthology/W/W11/W11-2507.pdf

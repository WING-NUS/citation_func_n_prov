_unit_id,_golden,_unit_state,_trusted_judgments,_last_judgment_at,what_is_the_function_of_this_citation,what_is_the_function_of_this_citation:confidence,article,current,link,marker,next,previous,what_is_the_function_of_this_citation_gold
1139042743,false,finalized,3,1/23/2017 16:24:49,Neut,1.0,W14-0118,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Fellbaum, 1998)","Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",1 Introduction,
1139042744,false,finalized,3,1/23/2017 16:04:32,Neut,1.0,W14-0118,"Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Rada et al., 1989)",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).",
1139042745,false,finalized,3,1/23/2017 16:27:50,Neut,1.0,W14-0118,"Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Leacock and Chodorow, 1998)",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).",
1139042746,false,finalized,3,1/23/2017 16:40:15,Neut,1.0,W14-0118,"Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Wu and Palmer, 1994)",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).",
1139042747,false,finalized,3,1/23/2017 17:07:28,Neut,1.0,W14-0118,"Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Resnik, 1995)",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).",
1139042748,false,finalized,3,1/23/2017 16:48:36,Neut,1.0,W14-0118,"Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Lin, 1998)",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).",
1139042749,false,finalized,3,1/23/2017 16:07:49,Neut,1.0,W14-0118,"Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Jiang and Conrath, 1997)",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).",
1139042750,false,finalized,3,1/23/2017 16:28:16,Neut,1.0,W14-0118,"Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Pedersen et al., 2004)",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).",
1139042751,false,finalized,3,1/23/2017 16:13:12,Neut,0.6692,W14-0118,The toolkit was used to evaluate the different measures against a gold standard of English words created by Rubenstein and Goodenough (1965) and Miller and Charles (1991).,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(1965),The evaluation results tell us something about the capacity of WordNet to mimic human judgements of similarity but also about the different methods in relation to each other.,WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,
1139042752,false,finalized,3,1/24/2017 05:53:28,Neut,0.6791,W14-0118,The toolkit was used to evaluate the different measures against a gold standard of English words created by Rubenstein and Goodenough (1965) and Miller and Charles (1991).,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(1991),The evaluation results tell us something about the capacity of WordNet to mimic human judgements of similarity but also about the different methods in relation to each other.,WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,
1139042753,false,finalized,3,1/23/2017 16:56:36,Neut,0.6664,W14-0118,"Unfortunately, WordNet::Similarity only works for the Princeton WordNet released in its proprietary format and not wordnets in other languages in other formats, such as Wordnet-LMF (Vossen, Soria and Monachini, 2013).",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Vossen, Soria and Monachini, 2013)","Furthermore, no gold standard exists for Dutch, the language that we study.",The evaluation results tell us something about the capacity of WordNet to mimic human judgements of similarity but also about the different methods in relation to each other.,
1139042754,false,finalized,3,1/23/2017 16:44:58,Pos,0.6566,W14-0118,"In section 5, we report the results using the Dutch wordnet Cornetto 2.1 (Vossen et al., 2013).",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Vossen et al., 2013)",2 Related work,Section 3 explains how we created the Dutch gold standard and section 4 the WordnetTools implementation of the similarity functions.,
1139042755,false,finalized,3,1/23/2017 16:28:16,Neut,0.6533,W14-0118,"This difference is dubbed the 'tennisphenomenon' in Fellbaum (1998) : where tennis ball, player, racket and game are closely related but all very different things.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(1998),"Since WordNet dominantly consists of synonymy and hyponymy relations, it more naturally reflects similarity than relatedness.","part-whole or causal relations, are most likely not similar but strongly related.",
1139042756,false,finalized,3,1/23/2017 16:04:32,Neut,1.0,W14-0118,"Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Lesk, 1986)",Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011).,"Since the first release of WordNet, researchers have tried to use it to simulate similarity.",
1139042757,false,finalized,3,1/23/2017 16:48:36,Neut,1.0,W14-0118,"Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Patwardhan and Pedersen, 2006)",Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011).,"Since the first release of WordNet, researchers have tried to use it to simulate similarity.",
1139042758,false,finalized,3,1/23/2017 16:32:37,Neut,1.0,W14-0118,"Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Patwardhan and Pedersen, 2006)",Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011).,"Since the first release of WordNet, researchers have tried to use it to simulate similarity.",
1139042759,false,finalized,3,1/23/2017 16:07:49,Neut,1.0,W14-0118,Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011).,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(2011),"The aim of their paper is to show that it might be possible to use the scores from the English gold standards in other languages, hence making it unnecessary to create gold standards with human-assigned judgements in every single language.","Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.",
1139042760,false,finalized,3,1/23/2017 16:56:36,Neut,1.0,W14-0118,"Besides Joubarne and Inkpen (2011), other studies have made an effort to translate the original datasets by Rubenstein & Goodenough and by Miller & Charles.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(2011),"Hassan and Mihalcea (2009) translated these datasets into Spanish, Arabic, and Romanian.","The difference between these correlations was relatively small, which is why they claim that it is possible to use the original scores from the English gold standard in other languages.",
1139042761,false,finalized,3,1/23/2017 16:13:12,Neut,1.0,W14-0118,"Hassan and Mihalcea (2009) translated these datasets into Spanish, Arabic, and Romanian.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(2009),"For Spanish, native speakers, who were highly proficient in English, were asked to translate the datasets.","Besides Joubarne and Inkpen (2011), other studies have made an effort to translate the original datasets by Rubenstein & Goodenough and by Miller & Charles.",
1139042762,false,finalized,3,1/23/2017 16:28:54,Neut,1.0,W14-0118,"Finally, Gurevych (2005) translated the datasets into German.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(2005),"However, no instructions, as to how it was done, were provided.","Because of the fact that the Pearson correlation with the original datasets was 0.86, only one translator translated the datasets into Arabic and Romanian.",
1139042763,false,finalized,3,1/23/2017 16:15:39,Neut,0.6824,W14-0118,"Inspired by Hassan and Mihalcea (2009), the following general procedure is followed in the translation of the 49 words: 2",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(2009),1.,"Whenever Rubenstein & Goodenough used the word cord, Miller & Charles uses the word chord.",
1139042764,false,finalized,3,1/23/2017 16:27:50,Pos,0.6528,W14-0118,"In order to calculate relative frequencies of the English words, the English sense-tagged corpus SemCor (Miller et al., 1993) was used.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Miller et al., 1993)","For Dutch, such a resource was not available.","In addition, the relative frequencies of the English word and its translation were checked.",
1139042765,false,finalized,3,1/23/2017 16:28:54,Neut,1.0,W14-0118,"We are aware of the fact that the Dutch sense-tagged corpus DutchSemCor (Vossen et al., 2012) exists.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Vossen et al., 2012)","However, an effort was made to provide an equal number of examples for each meaning in this corpus.","For Dutch, such a resource was not available.",
1139042766,false,finalized,3,1/23/2017 16:46:09,Neut,0.6825,W14-0118,"Therefore the frequencies of the lemmas in the Dutch corpus called SoNaR (Oostdijk et al., 2008) were used.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Oostdijk et al., 2008)",It was checked whether or not the English word and its Dutch counterpart were located in the same class of relative frequency.,"Although this is very useful for WSD-experiments, this makes this corpus less useful for Information Content calculations.",
1139042767,false,finalized,3,1/23/2017 16:52:43,Pos,0.6528,W14-0118,"The WordSimilarity-353 Test Collection (Finkelstein et al., 2002) was used to obtain example word pairs for each value that could be assigned to a word pair.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,"(Finkelstein et al., 2002)",This dataset contains two sets of English word pairs with similarity scores assigned by humans.,These instructions were explained to the participants by an example of each value that could be assigned to a word pair and a general description.,
1139042768,false,finalized,3,1/23/2017 16:24:49,Neut,0.6729,W14-0118,"Finally, we try to replicate the English experiment by Pedersen (2010) using English Wordnet-LMF and WordnetTools.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(2010),5,"We start by comparing the Dutch to the English gold standards, followed by an evaluation of the comparison between the Dutch gold standards and the similarity measures.",
1139042769,false,finalized,3,1/23/2017 16:52:43,CoCo,0.6528,W14-0118,"In order to do this, we compare the correlations that Pedersen (2010) reports when calculating the correlations between the original gold standards and the scores from the six similarity measures using WordNet::Similarity to the same procedure but using the WordNetTools to compute the similarity scores.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(2010),"We used the following settings for WordNetTools:7 -lmf-file Path to WordNet in LMF format -pos no pos-filter was used -relations has hypernym, has hyperonym, -input path to English gold standards -pairs ""words"" -method all.",can we reproduce the results of WordNet::Similarity with the original WordNet database with WordnetTools with the WordnetLMF version of the English WordNet.,
1139042770,false,finalized,3,1/23/2017 16:09:58,Neut,0.6548,W14-0118,Table 3: Comparison of the results by Pedersen (2010) and the replication of these results using Wordnet-LMF and the WordnetToolkit,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(2010),"7The depth parameter is set to 19, For more information, we refer to section 6.",SM McPed McWT diff RgPed RgWT diff path 0.68 0.72 -0.04 0.69 0.78 -0.09 lch 0.71 0.72 -0.01 0.70 0.78 -0.08 wup 0.74 0.74 0.00 0.69 0.78 -0.09 res 0.74 0.75 -0.01 0.69 0.76 -0.07 jcn 0.72 0.65 0.07 0.51 0.56 -0.05 lin 0.73 0.67 0.06 0.58 0.60 -0.02,
1139042771,false,finalized,3,1/23/2017 16:42:25,Pos,0.6566,W14-0118,"The results show that for both gold standards, we approach the correlations that are reported by Pedersen (2010), but that there are probably still differences in the implementation of the measures that lead to different output values.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(2010),6 Discussion,"7The depth parameter is set to 19, For more information, we refer to section 6.",
1139042772,false,finalized,3,1/23/2017 16:15:39,Neut,1.0,W14-0118,"Given the fact that this was also the case for the Spanish and English intuitions, as discussed by Hassan and Mihalcea (2009), it might be the case the people with different mother tongues have a shared sense of similarity of meaning.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(2009),It should be noted that all speakers from the different languages share a similar Western background.,"Firstly, the correlations between the English and Dutch gold standards are very high.",
1139042773,false,finalized,3,1/23/2017 16:59:13,Neut,1.0,W14-0118,"This in itself is not surprising, given the fact that Fokkens et al (2013) showed that even replicating the results that Pedersen (2010) reports can be challenging.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(2013),"They showed that even if the main properties are kept stable, such as software and versions of software, variations in minor properties can lead to completely different outcomes.","Finally, the differences between the scores from the WordNet::Similarity package and the WordNetTools show that we did not reproduce the results exactly.",
1139042774,false,finalized,3,1/23/2017 16:44:58,Neut,1.0,W14-0118,"This in itself is not surprising, given the fact that Fokkens et al (2013) showed that even replicating the results that Pedersen (2010) reports can be challenging.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf,(2010),"They showed that even if the main properties are kept stable, such as software and versions of software, variations in minor properties can lead to completely different outcomes.","Finally, the differences between the scores from the WordNet::Similarity package and the WordNetTools show that we did not reproduce the results exactly.",
1139042775,false,finalized,3,1/23/2017 16:30:31,Pos,1.0,S13-2019,"One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Fellbaum, 1998)",Our phrasal semantic relatedness approach is inspired from those methods.,Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.,
1139042776,false,finalized,3,1/23/2017 16:32:37,Neut,0.7042,S13-2019,"One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Leacock and Chodorow, 1998",Our phrasal semantic relatedness approach is inspired from those methods.,Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.,
1139042777,false,finalized,3,1/23/2017 16:24:49,Neut,0.6729,S13-2019,"One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"Wu and Palmer, 1994)",Our phrasal semantic relatedness approach is inspired from those methods.,Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.,
1139042778,false,finalized,3,1/24/2017 05:52:54,Neut,0.6497,S13-2019,"One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Hirst and St-Onge, 1998)",Our phrasal semantic relatedness approach is inspired from those methods.,Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.,
1139042779,false,finalized,3,1/23/2017 16:19:04,Neut,1.0,S13-2019,"One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Tsatsaronis et al., 2010)",Our phrasal semantic relatedness approach is inspired from those methods.,Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.,
1139042780,false,finalized,3,1/23/2017 16:04:32,Neut,0.6548,S13-2019,"ponym/hypernym relations but also all 26 available semantic relations found in WordNet in addition to relations extracted from each of the eXtended WordNet (Harabagiu et al., 1999) synset's logical form.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Harabagiu et al., 1999)","To implement our idea, we created a weighted and directed semantic network based on the relations of WordNet and eXtended WordNet.",Figure 1: Example of the semantic network around the word car.,
1139042781,false,finalized,3,1/23/2017 17:07:28,Pos,0.6441,S13-2019,"2The shortest path is based on an implementation of Dijkstras graph search algorithm (Dijkstra, 1959)",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Dijkstra, 1959)",109,1The weight can be seen as the cost of traversing an edge; hence a lower weight is assigned to a highly contributory relation.,
1139042782,false,finalized,3,1/23/2017 16:47:20,Neut,1.0,S13-2019,"Distributional similarity models rely on the distributional hypothesis (Harris, 1954) to represent a word by its context in order to compare word semantics.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Harris, 1954)","There are various approach for the selection, representation, and comparison of contextual data.",2.2 Distributional Similarity Model,
1139042783,false,finalized,3,1/23/2017 16:46:09,Neut,0.6825,S13-2019,"To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,(2008),"In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008).","Most use the vector space model to represent the context as dimensions in a vector space, where the feature are frequency of co-occurrence of the context words, and the comparison is usually the cosine similarity.",
1139042784,false,finalized,3,1/23/2017 16:24:49,Neut,0.6729,S13-2019,"To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,(2008),"In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008).","Most use the vector space model to represent the context as dimensions in a vector space, where the feature are frequency of co-occurrence of the context words, and the comparison is usually the cosine similarity.",
1139042785,false,finalized,3,1/23/2017 16:47:20,Neut,0.6824,S13-2019,"To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,(2011),"In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008).","Most use the vector space model to represent the context as dimensions in a vector space, where the feature are frequency of co-occurrence of the context words, and the comparison is usually the cosine similarity.",
1139042786,false,finalized,3,1/23/2017 16:28:16,Pos,1.0,S13-2019,"In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,(2008),"The compositional model is based on phrase words vectors addition, where each vector is composed of the collocation pointwise mutual information of the word up to a window of 3 words left and right of the main word.","To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).",
1139042787,false,finalized,3,1/23/2017 16:16:48,Neut,1.0,S13-2019,"The corpus used to collect the features and their frequencies is the Web 1TB corpus (Brants and Franz, 2006).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Brants and Franz, 2006)","For the Interview to Formal Meeting example, the vector of the word interview is first created from the corpus of the top 1000 words collocating interview between the window of 1 to 3 words with their frequencies.","The compositional model is based on phrase words vectors addition, where each vector is composed of the collocation pointwise mutual information of the word up to a window of 3 words left and right of the main word.",
1139042788,false,finalized,3,1/23/2017 16:21:10,Neut,0.6738,S13-2019,"To transform the semantic relatedness measure to a semantic similarity classification one, we first calculated the semantic relatedness of each word-phrase in the training set, and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999), in order to learn a set of rules that can differentiate between a positive semantic similarity and a negative one.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Witten et al., 1999)","The classifier resulted in rules for the semantic network model based relatedness that could be summarized as follows: If the semantic relatedness of the word-phrase is over 61% then the similarity is positive, otherwise it is negative.","The task provided an English dataset of 15,628 word-phrases, 60% annotated for training and 40% for testing, with the goal of classifying each word-phrase as either positive or negative.",
1139042789,false,finalized,3,1/23/2017 16:24:11,Pos,0.6795,S13-2019,"To transform the semantic relatedness measure to a semantic similarity classification one, we first calculated the semantic relatedness of each word-phrase in the training set, and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999), in order to learn a set of rules that can differentiate between a positive semantic similarity and a negative one.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Cohen and Singer, 1999)","The classifier resulted in rules for the semantic network model based relatedness that could be summarized as follows: If the semantic relatedness of the word-phrase is over 61% then the similarity is positive, otherwise it is negative.","The task provided an English dataset of 15,628 word-phrases, 60% annotated for training and 40% for testing, with the goal of classifying each word-phrase as either positive or negative.",
1139042790,false,finalized,3,1/23/2017 16:11:13,Neut,0.6657,S13-2019,"Our approach for this task is a supervised approached based on two main components: first, the availability of the phrases most frequent collocating expressions in a large corpus, and more specifically the top 1000 phrases by frequency in Web 1TB corpus (Brants and Franz, 2006).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Brants and Franz, 2006)","For example, for the phrase big picture, we collect the top 1000 phrases that come before and after the phrase in a corpus, those includes look at the, see the, understand the .....","For example, the phrase big picture is used literally in the sentence Click here for a bigger picture and figuratively in To solve this problem, you have to look at the bigger picture.",
1139042791,false,finalized,3,1/23/2017 16:09:58,Neut,1.0,S13-2019,"We use the training data and computed the three features (Frequent Collocation (FC), Semantic Relatedness word Before (SRB), and Semantic Relatedness word After (SRA), and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999) to learn a set of rule that differentiate between a figurative and literal phrase use.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Witten et al., 1999)","This method resulted in a set of rules that can be summarized as follows: if FC is equal to 0 and SRB < 75% then it is used literally in this context, else if FC is equal to 0 and SRA < 75% then it is is also used literally, otherwise it is used figuratively.","The data set contains a total of 1114 training instances, and 518 test instances.",
1139042792,false,finalized,3,1/23/2017 16:47:20,Neut,0.6559,S13-2019,"We use the training data and computed the three features (Frequent Collocation (FC), Semantic Relatedness word Before (SRB), and Semantic Relatedness word After (SRA), and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999) to learn a set of rule that differentiate between a figurative and literal phrase use.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf,"(Cohen and Singer, 1999)","This method resulted in a set of rules that can be summarized as follows: if FC is equal to 0 and SRB < 75% then it is used literally in this context, else if FC is equal to 0 and SRA < 75% then it is is also used literally, otherwise it is used figuratively.","The data set contains a total of 1114 training instances, and 518 test instances.",
1139042793,false,finalized,3,1/23/2017 16:42:25,Neut,0.6566,P08-1113,"One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Nagata et al, 2001, ","Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4).",There have been two approaches to finding such parenthetical translations.,
1139042794,false,finalized,3,1/23/2017 16:46:09,Neut,0.648,P08-1113,"One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"Kwok et al, 2005)","Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4).",There have been two approaches to finding such parenthetical translations.,
1139042795,false,finalized,3,1/23/2017 16:48:36,Neut,1.0,P08-1113,"Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Cao et al, 2007)",We followed the second approach since it does not require a predefined list of English terms and is amendable for extraction at large scale.,"One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005).",
1139042796,false,finalized,3,1/23/2017 16:32:37,Neut,1.0,P08-1113,"The lengths and frequency counts of the suffixes have been used to determine what is the translation of the in-parenthesis term (Kwok et al, 2005).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Kwok et al, 2005)","For example, Table 1 lists a set of Chinese segments (with word-to-word translation underneath) that precede the English term Lower Egypt.","In both cases, one can obtain a list of candidate pairs, where the translation of the in-parenthesis terms is a suffix of the pre-parenthesis text.",
1139042797,false,finalized,3,1/23/2017 16:21:10,Neut,1.0,P08-1113,"Examples with translations in Function of the initalic parenthesis text a ARTi3l9t1.4~3.0ZN to provide citation (MacArthur, 1967) The range of its values is within",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(MacArthur, 1967)","1.4~3.0 (MacArthur, 1967) b cJILS/ (VN901 flight information 15:20-22:30) Vietnam Airlines Beijing/Ho Chi Minh (VN901 15:20-22:30) c ' Af*#(255-8FT) product Id.",The example in row f is ruled out because '/' is not found in the pre-parenthesis text.,
1139042798,false,finalized,3,1/23/2017 16:11:13,Neut,0.6777,P08-1113,"1.4~3.0 (MacArthur, 1967) b cJILS/ (VN901 flight information 15:20-22:30) Vietnam Airlines Beijing/Ho Chi Minh (VN901 15:20-22:30) c ' Af*#(255-8FT) product Id.",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(MacArthur, 1967)","sale of pool table (255-8FT) d // _tV1T_- // void main ( void ) function declaration // main program // void main (void ) e *,*,?","Examples with translations in Function of the initalic parenthesis text a ARTi3l9t1.4~3.0ZN to provide citation (MacArthur, 1967) The range of its values is within",
1139042799,false,finalized,3,1/23/2017 16:15:39,Neut,0.6617,P08-1113,"Determining what should be counted as a term is a difficult task and there are not yet well-accepted solutions (Sag et al, 2003).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Sag et al, 2003)",We compiled an approximate term vocabulary by taking the top 5 million most frequent Chinese queries as according to a fully anonymized collection of search engine query logs.,"Since parenthetical translations are mostly translation of terms, it makes sense to further constrain the left boundary of the Chinese side to be a term boundary.",
1139042800,false,finalized,3,1/23/2017 16:13:12,Neut,0.6548,P08-1113,"Word alignment is a well-studied topic in Machine Translation with many algorithms having been proposed (Brown et al, 1993; Och and Ney 2003).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Brown et al, 1993","We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000).",4 Word Alignment,
1139042801,false,finalized,3,1/23/2017 16:30:31,Pos,0.648,P08-1113,"We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Melamed, 2000)",The algorithm assumes that there is a score associated with each pair of words in a bi-text.,"Word alignment is a well-studied topic in Machine Translation with many algorithms having been proposed (Brown et al, 1993; Och and Ney 2003).",
1139042802,false,finalized,3,1/23/2017 16:21:10,Neut,0.6738,P08-1113,Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest precision scores.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,(2004),"A disadvantage of Competitive Linking, however, is that the alignments are restricted word-to-word alignments, which implies that multi-word expressions can only be partially linked at best.",The algorithm terminates when there are no more links to make.,
1139042803,false,finalized,3,1/23/2017 16:27:50,Pos,0.6566,P08-1113,"We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel.",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Gale and Church, 1991)","2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005).",4.2 Link scoring,
1139042804,false,finalized,3,1/23/2017 16:28:54,Neut,1.0,P08-1113,"2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Dunning, 1993)",The T2 statistics for a pair of words e; and fj is computed as,"We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel.",
1139042805,false,finalized,3,1/23/2017 16:56:36,Neut,1.0,P08-1113,"2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Taskar et al, 2005)",The T2 statistics for a pair of words e; and fj is computed as,"We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel.",
1139042806,false,finalized,3,1/23/2017 16:21:10,Neut,1.0,P08-1113,"Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Cao et al, 2007",They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model.,"For example, the word happens to have a similar 2 score with Shapiro as the word  (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus.",
1139042807,false,finalized,3,1/23/2017 16:30:31,Neut,1.0,P08-1113,"Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"Jiang et al, 2007",They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model.,"For example, the word happens to have a similar 2 score with Shapiro as the word  (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus.",
1139042808,false,finalized,3,1/23/2017 16:16:48,Neut,1.0,P08-1113,"Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"Wu and Chang, 2007)",They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model.,"For example, the word happens to have a similar 2 score with Shapiro as the word  (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus.",
1139042809,false,finalized,3,1/23/2017 16:56:36,Neut,1.0,P08-1113,"To evaluate the coverage of output produced by their method, Cao et al (2007) extracted English queries from the query log of a Chinese search engine.",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,(2007),They assume that the reason why users typed the English queries in a Chinese search box is mostly to find out their Chinese translations.,5.2 Evaluation with term translation requests,
1139042810,false,finalized,3,1/23/2017 16:27:50,CoCo,0.6777,P08-1113,"When compared with the sample queries in (Cao et al., 2007), the queries in our sample seem to contain more phrasal words and technical terminology.",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Cao et al., 2007)","It is interesting to see that even though parenthetical translations tend to be out-of-vocabulary words, as we have remarked in the introduction, the sheer size of the web means that occasionally translations of common words such as 'use' are sometimes included as well.",We use * to mark incorrect translations.,
1139042811,false,finalized,3,1/23/2017 16:28:16,Pos,0.6657,P08-1113,"To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Koehn et al, 2003",We then added the extracted translation pairs as additional parallel training corpus.,The extracted translations may serve as training data for statistical machine translation systems.,
1139042812,false,finalized,3,1/23/2017 16:42:25,Pos,0.6777,P08-1113,"To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"Brants et al, 2007)",We then added the extracted translation pairs as additional parallel training corpus.,The extracted translations may serve as training data for statistical machine translation systems.,
1139042813,false,finalized,3,1/23/2017 16:24:11,Pos,1.0,P08-1113,"To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(NIST, 2003)",We then added the extracted translation pairs as additional parallel training corpus.,The extracted translations may serve as training data for statistical machine translation systems.,
1139042814,false,finalized,3,1/23/2017 16:24:11,Neut,1.0,P08-1113,Nagata et al (2001) made the first proposal to mine translations from the web.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,(2001),"Their work was concentrated on terminologies, and assumed the English terms were given as input.",6 Related Work,
1139042815,false,finalized,3,1/23/2017 16:16:48,Neut,1.0,P08-1113,"Wu and Chang (2007), Kwok et al (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration.",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,(2007),It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by.,"Their work was concentrated on terminologies, and assumed the English terms were given as input.",
1139042816,false,finalized,3,1/23/2017 16:09:58,Neut,1.0,P08-1113,"Wu and Chang (2007), Kwok et al (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration.",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,(2005),It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by.,"Their work was concentrated on terminologies, and assumed the English terms were given as input.",
1139042817,false,finalized,3,1/23/2017 16:07:49,Neut,0.6692,P08-1113,"Cao et al (2007), like us, used a 300GB collection of web documents as input.",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,(2007),They used supervised learning to build models that deal with phonetic transliterations and semantic translations separately.,It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by.,
1139042818,false,finalized,3,1/23/2017 16:30:31,Neut,0.6825,P08-1113,"Furthermore, we are able to extract two orders of magnitude more translations from than (Cao et al., 2007).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf,"(Cao et al., 2007)",7 Conclusion,Our work relies on unsupervised learning and does not make a distinction between translations and transliterations.,
1139042819,false,finalized,3,1/23/2017 16:19:04,Neut,1.0,W93-0235,"[Traum and Hinkelmau, 1992] presents a multistratal theory of Conversation Acts,",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf,"Traum and Hinkelmau, 1992]",This material is based upon work supported in part by the NSF under research grant no.,The cmly difference between speech acts and rhetorical relations is that the latter are explicitly concerned with the linkage of separate segments of language.,
1139042820,false,finalized,3,1/23/2017 16:16:48,Neut,1.0,W93-0235,"[Bratman, 1990] discusses three roles that intention plays in deliberative behavior: serving as a motivation for planning, a ""filter of admissibility"" on plans and further intentions, and a controller of conduct, motivating execution monitoring and repair and replanning when necessary.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf,"Bratman, 1990]","Rhetorical relations are thus actions in the world distinguished by conditions on their occurrence and effects, which will generally be changes to the conversational state and the beliefs of the conversants.",Intentions are commitments towards a course of action.,
1139042821,false,finalized,3,1/23/2017 16:48:36,Neut,1.0,W93-0235,"Of intentional actions, it is also possible to draw the distinction made in speech act theory between illocutionary acts, those in which part of the intended effect includes an awareness on the part of the hearer of this intention, and perlocutionary acts, in which it is only the effect that matters and not recognition of the intention [Austin, 1962].",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf,"Austin, 1962]","For non-illocutionary acts, the intention of the speaker is not relevant - these actions can be produced as side-effects of the speaker's intention, so that a determination of the intention is not necessary to determining whether the act was performed.","As with other actions, relations can be performed intentionally or incidentally.",
1139042822,false,finalized,3,1/23/2017 16:44:58,Neut,1.0,W93-0235,"For example, the evidence relation of [Mann and Thompson, 1987] may hold between two text spans even if the speaker did not intend such a relation, all that is required is that the hearer's belief in the nucleus is increased though the understanding of the satellite.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf,"Mann and Thompson, 1987]","For an illocutionary act, on the other hand, the recognition of communicative intention is crucial to understanding.","For non-illocutionary acts, the intention of the speaker is not relevant - these actions can be produced as side-effects of the speaker's intention, so that a determination of the intention is not necessary to determining whether the act was performed.",
1139042823,false,finalized,3,1/23/2017 16:52:43,Neut,1.0,W93-0235,"[Cohen and Levesque, 1990; Grosz and Sidner, 1986; Lochbaum; hughes and McCoy]) is centered: intuitively, all that is needed for successful communication is that the hearer understand the speaker's end intentions, not that the act types themselves be recognized.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf,"Cohen and Levesque, 1990","This intuition, along with the lack of general agreement on the precise set of acts or relations lead some to reject the utility of relations altogether and concentrate only On intentions.",It is on the following point that the main criticism of bounded sets of speech acts or rhetorical relations (e.g.,
1139042824,false,finalized,3,1/23/2017 16:19:04,Neut,1.0,W93-0235,"[Cohen and Levesque, 1990; Grosz and Sidner, 1986; Lochbaum; hughes and McCoy]) is centered: intuitively, all that is needed for successful communication is that the hearer understand the speaker's end intentions, not that the act types themselves be recognized.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf,"Grosz and Sidner, 1986","This intuition, along with the lack of general agreement on the precise set of acts or relations lead some to reject the utility of relations altogether and concentrate only On intentions.",It is on the following point that the main criticism of bounded sets of speech acts or rhetorical relations (e.g.,
1139042825,false,finalized,3,1/23/2017 16:46:09,Neut,1.0,W93-0235,"In the TRAINS Conversation System implementation [Allen and Schubert, 1991], we take a fairly pragmatic approach towards rhetorical relations.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf,"Allen and Schubert, 1991]",Those relations that are conventionally signalled by surface features (e.g.,Rhetorical Relations in the TRAINS System,
1139042826,false,finalized,3,1/23/2017 16:15:39,Neut,1.0,W93-0235,"by clue words such as ""so"", ""no"", ""okay"", purpose clauses) are hypothesized by the Speech Act Interpreter [Heeman, 1993] and used by the Dialogue Manager [Traum, 1993] to guide further interpretation.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf,"Heeman, 1993]","In the case of more implicit relationships we often do not identify the precise relation, merely operating on the speech act level forms.",Those relations that are conventionally signalled by surface features (e.g.,
1139042827,false,finalized,3,1/23/2017 16:28:54,Neut,1.0,W93-0235,"by clue words such as ""so"", ""no"", ""okay"", purpose clauses) are hypothesized by the Speech Act Interpreter [Heeman, 1993] and used by the Dialogue Manager [Traum, 1993] to guide further interpretation.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf,"Traum, 1993]","In the case of more implicit relationships we often do not identify the precise relation, merely operating on the speech act level forms.",Those relations that are conventionally signalled by surface features (e.g.,
1139042828,false,finalized,3,1/23/2017 16:19:04,Neut,1.0,W93-0235,"For example, a. purpose clause is useful for the domain plan recognizer [Ferguson and Allen, 1993] in incorporating new content into an existing (partial) plan, but in the absence of such a cue, the recognizer will still try to connect the new content to previous content.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf,"Ferguson and Allen, 1993]","It would then be possible to deduce the relations that this item holds with previous items, but we currently see no need to do this.",This is particularly true for subject matter relations.,
1139042829,false,finalized,3,1/23/2017 16:11:13,Neut,1.0,W12-2011,"Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Attali and Burstein, 2006","One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).",1 Introduction and Motivation,
1139042830,false,finalized,3,1/23/2017 16:04:32,Neut,1.0,W12-2011,"Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"Yannakoudakis et al., 2011)","One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).",1 Introduction and Motivation,
1139042831,false,finalized,3,1/24/2017 05:53:28,Neut,0.6497,W12-2011,"One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Rozovskaya and Roth, 2011","Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011).","Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).",
1139042832,false,finalized,3,1/23/2017 16:07:49,Neut,1.0,W12-2011,"One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"Tetreault and Chodorow, 2008)","Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011).","Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).",
1139042833,false,finalized,3,1/23/2017 16:24:11,Pos,0.6528,W12-2011,"One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Dale and Kilgarriff, 2011)","Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011).","Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).",
1139042834,false,finalized,3,1/24/2017 05:53:28,Neut,1.0,W12-2011,"Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Dickinson et al., 2011)","In our work, we extend the task to predicting the learner's level based on the errors, focusing on Hebrew.","One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).",
1139042835,false,finalized,3,1/23/2017 16:52:43,Neut,1.0,W12-2011,"(Fulcher, 1997)).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Fulcher, 1997)","To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels.","Placing learners into levels is generally done by a human, based on a written exam (e.g.",
1139042836,false,finalized,3,1/23/2017 16:09:58,Neut,0.6548,W12-2011,"There is only little work investigating this correspondence formally (see (Hawkins and Filipovic, 2010; Alexopoulou et al., 2010) for discussion) and only on error-annotated English learner corpora.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Hawkins and Filipovic, 2010","For this reason, we follow a data-driven approach to learn the correspondence between errors and levels, based on exercises from written placement exams.","To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels.",
1139042837,false,finalized,3,1/23/2017 16:47:20,Neut,0.6617,W12-2011,"There is only little work investigating this correspondence formally (see (Hawkins and Filipovic, 2010; Alexopoulou et al., 2010) for discussion) and only on error-annotated English learner corpora.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"Alexopoulou et al., 2010)","For this reason, we follow a data-driven approach to learn the correspondence between errors and levels, based on exercises from written placement exams.","To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels.",
1139042838,false,finalized,3,1/23/2017 16:42:25,Weak,0.6657,W12-2011,"learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Goldberg and Elhadad, 2011","For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses.","The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95-104, Montreal, Canada, June 3-8, 2012. c2012 Association for Computational Linguistics",
1139042839,false,finalized,3,1/23/2017 16:11:13,Neut,0.6777,W12-2011,"learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"Yona and Wintner, 2008","For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses.","The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95-104, Montreal, Canada, June 3-8, 2012. c2012 Association for Computational Linguistics",
1139042840,false,finalized,3,1/23/2017 16:32:37,Neut,0.662,W12-2011,"learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"Itai and Wintner, 2008)","For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses.","The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95-104, Montreal, Canada, June 3-8, 2012. c2012 Association for Computational Linguistics",
1139042841,false,finalized,3,1/23/2017 16:44:58,Neut,1.0,W12-2011,"(Dickinson, 2011)).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Dickinson, 2011)","An error could feature, for instance, a letter inserted in an irregular verb stem, or between two nouns; any of these properties may be relevant to describing the error (cf.",e.g.,
1139042842,false,finalized,3,1/23/2017 16:13:12,Neut,1.0,W12-2011,"(Diaz-Negrillo and Fernandez-Dominguez, 2006; Boyd, 2010)).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Diaz-Negrillo and Fernandez-Dominguez, 2006","Specific error types are unlikely to recur, making sparsity even more of a concern.","how errors are described in different taxonomies, e.g.",
1139042843,false,finalized,3,1/23/2017 18:17:32,Neut,1.0,W12-2011,"(Diaz-Negrillo and Fernandez-Dominguez, 2006; Boyd, 2010)).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"Boyd, 2010)","Specific error types are unlikely to recur, making sparsity even more of a concern.","how errors are described in different taxonomies, e.g.",
1139042844,false,finalized,3,1/23/2017 18:17:32,Neut,1.0,W12-2011,"As with many exams, the main purpose is to ""reduce the number of students who attend an oral interview"" (Fulcher, 1997).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Fulcher, 1997)",2 The Data,"The system is intended to semi-automatically place incoming students into the appropriate Hebrew course, i.e., level.",
1139042845,false,finalized,3,1/23/2017 18:16:54,Neut,0.644,W12-2011,"1We follow the transliteration scheme of the Hebrew Treebank (Sima'an et al., 2001).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"an et al., 2001)",'They did not always speak in the Hebrew language in the land of Israel.',(1) barC beph dibrw hybrit ieral la tmid beph in-the-language hybrit the-Hebrew,
1139042846,false,finalized,3,1/24/2017 05:54:22,Neut,0.6627,W12-2011,"Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al., 2011).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Yannakoudakis et al., 2011)","To detect errors, we align the learner sentence with a gold standard and extract the features.","While features can be based on individual phenomena of any type, we base our extracted features largely upon learner errors.",
1139042847,false,finalized,3,1/23/2017 18:18:30,Neut,1.0,W12-2011,"(Crigonyt`e et al., 2010)), but note that our problem is more restricted in that we have the same number of words, and in most cases identical words.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"e et al., 2010)","We use different distance and similarity metrics, to ensure robustness across different kinds of errors.",This method is reminiscent of alignment approaches in paraphrasing (e.g.,
1139042848,false,finalized,3,1/23/2017 17:42:19,Neut,1.0,W12-2011,"criterial features in (Hawkins and Buttery, 2010)), giving less noise to phase 2.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Hawkins and Buttery, 2010)","We may lose important non-best level information, but as we show in sec.",The second approach counteracts this confusion by selecting the most prototypical level for an individual phenomenon (cf.,
1139042849,false,finalized,3,1/23/2017 18:17:32,Neut,0.644,W12-2011,"We assume that the set of all individual phenomena and their quantities (e.g., proportion of phenomena classified as 200-level in phase 1) characterize a learner's level (Hawkins and Buttery, 2010).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Hawkins and Buttery, 2010)",The feature types are given in table 4.,We aggregate the information from phase 1 classification to classify overall learner levels.,
1139042850,false,finalized,3,1/23/2017 18:12:50,Pos,1.0,W12-2011,"We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999), a memory-based learner (MBL), for both phases.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Daelemans et al., 2010","We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001); allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space.",6.1 Details of the experiments,
1139042851,false,finalized,3,1/24/2017 10:38:46,Neut,0.3455,W12-2011,"We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999), a memory-based learner (MBL), for both phases.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"Daelemans et al., 1999)","We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001); allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space.",6.1 Details of the experiments,
1139042852,false,finalized,3,1/23/2017 17:44:46,Pos,1.0,W12-2011,"We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001); allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Banko and Brill, 2001)","We mostly use the default settings of TiMBL-the IB1 learning algorithm and overlap comparison metric between instances-and experiment with different values of k. For prediction of phenomenon level (phase 1) and learner level (phase 2), the system is trained on data from placement exams previously collected in a Hebrew language program, as described in sec.","We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999), a memory-based learner (MBL), for both phases.",
1139042853,false,finalized,3,1/23/2017 19:04:19,Pos,0.6719,W12-2011,"(Yannakoudakis et al., 2011))-we can determine their relative importance and usefulness.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Yannakoudakis et al., 2011)",We run phase 2 (k = 1) using different combinations of phase 1 classifiers (1-best) as input.,e.g.,
1139042854,false,finalized,3,1/24/2017 10:33:53,Neut,1.0,W12-2011,"(Fulcher, 1997)), while at the same time incorporating more linguistic processing for more complex input.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf,"(Fulcher, 1997)","For example, with question formation exercises, no closed set of correct answers exists, and one must use parse tree distance to delineate features.",e.g.,
1139042855,false,finalized,3,1/24/2017 05:54:22,Neut,0.6877,N04-1023,"The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Brown et al., 1990)","Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.",1 Introduction,
1139042856,false,finalized,3,1/23/2017 18:18:30,Pos,0.6771,N04-1023,"Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Och and Ney, 2002)","In this paper, we introduce two novel machine learning algorithms specialized for the MT task.","The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.",
1139042857,false,finalized,3,1/23/2017 17:41:33,Pos,0.6909,N04-1023,"Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Och, 2003)","In this paper, we introduce two novel machine learning algorithms specialized for the MT task.","The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.",
1139042858,false,finalized,3,1/23/2017 18:16:14,Pos,0.6505,N04-1023,"The seminal IBM models (Brown et al., 1990) were the first to introduce generative models to the MT task.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Brown et al., 1990)",The IBM models applied the sequence learning paradigm well-known from Hidden Markov Models in speech recognition to the problem of MT.,1.1 Generative Models for MT,
1139042859,false,finalized,3,1/23/2017 18:16:00,Neut,1.0,N04-1023,Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,(1998),"Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders.",Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality.,
1139042860,false,finalized,3,1/23/2017 17:42:19,Neut,1.0,N04-1023,"In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Och and Weber, 1998","However, phrase level alignment cannot handle long distance reordering effectively.","Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders.",
1139042861,false,finalized,3,1/24/2017 06:24:17,Neut,0.6942,N04-1023,"In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"Och et al., 1999)","However, phrase level alignment cannot handle long distance reordering effectively.","Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders.",
1139042862,false,finalized,3,1/23/2017 18:18:30,Neut,1.0,N04-1023,Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,(1997),"(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank.",Parse trees have also been used in alignment models.,
1139042863,false,finalized,3,1/23/2017 18:18:24,Neut,1.0,N04-1023,"(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Wu, 1997)",Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment.,Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form.,
1139042864,false,finalized,3,1/23/2017 19:04:19,Neut,1.0,N04-1023,Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,(2001),Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages.,"(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank.",
1139042865,false,finalized,3,1/23/2017 18:18:30,Neut,0.6771,N04-1023,Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,(2003),The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models.,Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment.,
1139042866,false,finalized,3,1/23/2017 18:14:31,Neut,0.6783,N04-1023,"Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model estimated using a maximum entropy model.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,(2002),A small number of feature functions defined on the source and target sentence were used to rerank the translations generated by a baseline MT system.,1.2 Discriminative Models for MT,
1139042867,false,finalized,3,1/24/2017 10:35:31,Neut,1.0,N04-1023,Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,(2003),The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation.,"While the total number of feature functions was small, each feature function was a complex statistical model by itself, as for example, the alignment template feature functions used in this approach.",
1139042868,false,finalized,3,1/24/2017 05:53:40,Pos,0.6861,N04-1023,"This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Och and Ney, 2002)","SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.",The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation.,
1139042869,false,finalized,3,1/24/2017 06:24:17,Neut,0.6579,N04-1023,"SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,(2003),More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output.,"This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).",
1139042870,false,finalized,3,1/24/2017 11:52:37,Neut,0.679,N04-1023,"SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,(2003),More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output.,"This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).",
1139042871,false,finalized,3,1/23/2017 18:15:18,Pos,0.6719,N04-1023,"By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,(2003),2 Ranking and Reranking,More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output.,
1139042872,false,finalized,3,1/23/2017 18:14:31,Pos,0.3495,N04-1023,"By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Papineni et al., 2001)",2 Ranking and Reranking,More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output.,
1139042873,false,finalized,3,1/23/2017 17:43:05,Neut,0.6712,N04-1023,"Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Collins, 2000)",The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models.,"In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing.",
1139042874,false,finalized,3,1/23/2017 18:49:26,Neut,1.0,N04-1023,"Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Collins and Duffy, 2002)",The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models.,"In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing.",
1139042875,false,finalized,3,1/23/2017 17:43:05,Neut,0.6712,N04-1023,"Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Shen and Joshi, 2003)",The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models.,"In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing.",
1139042876,false,finalized,3,1/23/2017 18:49:26,Pos,0.6719,N04-1023,"In (Shen and Joshi, 2004), we have introduced a new perceptron-like ordinal regression algorithm for parse reranking.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Shen and Joshi, 2004)","In that algorithm, pairwise samples are used for training and margins are defined as the distance between parses of different ranks.",The reranking problem is reduced to a classification problem by using pairwise samples.,
1139042877,false,finalized,3,1/23/2017 18:18:24,Neut,0.6743,N04-1023,"One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Crammer and Singer, 2001","However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks.",Two large margin approaches have been used.,
1139042878,false,finalized,3,1/24/2017 11:12:48,Neut,1.0,N04-1023,"One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"Harrington, 2003)","However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks.",Two large margin approaches have been used.,
1139042879,false,finalized,3,1/23/2017 17:44:46,Neut,0.6783,N04-1023,"The other approach is to reduce the ranking problem to a classification problem by using the method of pairwise samples (Herbrich et al., 2000).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Herbrich et al., 2000)",The underlying assumption is that the samples of consecutive ranks are separable.,"However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks.",
1139042880,false,finalized,3,1/23/2017 17:48:08,Neut,1.0,N04-1023,"In parse reranking, each training sentence has a ranked list of 27 candidates on average (Collins, 2000), but for machine translation, the number of candidate translations in the -best list is much higher.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Collins, 2000)","(SMT Team, 2003) show that to get a reasonable improvement in the BLEU score at least 1000 candidates need to be considered in the -best list.","Although we might invent metrics to define the quality of a translation, standard reranking algorithms cannot be directly applied to MT.",
1139042881,false,finalized,3,1/23/2017 18:15:18,Neut,1.0,N04-1023,"(SMT Team, 2003) show that to get a reasonable improvement in the BLEU score at least 1000 candidates need to be considered in the -best list.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(SMT Team, 2003)","In addition, the parallel hyperplanes separating and actually are unable to distinguish good translations from bad translations, since they are not trained to distinguish any translations in .","In parse reranking, each training sentence has a ranked list of 27 candidates on average (Collins, 2000), but for machine translation, the number of candidate translations in the -best list is much higher.",
1139042882,false,finalized,3,1/23/2017 18:16:54,Neut,0.6909,N04-1023,"There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Vapnik, 1998)",The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization.,3.6 Large Margin Classifiers,
1139042883,false,finalized,3,1/24/2017 11:53:24,Neut,1.0,N04-1023,"There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Schapire et al., 1997)",The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization.,3.6 Large Margin Classifiers,
1139042884,false,finalized,3,1/23/2017 18:15:40,Neut,1.0,N04-1023,"There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Zhang, 2000)",The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization.,3.6 Large Margin Classifiers,
1139042885,false,finalized,3,1/23/2017 17:41:33,Neut,1.0,N04-1023,"There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Krauth and Mezard, 1987)",The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization.,3.6 Large Margin Classifiers,
1139042886,false,finalized,3,1/24/2017 05:54:22,Neut,1.0,N04-1023,"For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Shen and Joshi, 2003)","Taking this into account, we use perceptron-like algorithms, since the perceptron algorithm is fast in training which allow us to do experiments on real-world data.","However, SVMs are extremely slow in training since they need to solve a quadratic programming search.",
1139042887,false,finalized,3,1/24/2017 05:54:22,Neut,1.0,N04-1023,"For samples, the total number of pairwise samples in (Herbrich et al., 2000) is roughly .",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Herbrich et al., 2000)","In the next section, we will introduce two perceptron-like algorithms that utilize pairwise samples while keeping the complexity of data space unchanged.","However, the size of generated training samples will be very large.",
1139042888,false,finalized,3,1/23/2017 18:12:50,Pos,1.0,N04-1023,"We also experimented with an ordinal regression algorithm proposed in (Shen and Joshi, 2004).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Shen and Joshi, 2004)","For the sake of completeness, we will briefly describe the algorithm here.",classification algorithm.,
1139042889,false,finalized,3,1/24/2017 05:56:05,Neut,0.6244,N04-1023,"The second algorithm that we will use for MT reranking is the -insensitive ordinal regression with uneven margin, which was proposed in (Shen and Joshi, 2004), as shown in Algorithm 2.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Shen and Joshi, 2004)","In Algorithm 2, the function is used to control the level of insensitivity, and the function is used to control the learning margin between pairs of translations with different ranks as described in Section 3.5.",4.2 Ordinal Regression,
1139042890,false,finalized,3,1/23/2017 18:12:50,Pos,1.0,N04-1023,"We use the data set used in (SMT Team, 2003).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(SMT Team, 2003)","The training data consists of about 170M English words, on which the baseline translation system is trained.",We provide experimental results on the NIST 2003 Chinese-English large data track evaluation.,
1139042891,false,finalized,3,1/24/2017 06:24:17,Neut,1.0,N04-1023,"Table 1: BLEU scores reported in (SMT Team, 2003).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(SMT Team, 2003)",Every single feature was combined with the 6 baseline features for the training and test.,do 4: compute and for all ; 5: for ( ) do 6: if and and then,
1139042892,false,finalized,3,1/23/2017 17:44:46,Pos,1.0,N04-1023,"The minimum error training (Och, 2003) was used on the development data for parameter estimation.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Och, 2003)",Feature BLEU% Baseline 31.6 POS Language Model 31.7 Supertag Language Model 31.7 Wrong NN Position 31.7 Word Popularity 31.8 Aligned Template Models 31.9 Count of Missing Word 31.9 Template Right Continuity 32.0 IBM Model 1 32.5,Every single feature was combined with the 6 baseline features for the training and test.,
1139042893,false,finalized,3,1/23/2017 17:42:19,Neut,1.0,N04-1023,"In (SMT Team, 2003), 450 features were generated.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(SMT Team, 2003)","Six features from (Och, 2003) were used as baseline features.",The test set is used to assess the quality of the reranking output.,
1139042894,false,finalized,3,1/23/2017 18:15:18,Pos,0.6769,N04-1023,"Six features from (Och, 2003) were used as baseline features.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Och, 2003)",Each of the 450 features was evaluated independently by combining it with 6 baseline features and assessing on the test data with the minimum error training.,"In (SMT Team, 2003), 450 features were generated.",
1139042895,false,finalized,3,1/23/2017 18:16:14,Neut,1.0,N04-1023,"In (SMT Team, 2003), aggressive search was used to combine features.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(SMT Team, 2003)","After combining about a dozen features, the BLEU score did not improve any more, and the score was 32.9%.",Table 1 shows some of the best performing features.,
1139042896,false,finalized,3,1/24/2017 10:39:14,Neut,0.6622,N04-1023,"In our experiments, we will use 4 different kinds of feature combinations: Baseline: The 6 baseline features used in (Och, 2003), such as cost of word penalty, cost of aligned template penalty.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Och, 2003)",Best Feature: Baseline + IBM Model 1 + matched parentheses + matched quotation marks + POS language model.,"By combining the four features, Model 1, matched parentheses, matched quotation marks and POS language model, the system achieved a BLEU score of 32.6%.",
1139042897,false,finalized,3,1/23/2017 17:43:05,Neut,0.6783,N04-1023,"(Shen and Joshi, 2004).",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Shen and Joshi, 2004)","If the number of the non-discriminative features is large enough, the data set becomes unsplittable.","In addition, generalization performance in the Top Twenty set is better than the Large Set due to the smaller set of ""better"" features, cf.",
1139042898,false,finalized,3,1/23/2017 17:44:46,Weak,0.6783,N04-1023,"We have tried using the trick as in (Li et al., 2002) to make data separable artificially, but the performance could not be improved with such features.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(Li et al., 2002)","We achieve similar results with Algorithm 2, the ordinal regression with uneven margin.","If the number of the non-discriminative features is large enough, the data set becomes unsplittable.",
1139042899,false,finalized,3,1/23/2017 18:15:18,CoCo,1.0,N04-1023,"We compare our perceptron-like algorithms with the minimum error training used in (SMT Team, 2003) as shown in Table 2.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf,"(SMT Team, 2003)","The splitting algorithm achieves slightly better results on the Baseline and the Best Feature set, while the minimum error training and the regression algorithm tie for first place on feature combinations.",This algorithm does not converge on the Large Set in 10000 iterations.,
1139042900,false,finalized,3,1/23/2017 18:15:40,Neut,0.6362,A97-2017,"Our response has been to design and implement a software environment called GATE (Cunninham et al., 1997), which we will demonstrate at ANLP.",http://www.aclweb.org/anthology/A/A97/A97-2017.pdf,"(Cunninham et al., 1997)",GATE attempts to meet the following objectives:,"But the pressure towards theoretical diversity means that there is no point attempting to gain agreement, in the short term, on what set of component technologies should be developed or on the informational content or syntax of representations that these components should require or produce.",
1139042901,false,finalized,3,1/23/2017 18:16:54,Neut,0.6651,A97-2017,"The basic concepts of the data model underlying the GDM are those of the TIPSTER architecture, which is specified (Grishman, 1996).",http://www.aclweb.org/anthology/A/A97/A97-2017.pdf,"(Grishman, 1996)","All the real work of analysing texts in a GATEbased LE system is done by CREOLE modules or objects (we use the terms module and object rather loosely to mean interfaces to resources which may be predominantly algorithmic or predominantly data, or a mixture of both).","All communication between the components of an LE system goes through GDM, which insulates these components from direct contact with each other and provides them with a uniform API for manipulating the data they produce and consume.",
1139042902,false,finalized,3,1/23/2017 17:43:05,Pos,0.6783,A97-2017,"To illustrate the process of converting pre-existing LE systems into GATE-compatible CREOLE sets we use as an example the creation of VIE (Vanilla Information Extraction system) from LaSIE (LargeScale Information Extraction system) (Gaizauskas et al., 1995), Sheffield's entry in the MUC-6 system evaluations.",http://www.aclweb.org/anthology/A/A97/A97-2017.pdf,"(Gaizauskas et al., 1995)",LaSIE module interfaces were not standardised when originally produced and its CREOLEization gives a good indication of the ease of integrating other LE tools into GATE.,(Modules running as external executables might also be recompiled between runs.),
1139042903,false,finalized,3,1/24/2017 11:37:53,Neut,0.6636,Q14-1040,"According to Vygotsky's zone of proximal development (Vygotsky, 1978), the range of suitable material is very small.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Vygotsky, 1978)","Thus, creating a test that fits this narrow target zone is a tedious and timeconsuming task.",The test difficulty needs to match the intended target group as the test should be challenging for the learner but not lead to frustration.,
1139042904,false,finalized,3,1/23/2017 18:12:50,Neut,1.0,Q14-1040,"The underlying principle of most language proficiency tests is the concept of reduced redundancy testing (Spolsky, 1969).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Spolsky, 1969)","It is based on the idea that ""natural language is redundant"" and that more advanced learners can be distinguished from beginners by their ability to deal with reduced redundancy.",This results in a subjective difficulty estimation that often lacks the consistency required for comparing learners over different tests.,
1139042905,false,finalized,3,1/24/2017 11:53:57,Pos,0.6605,Q14-1040,"The C-test is a variant of the cloze test which contains more gaps but provides part of the solution as a hint and has been found to be a good estimate for language proficiency (Eckes and Grotjahn, 2006).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Eckes and Grotjahn, 2006)",We present an approach for determining the difficulty of C-tests that overcomes the mentioned drawbacks of subjective evaluation by teachers.,"For language testing, redundancy can be reduced by eliminating words from a text and asking the learner to fill in the gap, also known as the cloze test.",
1139042906,false,finalized,3,1/23/2017 18:16:14,Neut,0.6783,Q14-1040,"Since its introduction, the C-test has been researched from many angles and has been adapted for over 20 languages (see Grotjahn et al (2002) for an overview).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(2002),2.1 C-Tests vs Cloze Tests,"For each gap, the smaller half of the word is provided and the missing part has to be completed by the learner.",
1139042907,false,finalized,3,1/23/2017 18:16:00,Weak,1.0,Q14-1040,"Unfortunately, this scoring method turned out to be quite subjective and time-consuming as it is not possible to anticipate all tolerable solutions (Raatz and Klein-Braley, 2002).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Raatz and Klein-Braley, 2002)",The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked.,"In relaxed scoring, teachers accept all tolerable candidates for a gap and not only the intended solution as in exact scoring.",
1139042908,false,finalized,3,1/24/2017 11:38:40,Neut,0.6636,Q14-1040,"Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Sakaguchi et al., 2013","However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results.",The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked.,
1139042909,false,finalized,3,1/23/2017 18:18:24,Neut,1.0,Q14-1040,"Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"Zesch and Melamud, 2014)","However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results.",The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked.,
1139042910,false,finalized,3,1/23/2017 17:48:08,Neut,0.3488,Q14-1040,"In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1984),"Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).","However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results.",
1139042911,false,finalized,3,1/23/2017 18:15:40,Neut,0.6362,Q14-1040,"Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Babaii and Ansary, 2001","For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option.","In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.",
1139042912,false,finalized,3,1/24/2017 11:21:17,Neut,0.6449,Q14-1040,"Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"Klein-Braley, 1997","For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option.","In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.",
1139042913,false,finalized,3,1/23/2017 17:48:08,Pos,0.6512,Q14-1040,"Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"Jafarpur, 1995)","For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option.","In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.",
1139042914,false,finalized,3,1/24/2017 11:53:57,Neut,1.0,Q14-1040,"As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1984),"However, Jakschik et al (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps.","In addition, the prefix hint allows for a narrower deletion pattern (every second gap) providing more empirical evidence for the students' abilities on less text.",
1139042915,false,finalized,3,1/24/2017 10:31:49,Pos,0.685,Q14-1040,"However, Jakschik et al (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(2010),"This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice.","As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition.",
1139042916,false,finalized,3,1/23/2017 17:41:33,Neut,0.6909,Q14-1040,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Mostow and Jang, 2012",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,
1139042917,false,finalized,3,1/24/2017 11:12:48,Neut,0.6916,Q14-1040,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"Agarwal and Mannem, 2011",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,
1139042918,false,finalized,3,1/24/2017 11:35:57,Neut,1.0,Q14-1040,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"Mitkov et al., 2006)",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,
1139042919,false,finalized,3,1/23/2017 17:41:33,Neut,0.6909,Q14-1040,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Skory and Eskenazi, 2010",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,
1139042920,false,finalized,3,1/23/2017 17:48:08,Neut,0.6719,Q14-1040,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"Heilman et al., 2007",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,
1139042921,false,finalized,3,1/24/2017 06:24:17,Neut,1.0,Q14-1040,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"Brown et al., 2005)",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,
1139042922,false,finalized,3,1/23/2017 18:49:26,Neut,1.0,Q14-1040,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Perez-Beltrachini et al., 2012)",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,
1139042923,false,finalized,3,1/23/2017 19:04:19,Neut,0.6719,Q14-1040,"While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Eckes and Grotjahn, 2006",The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,"The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: ""Which skills does the C-test measure?""",
1139042924,false,finalized,3,1/23/2017 18:14:13,CoCo,0.644,Q14-1040,"While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"Sigott, 1995",The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,"The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: ""Which skills does the C-test measure?""",
1139042925,false,finalized,3,1/23/2017 18:16:14,Neut,0.6712,Q14-1040,"While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"Klein-Braley, 1985)",The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,"The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: ""Which skills does the C-test measure?""",
1139042926,false,finalized,3,1/23/2017 17:42:19,CoCo,0.644,Q14-1040,"While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Babaii and Ansary, 2001)",The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,"The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: ""Which skills does the C-test measure?""",
1139042927,false,finalized,3,1/24/2017 05:56:05,CoCo,0.6244,Q14-1040,"While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Chapelle, 1994",The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,"The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: ""Which skills does the C-test measure?""",
1139042928,false,finalized,3,1/23/2017 18:49:26,CoCo,0.6769,Q14-1040,"While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"Singleton and Little, 1991)",The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,"The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: ""Which skills does the C-test measure?""",
1139042929,false,finalized,3,1/24/2017 05:53:40,Neut,1.0,Q14-1040,Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators - average sentence length and type-token ratio - obtaining good results for her target group.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1984),Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level.,The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,
1139042930,false,finalized,3,1/24/2017 10:34:57,Neut,1.0,Q14-1040,Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(2011),He created a tailored C-test that only contains selected gaps in order to better discriminate between the students.,Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators - average sentence length and type-token ratio - obtaining good results for her target group.,
1139042931,false,finalized,3,1/23/2017 18:15:40,Neut,0.6841,Q14-1040,Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1993),He created a tailored C-test that only contains selected gaps in order to better discriminate between the students.,Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators - average sentence length and type-token ratio - obtaining good results for her target group.,
1139042932,false,finalized,3,1/24/2017 11:10:36,Neut,1.0,Q14-1040,"Brown (1989) identifies the word class, the local word frequency, and readability measures as factors correlating with cloze gap difficulty.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1989),Sigott (1995) exam,Previous work on gap difficulty is based on correlation analyses.,
1139042933,false,finalized,3,1/23/2017 18:17:32,Neut,0.6731,Q14-1040,Sigott (1995) exam,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1995),"2It should be noted, that their definition of ""vocabulary"" is very wide.","Brown (1989) identifies the word class, the local word frequency, and readability measures as factors correlating with cloze gap difficulty.",
1139042934,false,finalized,3,1/24/2017 10:42:02,Neut,1.0,Q14-1040,"Klein-Braley (1996) identifies additional error patterns related to production problems (right word stem in wrong form) and early closure, i.e.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1996),the solution works locally but not in the larger context.,"ines word frequency, word class, and constituent type of the gap for the C-test and finds high correlation only for the word frequency.",
1139042935,false,finalized,3,1/23/2017 19:04:19,Pos,0.6719,Q14-1040,"As the generous time limit allows the students to revise their solutions for typos, we consider them as normal errors in line with Raatz and Klein-Braley (2002).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(2002),4 C-Test Difficulty Model,of vs. off or then vs. than and we cannot decide whether it is a spelling error or a wrong word choice.,
1139042936,false,finalized,3,1/24/2017 11:38:12,Neut,1.0,Q14-1040,"In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Babaii and Ansary, 2001)","Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving.",We find that the difficulty of C-tests is determined by a combination of many factors.,
1139042937,false,finalized,3,1/23/2017 18:14:31,Neut,0.6783,Q14-1040,"Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Sigott, 2006","Therefore, we developed a model for C-test difficulty that incorporates features from both processing levels (see Figure 4).","In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001).",
1139042938,false,finalized,3,1/23/2017 18:14:31,Pos,0.6505,Q14-1040,"Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"Grotjahn and Stemmer, 2002)","Therefore, we developed a model for C-test difficulty that incorporates features from both processing levels (see Figure 4).","In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001).",
1139042939,false,finalized,3,1/23/2017 18:16:54,Neut,1.0,Q14-1040,"In previous work, Brown (1989) calculates the frequency of the target word on the basis of the current test text.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1989),"This is clearly a biased estimate of the frequency, but it is still identified as a good indicator for cloze gap difficulty.",We therefore calculate the frequency of the solution and also its length as more frequent words tend to be shorter in English.,
1139042940,false,finalized,3,1/24/2017 10:40:11,Neut,0.6622,Q14-1040,"Sigott (1995) calculates the frequency of the solution word using counts from the SUSANNE corpus.7 For our calculations, we use the larger Web1T corpus (Brants and Franz, 2006) and extract normalized probabilities instead of absolute frequencies for better comparison.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1995),"Furthermore, a gap is easier to solve, if the solution occurs in a very typical context, e.g.","This is clearly a biased estimate of the frequency, but it is still identified as a good indicator for cloze gap difficulty.",
1139042941,false,finalized,3,1/23/2017 18:16:00,Neut,0.644,Q14-1040,"Sigott (1995) calculates the frequency of the solution word using counts from the SUSANNE corpus.7 For our calculations, we use the larger Web1T corpus (Brants and Franz, 2006) and extract normalized probabilities instead of absolute frequencies for better comparison.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Brants and Franz, 2006)","Furthermore, a gap is easier to solve, if the solution occurs in a very typical context, e.g.","This is clearly a biased estimate of the frequency, but it is still identified as a good indicator for cloze gap difficulty.",
1139042942,false,finalized,3,1/23/2017 18:16:00,Neut,0.644,Q14-1040,"In order to account for polysemy, we count the number of represented word senses for the solution in the lexical-semantic resource UBY (Gurevych et al., 2012).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Gurevych et al., 2012)",The two senses of well also differ in their word class.,"the word well has a high frequency, but it occurs only rarely in its sense fountain.",
1139042944,false,finalized,3,1/24/2017 05:31:03,CoCo,0.3523,Q14-1040,"Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1989),Sigott (1995) could not confirm any effect of the word class on C-test difficulty.,The word class has been studied as a difficulty indicator by several researchers but with mixed results.,
1139042945,false,finalized,3,1/24/2017 05:33:39,CoCo,1.0,Q14-1040,"Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1996),Sigott (1995) could not confirm any effect of the word class on C-test difficulty.,The word class has been studied as a difficulty indicator by several researchers but with mixed results.,
1139042946,false,finalized,3,1/24/2017 05:17:14,Weak,0.6715,Q14-1040,Sigott (1995) could not confirm any effect of the word class on C-test difficulty.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1995),"6In all examples, we only highlight a single gap to illustrate a certain phenomenon.","Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners.",
1139042947,false,finalized,3,1/24/2017 05:59:51,Neut,0.6831,Q14-1040,"Cognateness Frequency is not the only indicator for word familiarity and can sometimes even be misleading (Beinborn et al., 2014).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Beinborn et al., 2014)","Many solution words are cognates, i.e.","As additional feature, we calculate the probability of the POS sequence of the micro context.",
1139042948,false,finalized,3,1/24/2017 05:32:14,Pos,0.6795,Q14-1040,"We retrieve cognates from UBY using string similarity and from a cognate production algorithm (Beinborn et al., 2013).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Beinborn et al., 2013)","In addition, we consult the COCA list of academic words12 and a list of words with latin roots.13 Inflection Many errors are caused by wrong morphological inflection as in this example: And in har times like these, ... [harder] The base form hard (72) is provided more often than the correct comparative harder (48), although it is too short.","for skeletons we find *skellets, *skelleton(s), *skelets, *skelletts, *skeletton(s), *skeltons, *skeletes, and *skelette(s).11 In order to account for this phenomenon, we estimate the cognateness of words by gathering data from four different lists.",
1139042949,false,finalized,3,1/24/2017 05:24:48,CoCo,1.0,Q14-1040,This feature is comparable to the semantic cache used by Brown (1989).,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1989),Phonetic complexity Wrong answer variants for C-test gaps are often rooted in phonetic problems.,not as a gap) because it facilitates the correct production for the student.,
1139042950,false,finalized,3,1/24/2017 05:53:00,Neut,1.0,Q14-1040,"We build a character-based language model that indicates the probability of a character sequence using BerkeleyLM (Pauls and Klein, 2011).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Pauls and Klein, 2011)","In addition, we build a phonetic model using phonetisaurus, a statistical alignment algorithm that maps characters onto phonemes.14 Both models are trained only on words from the Basic English list in order to reflect the knowledge of a language learner.15 Based on this scarce data, the phonetic model only learns the most frequent character-tophoneme mappings and assigns higher phonetic scores to less general letter sequences.","In addition, a spelling error is more likely to occur, in words with rare grapheme-phoneme mapping as in Wednesday.",
1139042951,false,finalized,3,1/24/2017 05:12:16,Neut,1.0,Q14-1040,Bresnihan and Ray (1992) show that students perform,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1992),522,It would be interesting to repeat the test with the constraint that false answers have a negative influence on the overall score in order to find out whether the students are aware of the length violation.,
1139042952,false,finalized,3,1/24/2017 05:30:40,Neut,0.6742,Q14-1040,"In previous work, Harsch and Hartig (2010) examine dependencies between individual gaps using a Rasch testlet model and find that some gaps strongly depend on each other, while others can be solved independently.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(2010),"At the same time, fertility is set to fall as women leave childbirth la and la .","This macro-level dimension assesses the dependency of the current gap on previous gaps: can it be solved, even if the previous gap has not been solved?",
1139042953,false,finalized,3,1/24/2017 05:31:03,Neut,0.6826,Q14-1040,Average word and sentence length are the underlying basis of traditional readability measures such as Flesch-Kincaid and Fry which correlate with cloze test difficulty according to Brown (1989).,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1989),"We calculate both, but do not find much variety as the paragraphs in our data are all of comparable length (64-99 words, 3-7 sentences, 4.85 characters per word).",We calculate the following readability features for the whole paragraph and for the sentence containing the gap.,
1139042954,false,finalized,3,1/24/2017 05:29:10,Neut,0.6673,Q14-1040,Klein-Braley (1984) already determined the type-token ratio as useful cue for paragraph difficulty prediction.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1984),"We also use syntactic readability features such as the number of entity mentions, the number of certain POS types (e.g.","The type-token ratio, the verb variation, and the pronoun ratio are used as indicators for lexical diversity and referentiality.",
1139042955,false,finalized,3,1/24/2017 06:04:01,Neut,1.0,Q14-1040,"Difficulty prediction of C-tests has up to now only been performed on the paragraph level (KleinBraley, 1984; Traxel and Dresemann, 2010).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(KleinBraley, 1984","In this article, we go beyond paragraphs and predict the difficulty of gaps.","Having introduced all four dimensions of C-test difficulty, we now report on the results of the actual difficulty prediction.",
1139042956,false,finalized,3,1/24/2017 06:21:55,Neut,1.0,Q14-1040,"Difficulty prediction of C-tests has up to now only been performed on the paragraph level (KleinBraley, 1984; Traxel and Dresemann, 2010).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"Traxel and Dresemann, 2010)","In this article, we go beyond paragraphs and predict the difficulty of gaps.","Having introduced all four dimensions of C-test difficulty, we now report on the results of the actual difficulty prediction.",
1139042957,false,finalized,3,1/24/2017 06:22:50,Neut,1.0,Q14-1040,Classification Regression P R Fl Pearson's r RMSE Majority Baseline .19 .43 .26 .00 .25 Sigott (1995) .23 .40 .28 .34 .24 Our Approach .46 .48 .46 .64 .20 Human Median .56 .53 .54 -,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1995),Table 3: Results for leave-one-out crossvalidation on the training set for regression and classification prediction (both trained on support vector machines).,"Thus, it is possible to outperform human performance with automatic methods and provide a very helpful tool.",
1139042958,false,finalized,3,1/24/2017 05:36:06,Pos,0.6707,Q14-1040,"We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych, 2014).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(de Castilho and Gurevych, 2014)","We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18",Our difficulty prediction approach is based on the model described in the previous section.,
1139042959,false,finalized,3,1/24/2017 05:39:21,Pos,0.6795,Q14-1040,"We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Hall et al., 2009)",6.1 Classification vs Regression,"We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych, 2014).",
1139042960,false,finalized,3,1/24/2017 06:22:50,Neut,0.6829,Q14-1040,"We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,"(Daxenberger et al., 2014)",6.1 Classification vs Regression,"We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych, 2014).",
1139042961,false,finalized,3,1/24/2017 05:32:52,Neut,0.6492,Q14-1040,"We compare our model against the human performance and two baselines: A naive one that predicts the majority class for classification and the mean value for regression and one that only uses the features proposed by Sigott (1995) (solution probability, word class of solution, and constituent type of gap).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1995),"In Table 3, we report weighted precision, recall and Fl-measure over all classes for classification and Pearson correlation and root mean squared error for regression.",We perform leave-one-out testing on the training set in order to determine the best approach.,
1139042962,false,finalized,3,1/24/2017 05:26:35,Neut,1.0,Q14-1040,# LOOCV Train Train-Test LOOCV All Mean Baseline 1 .00 .00 .00 Sigott (1995) 7 .34 .38 .36 Full Model 87 .64 .32 .60 Selected Features 21 .68 .44 .57,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf,(1995),Table 8: Results on the train and the test set,"Starting from a sample size of about 70 instances, the learning curve proceeds as",
1139042963,false,finalized,3,1/24/2017 05:29:10,Neut,0.3403,N09-1070,"Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Frank et al., 1999","However, we may not have a big background collection that matches the test domain for a reliable IDF estimate.","The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection.",
1139042964,false,finalized,3,1/24/2017 05:36:06,Neut,0.6781,N09-1070,"Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"Hulth, 2003","However, we may not have a big background collection that matches the test domain for a reliable IDF estimate.","The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection.",
1139042965,false,finalized,3,1/24/2017 05:34:08,Neut,0.6788,N09-1070,"Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"Kerner et al., 2005)","However, we may not have a big background collection that matches the test domain for a reliable IDF estimate.","The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection.",
1139042966,false,finalized,3,1/24/2017 06:22:50,Neut,1.0,N09-1070,"(Matsuo and Ishizuka, 2004) proposed a co-occurrence distribution based method using a clustering strategy for extracting keywords for a single document without relying on a large corpus, and reported promising results.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Matsuo and Ishizuka, 2004)",Web information has also been used as an additional knowledge source for keyword extraction.,"However, we may not have a big background collection that matches the test domain for a reliable IDF estimate.",
1139042967,false,finalized,3,1/24/2017 05:37:37,Neut,1.0,N09-1070,"(Turney, 2002) selected a set of keywords first and then determined whether to add another keyword hypothesis based on its PMI (point-wise mutual information) score to the current selected keywords.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Turney, 2002)",The preselected keywords can be generated using basic extraction algorithms such as TFIDF.,Web information has also been used as an additional knowledge source for keyword extraction.,
1139042968,false,finalized,3,1/24/2017 05:25:21,Neut,1.0,N09-1070,"Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Inkpen and Desilets, 2004)","Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).",It is important to ensure the quality of the first selection for the subsequent addition of keywords.,
1139042969,false,finalized,3,1/24/2017 05:38:51,Neut,1.0,N09-1070,"Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Frank et al., 1999","In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.","Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).",
1139042970,false,finalized,3,1/24/2017 05:31:03,Neut,0.6477,N09-1070,"Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"Turney, 2000","In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.","Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).",
1139042971,false,finalized,3,1/24/2017 05:58:52,Neut,1.0,N09-1070,"Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"Kerner et al., 2005","In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.","Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).",
1139042972,false,finalized,3,1/24/2017 05:39:36,Neut,0.6838,N09-1070,"Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"Turney, 2002","In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.","Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).",
1139042973,false,finalized,3,1/24/2017 05:51:26,Neut,1.0,N09-1070,"Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"Turney, 2003)","In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.","Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).",
1139042974,false,finalized,3,1/24/2017 05:29:10,Neut,0.6673,N09-1070,"Useful features for this approach include TFIDF and its variations, position of a phrase, POS information, and relative length of a phrase (Turney, 2000).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Turney, 2000)",Some of these features may not work well for meeting transcripts.,"In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.",
1139042975,false,finalized,3,1/24/2017 05:42:12,Neut,0.6831,N09-1070,"A supervised approach to keyword extraction was used in (Liu et al., 2008).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Liu et al., 2008)","Even though the data set in that study is not very big, it seems that a supervised learning approach can achieve reasonable performance for this task.","However, for the less well structured meeting domain (lack of title and paragraph), these kinds of features may not be indicative.",
1139042976,false,finalized,3,1/24/2017 05:43:54,Neut,1.0,N09-1070,"Another line of research for keyword extraction has adopted graph-based methods similar to Google's PageRank algorithm (Brin and Page, 1998).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Brin and Page, 1998)","In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences.","Even though the data set in that study is not very big, it seems that a supervised learning approach can achieve reasonable performance for this task.",
1139042977,false,finalized,3,1/24/2017 05:20:55,Pos,0.6608,N09-1070,"In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Wan et al., 2007)",We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study.,"Another line of research for keyword extraction has adopted graph-based methods similar to Google's PageRank algorithm (Brin and Page, 1998).",
1139042978,false,finalized,3,1/24/2017 05:32:14,Neut,1.0,N09-1070,"The most relevant work to our study is (Plas et al., 2004), where the task is keyword extraction in the multiparty meeting corpus.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Plas et al., 2004)",They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).,Not many studies have been performed on speech transcripts for keyword extraction.,
1139042979,false,finalized,3,1/24/2017 06:22:50,Neut,1.0,N09-1070,"There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Munteanu et al., 2007","(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.",They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).,
1139042980,false,finalized,3,1/24/2017 05:32:14,CoCo,0.3505,N09-1070,"There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"Bulyko et al., 2007","(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.",They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).,
1139042981,false,finalized,3,1/24/2017 05:59:51,Neut,1.0,N09-1070,"There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"Wu et al., 2007","(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.",They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).,
1139042982,false,finalized,3,1/24/2017 05:32:14,Pos,0.6495,N09-1070,"There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"Desilets et al., 2002","(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.",They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).,
1139042983,false,finalized,3,1/24/2017 05:50:54,Neut,0.6512,N09-1070,"There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"Rogina, 2002)","(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.",They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).,
1139042984,false,finalized,3,1/24/2017 05:37:37,Neut,0.6781,N09-1070,"(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Wu et al., 2007)","In (Rogina, 2002), keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures.","There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).",
1139042985,false,finalized,3,1/24/2017 05:51:29,Neut,0.6831,N09-1070,"In (Rogina, 2002), keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Rogina, 2002)",There are many differences between written text and speech - meetings in particular.,"(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.",
1139042986,false,finalized,3,1/24/2017 05:00:10,Neut,0.6548,N09-1070,"We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Janin et al., 2003)","All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).",3 Data,
1139042987,false,finalized,3,1/24/2017 05:32:29,Neut,1.0,N09-1070,"All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Shriberg et al., 2004)","The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus.","We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings.",
1139042988,false,finalized,3,1/24/2017 05:26:35,Neut,1.0,N09-1070,"All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Murray et al., 2005)","The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus.","We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings.",
1139042989,false,finalized,3,1/24/2017 05:56:37,Neut,0.6683,N09-1070,"The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Zhu et al., 2005)","We align the human transcripts and ASR output, then map the human annotated DA boundaries and topic boundaries to the ASR words, such that we have human annotation of these information for the ASR output.","All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).",
1139042990,false,finalized,3,1/24/2017 05:30:41,Pos,0.6803,N09-1070,"We used six meetings as our development set (the same six meetings as the test set in (Murray et al., 2005)) to optimize our keyword extraction methods, and the remaining 21 meetings for final testing in Section 5.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Murray et al., 2005)",One example of the annotated keywords for a topic segment is:,"The average length of the topics (measured using the number of dialog acts) among all the meetings is 172.5, with a high standard deviation of 236.8.",
1139042991,false,finalized,3,1/24/2017 05:42:12,Pos,0.6486,N09-1070,"1We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Galley et al., 2003",the topics discussed and often had trouble deciding the important sentences or keywords.,"Note that these meetings are research discussions, and that the annotators may not be very familiar with",
1139042992,false,finalized,3,1/24/2017 05:26:35,Neut,0.6703,N09-1070,"1We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"Murray et al., 2005)",the topics discussed and often had trouble deciding the important sentences or keywords.,"Note that these meetings are research discussions, and that the annotators may not be very familiar with",
1139042993,false,finalized,3,1/24/2017 05:56:37,Pos,0.6887,N09-1070,"We used the TnT POS tagger (Brants, 2000) trained from the Switchboard data to tag the meeting transcripts.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Brants, 2000)","(C) Integrating word clustering One weakness of the baseline TFIDF is that it counts the frequency for a particular word, without considering any words that are similar to it in terms of semantic meaning.","Our hypothesis is that verb, noun and adjective words are more likely to be keywords, so we restrict our selection to words with these POS tags only.",
1139042994,false,finalized,3,1/24/2017 04:58:54,Pos,0.6803,N09-1070,"We used the SRILM toolkit (Stolcke, 2002) for automatic word clustering over the entire document collection.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Stolcke, 2002)",It minimizes the perplexity of the induced class-based n-gram language model compared to the original word-based model.,Thus we want to assign higher weights to the words in this cluster.,
1139042995,false,finalized,3,1/24/2017 05:52:54,Neut,0.6617,N09-1070,"This score is often used in extractive summarization to select summary sentences (Radev et al., 2001).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Radev et al., 2001)","The cosine similarity between two vectors, D1 and D2, is defined as:",The sentence score is calculated based on its cosine similarity to the entire meeting.,
1139042996,false,finalized,3,1/24/2017 05:10:19,Neut,0.7112,N09-1070,"For the graph-based approach, we adopt the iterative reinforcement approach from (Wan et al., 2007) in the hope of leveraging sentence information for keyword extraction.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Wan et al., 2007)",This algorithm is based on the assumption that important sentences/words are connected to other important sentences/words.,4.2 Graph-based Methods,
1139042997,false,finalized,3,1/24/2017 05:20:55,Neut,0.6548,N09-1070,"These weights are initially added only to the S-W graph, as in (Wan et al., 2007); then that graph is normalized and transposed to create the W-S graph.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Wan et al., 2007)",S-S Graph: The sentence node uses a vector,S-W and W-S Graphs: The weight for an edge between a sentence and a word is the TF of the word in the sentence multiplied by the word's IDF value.,
1139042998,false,finalized,3,1/24/2017 05:05:37,Neut,0.6673,N09-1070,"In (Liu et al., 2008), a lenient metric is used which accounts for some inflection of words.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Liu et al., 2008)","Since that is highly correlated with the results using exact word match, we report results based on strict matching in the following experiments.","Therefore, when the system only generates five keywords, the upper bound of the recall rate may not be 100%.",
1139042999,false,finalized,3,1/24/2017 05:32:52,Neut,0.6492,N09-1070,"The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Nenkova and Passonneau, 2004)","Instead of comparing the system output with each individual human annotation, the method creates a ""pyramid"" using all the human annotated keywords, and then compares system output to this pyramid.","Since that is highly correlated with the results using exact word match, we report results based on strict matching in the following experiments.",
1139043000,false,finalized,3,1/24/2017 05:30:40,Neut,1.0,N09-1070,"For comparison, we also show results using the supervised approach as in (Liu et al., 2008), which is the average of the 21fold cross validation.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Liu et al., 2008)","We only show the maximum F-measure with respect to individual annotations, since the average scores show similar trend.",Table 1 shows the results using human transcripts for different methods on the 21 test meetings (139 topic segments in total).,
1139043001,false,finalized,3,1/24/2017 05:32:29,Weak,0.673,N09-1070,"Unlike the study in (Wan et al., 2007), this information does not yield any gain.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Wan et al., 2007)","We did find that the graph approach performed better in the development set, but it seems that it does not generalize to this test set.",The global information in the S-S graph (connecting a sentence to other sentences in the document) is propagated to the word nodes.,
1139043002,false,finalized,3,1/24/2017 05:39:21,Neut,0.6739,N09-1070,"(Liu et al., 2008) investigated adding bigram key phrases, which we expect to be independent of these unigram-based approaches and adding bigram phrases will yield further performance gain for the unsupervised approach.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf,"(Liu et al., 2008)","Finally, we analyzed if the system's keyword extraction performance is correlated with human annotation disagreement using the unsupervised approach (TFIDF+POS+Sent weight).",Again note that these results used wordbased selection.,
1139043003,false,finalized,3,1/24/2017 05:36:42,Neut,0.6512,N12-4007,"We focus on distributional paraphrasing (Pasca and Dienes 2005; Marton et al., 2009; Marton, to appear 2012).",http://www.aclweb.org/anthology/N/N12/N12-4007.pdf,"Marton et al., 2009","We will also cover pivot paraphrasing (Bannard and Callison-Burch, 2005).","""phrases"" -- and doing so overcoming previous working memory and representation limitations.",
1139043004,false,finalized,3,1/24/2017 05:28:25,Neut,0.6788,N12-4007,"We will also cover pivot paraphrasing (Bannard and Callison-Burch, 2005).",http://www.aclweb.org/anthology/N/N12/N12-4007.pdf,"(Bannard and Callison-Burch, 2005)","We will discuss several weaknesses of distributional paraphrasing, and where the stateof-the-art is.","We focus on distributional paraphrasing (Pasca and Dienes 2005; Marton et al., 2009; Marton, to appear 2012).",
1139043005,false,finalized,3,1/24/2017 05:56:37,Neut,1.0,N12-4007,"(Mohammad et al., EMNLP 2008; Hovy, 2010; Marton et al., WMT 2011).",http://www.aclweb.org/anthology/N/N12/N12-4007.pdf,"Hovy, 2010","Another potential weakness is the difficulty in detecting and generating longer-thanword (phrasal) paraphrases, because pre-calculating a collocation matrix for phrases becomes prohibitive in the matrix size with longer phrases, even with sparse representation.",What else can be done to ameliorate this problem?,
1139043006,false,finalized,3,1/24/2017 05:29:10,Neut,1.0,N12-4007,"There, searching the monolingual text resource is done ondemand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007).",http://www.aclweb.org/anthology/N/N12/N12-4007.pdf,"(Manber and Myers, 1993","This enables constructing large vector representation, since there is no longer a need to compute a whole matrix.","We will present an alternative to pre-calculation: on-demand paraphrasing, as described in Marton (to appear 2012).",
1139043007,false,finalized,3,1/24/2017 05:33:39,Neut,1.0,N12-4007,"There, searching the monolingual text resource is done ondemand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007).",http://www.aclweb.org/anthology/N/N12/N12-4007.pdf,"Gusfield, 1997","This enables constructing large vector representation, since there is no longer a need to compute a whole matrix.","We will present an alternative to pre-calculation: on-demand paraphrasing, as described in Marton (to appear 2012).",
1139043008,false,finalized,3,1/24/2017 05:26:06,Neut,1.0,N12-4007,"There, searching the monolingual text resource is done ondemand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007).",http://www.aclweb.org/anthology/N/N12/N12-4007.pdf,"Lopez, 2007)","This enables constructing large vector representation, since there is no longer a need to compute a whole matrix.","We will present an alternative to pre-calculation: on-demand paraphrasing, as described in Marton (to appear 2012).",
1139043009,false,finalized,3,1/24/2017 05:38:14,Neut,1.0,W08-0315,"Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system (Marino et al., 2006).",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf,"(Marino et al., 2006)","In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006), Callison-Burch et al (2007).",1 Introduction,
1139043010,false,finalized,3,1/24/2017 06:00:44,Neut,0.6492,W08-0315,"In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006), Callison-Burch et al (2007).",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf,(2006),We present a summary of the TALP-UPC Ngrambased SMT system used for this shared task.,"Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system (Marino et al., 2006).",
1139043011,false,finalized,3,1/24/2017 05:06:06,CoCo,0.6608,W08-0315,"In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006), Callison-Burch et al (2007).",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf,(2007),We present a summary of the TALP-UPC Ngrambased SMT system used for this shared task.,"Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system (Marino et al., 2006).",
1139043012,false,finalized,3,1/24/2017 05:37:38,Neut,1.0,W08-0315,"Our translation system implements a log-linear model in which a foreign language sentence fJ1 = f1, f2, ..., fJ is translated into another language eI1 = f1, f2, ..., eI by searching for the translation hypothesis oI1 maximizing a log-linear combination of several feature models (Brown et al., 1990):",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf,"(Brown et al., 1990)","M oI1 = argmax { m=1 E amhm(ef, fJ1 ) eI l 1",2 Ngram-based SMT System,
1139043013,false,finalized,3,1/24/2017 08:34:39,Neut,0.6754,W08-0315,The procedure of tuples extraction from awordto-word alignment according to certain constraints is explained in detail in et al (2006).,http://www.aclweb.org/anthology/W/W08/W08-0315.pdf,(2006),The Ngram-based approach differs fr,"It actually constitutes an Ngram-based LM of bilingual units (called tuples), which approximates the joint probability between the languages under consideration.",
1139043014,false,finalized,3,1/24/2017 05:46:03,Neut,0.6683,W08-0315," a target language model (a 4-gram model of words, estimated with Kneser-Ney smoothing);  a POS target language model (a 4-gram model of tags with Good-Turing discounting (TPOS));  a word bonus model, which is used to compensate the system's preference for short output sentences;  a source-to-target lexicon model and a target-tosource lexicon model, these models use word-toword IBM Model 1 probabilities (Och and Ney, 2004) to estimate the lexical weights for each tuple in the translation table.",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf,"(Och and Ney, 2004)",Decisions on the particular LM configuration and smoothing technique were taken on the minimalperplexity and maximal-BLEU bases.,"The TALP-UPC 2008 translation system, besides the bilingual translation model, which consists of a 4-gram LM of tuples with Kneser-Ney discounting (estimated with SRI Language Modeling Toolkit1), implements a log-linear combination of five additional feature models:",
1139043015,false,finalized,3,1/24/2017 05:59:51,Pos,0.6683,W08-0315,A detailed description can be found in Crego and Marino (2006).,http://www.aclweb.org/anthology/W/W08/W08-0315.pdf,(2006),1http://www.speech.sri.com/projects/srilm/ 2http://gps-tsc.upc.es/veu/soft/soft/marie/,In our system we use an extended monotone reordering model based on automatically learned reordering rules.,
1139043016,false,finalized,3,1/24/2017 05:26:35,Neut,0.6703,W08-0315,"Patterns are extracted in training from the crossed links found in the word alignment, in other words, found in translation tuples (as no word within a tuple can be linked to a word out of it (Crego and Marino, 2006)).",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf,"(Crego and Marino, 2006)","Having all the instances of rewrite patterns, a score for each pattern on the basis of relative frequency is calculated as shown below:","where t1, ..., tn is a sequence of POS tags (relating a sequence of source words), and i1, ..., in indicates which order of the source words generate monotonically the target words.",
1139043017,false,finalized,3,1/24/2017 05:05:37,Neut,0.673,W08-0315,"The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English.",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf,"(Carreras et al., 2004)","The number of classes for English is 44, while Spanish is considered as a more inflectional language, and the tag set contains 376 different tags.",POS information for the source and the target languages was considered for both translation tasks that we have participated.,
1139043018,false,finalized,3,1/24/2017 05:43:54,Pos,0.3514,W08-0315,"The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English.",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf,"(Brants, 2000)","The number of classes for English is 44, while Spanish is considered as a more inflectional language, and the tag set contains 376 different tags.",POS information for the source and the target languages was considered for both translation tasks that we have participated.,
1139043019,false,finalized,3,1/24/2017 05:26:06,Neut,1.0,W08-0315,"The word alignment is automatically computed by using GIZA++4(Och and Ney, 2000) in both directions, which are symmetrized by using the union operation.",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf,"(Och and Ney, 2000)","Instead of aligning words themselves, stems are used for aligning.",Word Alignment.,
1139043020,false,finalized,3,1/24/2017 05:39:05,Neut,0.6887,W08-0315,"As a post-processing, in the En2Es direction we used a POS target LM as a feature (instead of the target language model based on classes) that allowed to recover the segmentations (de Gispert, 2006).",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf,"(de Gispert, 2006)",4.3 Experiments and Results,"In particular, the pronouns attached to the verb were separated and contractions as del or al were splitted into de el or a el.",
1139043021,false,finalized,3,1/24/2017 05:05:37,Neut,0.6673,W04-3237,"Similar to the work in (Lita et al., 2003), we tag each word in a sentence with one of the tags:",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Lita et al., 2003)", LOC lowercase  CAP capitalized  MXC mixed case; no further guess is made as to the capitalization of such words.,Automatic capitalization can be seen as a sequence tagging problem: each lower-case word receives a tag that describes its capitalization form.,
1139043022,false,finalized,3,1/24/2017 05:27:35,Neut,0.6884,W04-3237,"Due to its popularity, both our work and that of (Lita et al., 2003) uses the 1-gram capitalizer as a baseline.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Lita et al., 2003)","The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well.","2As with everything in natural language, it is not hard to find exceptions to this ""rule"".",
1139043023,false,finalized,3,1/24/2017 05:00:10,Neut,0.6548,W04-3237,"The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Kim and Woodland, 2004)",2.2 Previous Work,"Due to its popularity, both our work and that of (Lita et al., 2003) uses the 1-gram capitalizer as a baseline.",
1139043024,false,finalized,3,1/24/2017 05:40:33,Pos,1.0,W04-3237,"We share the approach to capitalization as sequence tagging with that of (Lita et al., 2003).",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Lita et al., 2003)","In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques.",2.2 Previous Work,
1139043025,false,finalized,3,1/24/2017 05:39:05,Neut,0.3482,W04-3237,"The same idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from speech recognition output.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Kim and Woodland, 2004)","A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors.","In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques.",
1139043026,false,finalized,3,1/24/2017 05:24:48,Neut,0.6884,W04-3237,"A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Brill, 1994)","Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996).","The same idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from speech recognition output.",
1139043027,false,finalized,3,1/24/2017 05:26:06,Neut,0.6495,W04-3237,"Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996).",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Ratnaparkhi, 1996)","The MEMM approach models the tag sequence T conditionally on the word sequence W, which has a few substantial advantages over the 1-gram tagging approach:","A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors.",
1139043028,false,finalized,3,1/24/2017 05:30:41,CoCo,0.3549,W04-3237,"More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al., 2001) which slightly outperforms MEMMs on a standard partof-speech tagging task.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Lafferty et al., 2001)","In a similar vein, the work"," discriminative training of probability model P(T|W) using conditional maximum likelihood is well correlated with tagging accuracy  ability to use a rich set of word-level features in a parsimonious way: sub-word features such as prefixes and suffixes, as well as future words3 are easily incorporated in the probability model  no concept of ""out-of-vocabulary"" word: subword features are very useful in dealing with words not seen in the training data  ability to integrate rich contextual features into the model",
1139043029,false,finalized,3,1/24/2017 05:24:48,Neut,0.6884,W04-3237,"of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Collins, 2002)","The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).","3Relative to the current word, whose tag is assigned a probability value by the MEMM.",
1139043030,false,finalized,3,1/24/2017 05:29:32,Neut,0.6477,W04-3237,"The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Chen and Rosenfeld, 2000)","We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).","of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs.",
1139043031,false,finalized,3,1/24/2017 04:50:37,Neut,0.6776,W04-3237,"We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Goodman, 2004)","Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario.","The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).",
1139043032,false,finalized,3,1/24/2017 05:33:39,Neut,1.0,W04-3237,"A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Brill, 1994)",3 MEMM for Sequence Labeling,"Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario.",
1139043033,false,finalized,3,1/24/2017 05:32:29,Neut,0.6673,W04-3237,"A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Ratnaparkhi, 1996)",3 MEMM for Sequence Labeling,"Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario.",
1139043034,false,finalized,3,1/24/2017 05:32:14,Pos,0.6795,W04-3237,"The approach we took is the one in (Ratnaparkhi, 1996), which uses xi(W, T i1 1 ) = {wi, wi1, wi+1, ti1, ti2}.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Ratnaparkhi, 1996)","We note that the probability model is causal in the sequencing of tags (the probability assignment for ti only depends on previous tags ti1, ti2) which allows for efficient algorithms that search for the most likely tag sequence T(W) = arg maxT P(T |W) as well as ensures a properly normalized conditional probability model P(T|W).",model is built.,
1139043035,false,finalized,3,1/24/2017 05:51:29,Neut,0.6486,W04-3237,"The next section briefly describes the training procedure; for details the reader is referred to (Berger et al., 1996).",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Berger et al., 1996)",3.1 Maximum Entropy State Transition Model,"The probability P(ti|xi(W,T i1 1 )) is modeled using a maximum entropy model.",
1139043036,false,finalized,3,1/24/2017 06:04:01,Neut,1.0,W04-3237,"The model parameters A are estimated such that the model assigns maximum log-likelihood to the training data subject to a Gaussian prior centered at 0, A  N(0, diag(u2i )), that ensures smoothing (Chen and Rosenfeld, 2000):",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Chen and Rosenfeld, 2000)","L(A) = E p(x, y)log pA(y|x) (1) x,y n P(T|W) = i=1 E Z(x, A) = y i 2' + const(A)i",3.1.2 Parameter Estimation,
1139043037,false,finalized,3,1/24/2017 05:59:51,Neut,0.6486,W04-3237,"As shown in (Chen and Rosenfeld, 2000) - and rederived in Appendix A for the non-zero mean case - the update equations are:",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Chen and Rosenfeld, 2000)","(t+1) i= (t) i+ i, where i satisfies: i p(x, y)fi(x, y)  = 2 i p(x)p(y|x)fi(x, y)exp(if#(x, y))","L(A) = E p(x, y)log pA(y|x) (1) x,y n P(T|W) = i=1 E Z(x, A) = y i 2' + const(A)i",
1139043038,false,finalized,3,1/24/2017 06:21:55,Neut,1.0,W04-3237,"A Gaussian prior for the model parameters A has been previously used in (Chen and Rosenfeld, 2000) for smoothing MaxEnt models.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Chen and Rosenfeld, 2000)","The prior has 0 mean and diagonal covariance: A  N(0, diag(2 i)).",A simple way to accomplish this is to use MAP adaptation using a prior distribution on the model parameters.,
1139043039,false,finalized,3,1/24/2017 05:38:14,Neut,1.0,W04-3237,"ing (Pietra et al., 1995) between the background exponential model B - assumed fixed - and an exponential model A built using the Fbackground U Fadapt feature set.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Pietra et al., 1995)","It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0 - following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models - then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed5.","F  i=1 E x,y i + (2) 2i E x,y F  i=1",
1139043040,false,finalized,3,1/24/2017 06:00:44,Neut,1.0,W04-3237,"It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0 - following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models - then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed5.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Chen and Rosenfeld, 2000)","However, we wish to point out that the equivalence holds only if the feature set for the new model A is Fbackground U Fadapt.","ing (Pietra et al., 1995) between the background exponential model B - assumed fixed - and an exponential model A built using the Fbackground U Fadapt feature set.",
1139043041,false,finalized,3,1/24/2017 04:51:57,Neut,1.0,W04-3237,"The baseline 1-gram and the background MEMM capitalizer were trained on various amounts of WSJ (Paul and Baker, 1992) data from 1987 - files WS87_{001-126}.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf,"(Paul and Baker, 1992)",The in-domain test data used was file WS94_000 (8.7kwds).,5 Experiments,
1139043042,false,finalized,3,1/24/2017 06:19:11,Neut,1.0,P13-1112,"Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(2001),"In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues.","The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing.",
1139043043,false,finalized,3,1/24/2017 05:46:03,Neut,0.6683,P13-1112,"Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(1998),"In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues.","The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing.",
1139043044,false,finalized,3,1/24/2017 05:38:51,Neut,1.0,P13-1112,"Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(2001),"In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues.","The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing.",
1139043045,false,finalized,3,1/24/2017 05:36:11,Neut,1.0,P13-1112,"Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(1998),"In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues.","The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing.",
1139043046,false,finalized,3,1/24/2017 05:35:02,Neut,1.0,P13-1112,"Besides, Wong and Dras (2009) show that there are no significant differences, between mother tongues, in the misuse of certain syntactic features such as subject-verb agreement that have different tendencies depending on their mother tongues.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(2009),"Considering these, one could not be so sure which argument is correct.","A similar argument can be made about some parts of gender, tense, and aspect systems.",
1139043047,false,finalized,3,1/24/2017 05:46:03,Neut,0.6831,P13-1112,"This becomes important in native language identification) which is useful for improving grammatical error correction systems (Chodorow et al., 2010) or for providing more targeted feedback to language learners.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Chodorow et al., 2010)",As we will see in Sect.,"One of the major contributions of this work is to reveal and visualize a language family tree preserved in non-native texts, by examining the hypothesis.",
1139043048,false,finalized,3,1/24/2017 05:56:37,Pos,1.0,P13-1112,"In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Enright and Kondrak, 2011",The rest of this paper is structured as follows.,"6, this paper reveals several crucial findings that contribute to improving native language identification.",
1139043049,false,finalized,3,1/24/2017 05:17:14,Neut,1.0,P13-1112,"In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"Gray and Atkinson, 2003",The rest of this paper is structured as follows.,"6, this paper reveals several crucial findings that contribute to improving native language identification.",
1139043050,false,finalized,3,1/24/2017 05:53:40,Pos,0.6744,P13-1112,"In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"Barbancon et al., 2007",The rest of this paper is structured as follows.,"6, this paper reveals several crucial findings that contribute to improving native language identification.",
1139043051,false,finalized,3,1/24/2017 05:25:21,Neut,0.6542,P13-1112,"In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"Batagelj et al., 1992",The rest of this paper is structured as follows.,"6, this paper reveals several crucial findings that contribute to improving native language identification.",
1139043052,false,finalized,3,1/24/2017 05:37:38,Neut,0.6545,P13-1112,"In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"Nakhleh et al., 2005)",The rest of this paper is structured as follows.,"6, this paper reveals several crucial findings that contribute to improving native language identification.",
1139043053,false,finalized,3,1/24/2017 05:37:37,Neut,1.0,P13-1112,"To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother tongue is one of the Indo-European languages (Beekes, 2011; Ramat and Ramat, 2006).",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Beekes, 2011","If the reconstructed tree is sufficiently similar to the original Indo-European family tree, it will support the hypothesis.",2 Approach,
1139043054,false,finalized,3,1/24/2017 05:20:11,Neut,0.6925,P13-1112,"To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother tongue is one of the Indo-European languages (Beekes, 2011; Ramat and Ramat, 2006).",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"Ramat and Ramat, 2006)","If the reconstructed tree is sufficiently similar to the original Indo-European family tree, it will support the hypothesis.",2 Approach,
1139043055,false,finalized,3,1/24/2017 06:04:01,Neut,0.6496,P13-1112,"The approach we use for reconstructing a language family tree is to apply agglomerative hierarchical clustering (Han and Kamber, 2006) to English texts written by non-native speakers.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Han and Kamber, 2006)",Researchers have already performed related work on reconstructing language family trees.,"If not, it suggests that some features other than mother tongue interference are more influential.",
1139043056,false,finalized,3,1/24/2017 05:39:36,Neut,0.6838,P13-1112,"For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(1937),"More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering.",Researchers have already performed related work on reconstructing language family trees.,
1139043057,false,finalized,3,1/24/2017 05:34:08,Neut,1.0,P13-1112,"For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(1959),"More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering.",Researchers have already performed related work on reconstructing language family trees.,
1139043058,false,finalized,3,1/24/2017 05:29:32,Neut,1.0,P13-1112,"More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(1992),"Among them, the 'Recently, native language identification has drawn the attention of NLP researchers.","For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.",
1139043059,false,finalized,3,1/24/2017 05:30:40,Neut,1.0,P13-1112,"More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(1999),"Among them, the 'Recently, native language identification has drawn the attention of NLP researchers.","For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.",
1139043060,false,finalized,3,1/24/2017 05:32:52,Neut,1.0,P13-1112,most related method is that of Kita (1999).,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(1999),"In his method, a variety of languages are modeled by their spelling systems (i.e., character-based n-gram language models).","For instance, a shared task on native language identification took place at an NAACL-HLT 2013 workshop.",
1139043061,false,finalized,3,1/24/2017 05:39:05,Neut,0.6887,P13-1112,The similarity used for clustering is based on a divergence-like distance between two language models that was originally proposed by Juang and Rabiner (1985).,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(1985),This method is purely data-driven and does not require human expert knowledge for the selection of linguistic features.,"Then, agglomerative hierarchical clustering is applied to the language models to reconstruct a language family tree.",
1139043062,false,finalized,3,1/24/2017 05:55:36,Neut,1.0,P13-1112,"For instance, Batagelj et al (1992) use basic vocabularies such as belly in English and ventre in French to measure similarity between languages.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(1992),"Obviously, this does not work on our task; belly is belly in English writing whoever writes it.",This significant difference prevents us from directly applying techniques in the literature to our task.,
1139043063,false,finalized,3,1/24/2017 05:26:06,Neut,0.6583,P13-1112,"We set n = 3 (i.e., trigram language model) following Kita's work and use Kneser-Ney (KN) smoothing (Kneser and Ney, 1995) to estimate its conditional probabilities.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Kneser and Ney, 1995)","With Mi and Di, we can naturally apply Kita's method to our task.","Now, the language model Mi can be built from Di.",
1139043064,false,finalized,3,1/24/2017 05:58:45,Neut,0.6486,P13-1112,"We selected the ICLE corpus v.2 (Granger et al., 2009) as the target language data.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Granger et al., 2009)",It consists of English essays written by a wide variety of nonnative speakers of English.,4 Experiments,
1139043065,false,finalized,3,1/24/2017 05:35:02,Neut,0.6588,P13-1112,"For reference, we also used native English (British and American university students' essays in the LOCNESS corpus5) and two sets of Japanese English (ICLE and the NICE corpus (Sugiura et al., 2007)).",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Sugiura et al., 2007)",Table 1 shows the statistics on the corpus data.,"Also, the symbols ' and ' were unified into '4.",
1139043066,false,finalized,3,1/24/2017 06:19:11,Weak,0.3469,P13-1112,"Considering this, we tested CRFTagger6 on non-native English texts containing various grammatical errors before the experiments (Nagata et al., 2011).",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Nagata et al., 2011)",It turned out that CRFTagger achieved an accuracy of 0.932 (compared to 0.970 on native texts).,Existing POS taggers might not perform well on non-native English texts because they are normally developed to analyze native English texts.,
1139043067,false,finalized,3,1/24/2017 06:19:11,Neut,0.664,P13-1112,The tree at the top is the Indo-European family tree drawn based on the figure shown in Crystal (1997).,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(1997),"It shows that the 11 languages are divided into three groups: Italic, Germanic, and Slavic branches.",Fig.1 shows the experimental results.,
1139043068,false,finalized,3,1/24/2017 05:58:52,Neut,0.6539,P13-1112,"This corresponds to the fact that noun-noun compounds are less common in the Italic languages than in English and that instead, the of-phrase (NN of NN) is preferred (Swan and Smith, 2001).",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Swan and Smith, 2001)",For,4.,
1139043069,false,finalized,3,1/24/2017 05:38:14,Neut,1.0,P13-1112,"5 shows that the average length roughly distinguishes the Italic Englishes from the other nonnative Englishes; French-English is the shortest, which is explained by the discussion above, while Dutch- and German-Englishes are longest, which may correspond to the fact that they have a preference for noun-noun compounds as Snyder (1996) argues.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(1996),"For instance, German allows the concatenated form as in Orangensaft (equivalently orangejuice).",Fig.,
1139043070,false,finalized,3,1/24/2017 05:29:32,Neut,1.0,P13-1112,"Researchers including Wong and Dras (2009), Wong et al (2011; 2012), and Koppel et al (2005) work on native language identification and show that machine learning-based methods are effective.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(2009),"Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.",6 Implications for Work in Related Domains,
1139043071,false,finalized,3,1/24/2017 05:30:41,Neut,0.6747,P13-1112,"Researchers including Wong and Dras (2009), Wong et al (2011; 2012), and Koppel et al (2005) work on native language identification and show that machine learning-based methods are effective.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(2005),"Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.",6 Implications for Work in Related Domains,
1139043072,false,finalized,3,1/24/2017 05:32:14,Neut,0.6739,P13-1112,"Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,(2009),"Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identification because translation is not typically performed by non-native speakers but rather native speakers of the target language11.","Researchers including Wong and Dras (2009), Wong et al (2011; 2012), and Koppel et al (2005) work on native language identification and show that machine learning-based methods are effective.",
1139043073,false,finalized,3,1/24/2017 05:06:06,Pos,0.6608,P13-1112,"Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identification because translation is not typically performed by non-native speakers but rather native speakers of the target language11.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Koppel and Ordan, 2011",The experimental results show that n-grams containing articles are predictive for identifying native languages.,"Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.",
1139043074,false,finalized,3,1/24/2017 05:35:02,Neut,1.0,P13-1112,"Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identification because translation is not typically performed by non-native speakers but rather native speakers of the target language11.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"van Halteren, 2008)",The experimental results show that n-grams containing articles are predictive for identifying native languages.,"Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.",
1139043075,false,finalized,3,1/24/2017 05:38:51,Neut,0.6788,P13-1112,"7 shows that the observed values in the FrenchEnglish data very closely fit the theoretical proba""For comparison, we conducted a pilot study where we reconstructed a language family tree from English texts in European Parliament Proceedings Parallel Corpus (Europarl) (Koehn, 2011).",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Koehn, 2011)",It turned out that the reconstructed tree was different from the canonical tree (available at http: //web.hyogo-u.ac.jp/nagata/acl/).,Fig.,
1139043076,false,finalized,3,1/24/2017 05:58:52,Neut,1.0,P13-1112,"Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Enright and Kondrak, 2011","Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.","In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.",
1139043077,false,finalized,3,1/24/2017 05:36:11,Neut,1.0,P13-1112,"Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"Gray and Atkinson, 2003","Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.","In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.",
1139043078,false,finalized,3,1/24/2017 08:34:39,Neut,1.0,P13-1112,"Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"Barbancon et al., 2007","Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.","In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.",
1139043079,false,finalized,3,1/24/2017 05:20:55,Neut,0.6548,P13-1112,"Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"Batagelj et al., 1992","Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.","In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.",
1139043080,false,finalized,3,1/24/2017 05:00:10,Neut,0.6548,P13-1112,"Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"Nakhleh et al., 2005)","Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.","In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.",
1139043081,false,finalized,3,1/24/2017 05:58:45,Neut,1.0,P13-1112,"Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"(Kita, 1999","These methods reconstruct language family trees based on linguistic features that exist within words including lexical, phonological, and morphological features.","Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.",
1139043082,false,finalized,3,1/24/2017 05:36:06,Neut,1.0,P13-1112,"Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf,"Rama and Singh, 2009)","These methods reconstruct language family trees based on linguistic features that exist within words including lexical, phonological, and morphological features.","Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.",
1139043083,false,finalized,3,1/24/2017 08:34:39,Neut,0.6754,W03-1717,"The approach is in the spirit of Smadja (1993) on retrieving collocations from text corpora, but is more integrated with parsing.",http://www.aclweb.org/anthology/W/W03/W03-1717.pdf,(1993),We will show in the evaluation section how much the learned knowledge can help improve sentence analysis.,"In the rest of this paper, we will present a learning procedure that learns those relations by processing a large corpus with a chart-filter, a treefilter and an LLR filter.",
1139043084,false,finalized,3,1/24/2017 05:36:42,Pos,0.6707,W03-1717,"To prune away those pairs, we used the log-likelihood-ratio algorithm (Dunning, 1993) to compute the degree of association between the verb and the noun in each pair.",http://www.aclweb.org/anthology/W/W03/W03-1717.pdf,"(Dunning, 1993)","Pairs where there is high ""mutual information"" between the verb and noun would receive higher scores while pairs where the verb can co-occur with many different nouns would receive lower scores.","Secondly, those verbs tend not to occur in the modifier-head relation with a following noun and we gain very little in terms of disambiguation by storing those pairs in the knowledge base.",
1139043085,false,finalized,3,1/24/2017 05:53:40,Neut,0.6817,W14-6700,"21me Traitement Automatique des Langues Naturelles, Marseille, 2014",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"me Traitement Automatique des Langues Naturelles, Marseille, 2014",Actes de l'atelier  Reseaux Lexicaux et Traitement des Langues Naturelles.,,
1139043086,false,finalized,3,1/24/2017 06:04:06,Neut,1.0,W14-6700,"Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"(Barrat, 2008, ","En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.","Parallelement  l'evolution des ressources lexicales, on a pu observer une explosion de travaux portant sur les graphes (graphes complexes, phenomene 'petit monde', etc.).",
1139043087,false,finalized,3,1/24/2017 05:39:36,Neut,0.6838,W14-6700,"Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"Barabsi, 2003)","En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.","Parallelement  l'evolution des ressources lexicales, on a pu observer une explosion de travaux portant sur les graphes (graphes complexes, phenomene 'petit monde', etc.).",
1139043088,false,finalized,3,1/24/2017 06:04:06,Neut,0.6801,W14-6700,"En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"(Bieman, 2012","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.","Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.",
1139043089,false,finalized,3,1/24/2017 05:50:54,Neut,1.0,W14-6700,"En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"Mihalcea et Radev, 2011","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.","Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.",
1139043090,false,finalized,3,1/24/2017 06:21:55,Neut,0.3514,W14-6700,"En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"Widdows, 2004","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.","Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.",
1139043091,false,finalized,3,1/24/2017 05:56:05,Neut,0.3587,W14-6700,"En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"Sowa, 1991)","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.","Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.",
1139043092,false,finalized,3,1/24/2017 04:58:54,Neut,0.6451,W14-6700,"En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"(Schvaneveldt, 1989, ","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.","Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.",
1139043093,false,finalized,3,1/24/2017 06:24:43,Error,0.6284,W14-6700,"En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"Nelson et al., 1998)","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.","Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.",
1139043094,false,finalized,3,1/24/2017 05:36:06,Neut,1.0,W14-6700,"2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"(Vitevitch, 2008)",Les graphes sont essentiellement une forme de representation mathematique et visuelle des relations entre des objets/entites.,"En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.",
1139043095,false,finalized,3,1/24/2017 05:56:05,Neut,0.6973,W14-6700,"2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"(Ferrer i Cancho & Sole, 2001)",Les graphes sont essentiellement une forme de representation mathematique et visuelle des relations entre des objets/entites.,"En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.",
1139043096,false,finalized,3,1/24/2017 05:12:16,Neut,1.0,W14-6700,"2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"(Dion, 2012)",Les graphes sont essentiellement une forme de representation mathematique et visuelle des relations entre des objets/entites.,"En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.",
1139043097,false,finalized,3,1/24/2017 06:04:06,Neut,0.6801,W14-6700,"Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"(Widdows, 2004, ","Il n'est pas facile de savoir ce qui a pu causer ce 'silence', car il contraste 6norm6ment avec le succes d'un autre 6v6nement, consacr6  des problemes tres voisins : CogALex (http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/index.html).",4 Conclusion,
1139043098,false,finalized,3,1/24/2017 05:33:39,Neut,0.6868,W14-6700,"Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"Vitevitch, 2008)","Il n'est pas facile de savoir ce qui a pu causer ce 'silence', car il contraste 6norm6ment avec le succes d'un autre 6v6nement, consacr6  des problemes tres voisins : CogALex (http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/index.html).",4 Conclusion,
1139043099,false,finalized,3,1/24/2017 05:36:42,Neut,1.0,W14-6700,"Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"(Sahlgren, 2008)","Il n'est pas facile de savoir ce qui a pu causer ce 'silence', car il contraste 6norm6ment avec le succes d'un autre 6v6nement, consacr6  des problemes tres voisins : CogALex (http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/index.html).",4 Conclusion,
1139043100,false,finalized,3,1/24/2017 05:42:12,Neut,0.6831,W14-6700,"Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf,"(Baroni et Lenci, 2010)","Il n'est pas facile de savoir ce qui a pu causer ce 'silence', car il contraste 6norm6ment avec le succes d'un autre 6v6nement, consacr6  des problemes tres voisins : CogALex (http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/index.html).",4 Conclusion,
1139043101,false,finalized,3,1/24/2017 05:39:21,Neut,0.6739,N06-1017,"This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Markou and Singh, 2003a","Outlier detection approaches typically derive some model of ""normal"" objects from the training set and use a distance measure and a threshold to detect abnormal items.","If a system has seen only positive examples, how does it recognize a negative example?",
1139043102,false,finalized,3,1/24/2017 05:56:37,Neut,1.0,N06-1017,"This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"Markou and Singh, 2003b","Outlier detection approaches typically derive some model of ""normal"" objects from the training set and use a distance measure and a threshold to detect abnormal items.","If a system has seen only positive examples, how does it recognize a negative example?",
1139043103,false,finalized,3,1/24/2017 05:34:08,Pos,0.3504,N06-1017,"This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"Marsland, 2003)","Outlier detection approaches typically derive some model of ""normal"" objects from the training set and use a distance measure and a threshold to detect abnormal items.","If a system has seen only positive examples, how does it recognize a negative example?",
1139043104,false,finalized,3,1/24/2017 05:55:36,Neut,1.0,N06-1017,"Unknown sense detection is related to word sense disambiguation (WSD) and to word sense discrimination (Schutze, 1998), but differs from both.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Schutze, 1998)","In WSD all senses are assumed known, and the task is to select one of them, while in unknown sense detection the task is to decide whether a given occurrence matches any of the known senses or none of them, and all training instances, regardless of the sense to which they belong, are modeled as one group of known data.",against which new occurrences are compared will consist of sense-annotated text.,
1139043105,false,finalized,3,1/24/2017 05:58:45,Neut,0.6486,N06-1017,"Figure 1 shows an example, a sentence from the Hound of the Baskervilles, analyzed by the SHALMANESER (Erk and Pado, 2006) shallow semantic parser.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Erk and Pado, 2006)","The analysis is based on FrameNet (Baker et al., 1998), a resource that lists senses and semantic roles for English expressions.","In cases where a sense is missing from the inventory, WSD will wrongly assign one of the existing senses.",
1139043106,false,finalized,3,1/24/2017 05:17:14,Neut,0.6496,N06-1017,"The analysis is based on FrameNet (Baker et al., 1998), a resource that lists senses and semantic roles for English expressions.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Baker et al., 1998)","FrameNet is lacking a sense of ""expectation"" or ""being mentally prepared"" for the verb prepare, so prepared has been assigned the sense COOKING CREATION, a possible but improbable analysis2.","Figure 1 shows an example, a sentence from the Hound of the Baskervilles, analyzed by the SHALMANESER (Erk and Pado, 2006) shallow semantic parser.",
1139043107,false,finalized,3,1/24/2017 04:58:54,Pos,0.3549,N06-1017,"In this paper we model unknown sense detection as outlier detection, using a simple Nearest Neighbor-based method (Tax and Duin, 2000) that compares the local probability density at each test item with that of its nearest training item.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Tax and Duin, 2000)","To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.","Our study will be evaluated on FrameNet because of our main aim of improving shallow semantic parsing, but the method we propose is applicable to any sense inventory that has annotated data; in particular, it is also applicable to WordNet.",
1139043108,false,finalized,3,1/24/2017 05:10:19,Neut,0.7112,N06-1017,"There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Widdows, 2003",Plan of the paper.,"To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.",
1139043109,false,finalized,3,1/24/2017 06:00:44,Neut,1.0,N06-1017,"There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"Curran, 2005",Plan of the paper.,"To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.",
1139043110,false,finalized,3,1/24/2017 05:32:29,Neut,1.0,N06-1017,"There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"Burchardt et al., 2005)",Plan of the paper.,"To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.",
1139043111,false,finalized,3,1/24/2017 05:35:02,Neut,1.0,N06-1017,"Frame Semantics (Fillmore, 1982) models the meanings of a word or expression by reference to frames which describe the background and situational knowledge necessary for understanding what the predicate is ""about"".",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Fillmore, 1982)",Each frame provides its specific set of semantic roles.,2 FrameNet,
1139043112,false,finalized,3,1/24/2017 05:51:26,Neut,1.0,N06-1017,"The Berkeley FrameNet project (Baker et al., 1998) is building a semantic lexicon for English describing the frames and linking them to the words and expressions that can evoke them.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Baker et al., 1998)","These can be verbs as well as nouns, adjectives, prepositions, adverbs, and multiword expressions.",Each frame provides its specific set of semantic roles.,
1139043113,false,finalized,3,1/24/2017 05:39:21,Neut,1.0,N06-1017,in Tatu and Moldovan (2005).,http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,(2005),"For example, the verb prepare from Figure 1 is associated with the frames","This is problematic for the use of FrameNet analyses as a basis for inferences over text, as e.g.",
1139043114,false,finalized,3,1/24/2017 05:40:33,Pos,0.3488,N06-1017,"They were parsed with Minipar (Lin, 1993); named entities were computed using Heart of Gold (Callmeier et al., 2004).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Lin, 1993)",4 Experiment 1: WSD confidence scores,"After removal of instances that were annotated with more than one sense, we obtain 26,496 annotated sentences for the 1,031 ambiguous lemmas.",
1139043115,false,finalized,3,1/24/2017 04:50:37,Neut,1.0,N06-1017,"They were parsed with Minipar (Lin, 1993); named entities were computed using Heart of Gold (Callmeier et al., 2004).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Callmeier et al., 2004)",4 Experiment 1: WSD confidence scores,"After removal of instances that were annotated with more than one sense, we obtain 26,496 annotated sentences for the 1,031 ambiguous lemmas.",
1139043116,false,finalized,3,1/24/2017 05:24:48,Neut,0.6884,N06-1017,"We test whether the WSD system built into SHALMANESER (Erk, 2005) can distinguish known sense items from unknown sense items reliably by its confidence scores.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Erk, 2005)","The system extracts a rich feature set, which forms the basis of all three experiments in this paper:",Modeling.,
1139043117,false,finalized,3,1/24/2017 05:39:05,Neut,1.0,N06-1017,The feature set is based on Florian et al (2002) but contains additional syntax-related features.,http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,(2002),"Each word-related feature is represented as four features for word, lemma, part of speech, and named entity.","for verb targets, the target voice.",
1139043118,false,finalized,3,1/24/2017 05:46:03,Neut,1.0,N06-1017,"Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Markou and Singh, 2003a","Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).","In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not.",
1139043119,false,finalized,3,1/24/2017 05:17:14,Neut,0.6788,N06-1017,"Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"Markou and Singh, 2003b","Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).","In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not.",
1139043120,false,finalized,3,1/24/2017 05:50:54,Neut,0.6781,N06-1017,"Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"Marsland, 2003)","Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).","In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not.",
1139043121,false,finalized,3,1/24/2017 05:43:54,Neut,0.6683,N06-1017,"Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Hickinbotham and Austin, 2000)","One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.","Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).",
1139043122,false,finalized,3,1/24/2017 05:27:35,Neut,1.0,N06-1017,"Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Tax and Duin, 1998","One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.","Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).",
1139043123,false,finalized,3,1/24/2017 05:57:49,Neut,1.0,N06-1017,"Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"Scholkopf et al., 2000)","One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.","Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).",
1139043124,false,finalized,3,1/24/2017 05:37:37,Neut,0.6781,N06-1017,"Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Yeung and Chow, 2002","One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.","Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).",
1139043125,false,finalized,3,1/24/2017 05:28:25,Pos,0.6715,N06-1017,"Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"Dasgupta and Forrest, 1999)","One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.","Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).",
1139043126,false,finalized,3,1/24/2017 05:10:19,Neut,1.0,N06-1017,"Rather than estimating the complete density function, Tax and Duin (2000) approximate local density at the test object by comparing distances between nearest neighbors.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,(2000),"Given a test object x, the approach considers the training object t nearest to x and compares the distance dxt between x and t to the distance dtt, between t and its own nearest training data neighbor t'.","One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.",
1139043127,false,finalized,3,1/24/2017 06:21:55,Neut,0.6486,N06-1017,"Nearest neighbors (by Euclidean distance) were computed using the ANN tool (Mount and Arya, 2005).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Mount and Arya, 2005)",We compute one outlier detection model per lemma.,"We model unknown sense detection as an outlier detection task, using Tax and Duin's outlier detection approach that we have outlined in the previous section.",
1139043128,false,finalized,3,1/24/2017 04:51:57,Neut,0.6583,N06-1017,"Furthermore, more sophisticated feature vectors can be employed to generalize over context words, and other outlier detection approaches (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003) can be tested on this task.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Markou and Singh, 2003a","Our immediate goal is to use unknown sense detection in combination with WSD, to filter out items that the WSD system cannot handle due to missing senses.",One obvious possibility is the extension of Tax and Duin's method to more than one nearest training neighbor for a more accurate estimate of local density.,
1139043129,false,finalized,3,1/24/2017 05:29:32,Pos,0.6697,N06-1017,"Furthermore, more sophisticated feature vectors can be employed to generalize over context words, and other outlier detection approaches (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003) can be tested on this task.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"Markou and Singh, 2003b","Our immediate goal is to use unknown sense detection in combination with WSD, to filter out items that the WSD system cannot handle due to missing senses.",One obvious possibility is the extension of Tax and Duin's method to more than one nearest training neighbor for a more accurate estimate of local density.,
1139043130,false,finalized,3,1/24/2017 05:06:06,Neut,0.6548,N06-1017,"Furthermore, more sophisticated feature vectors can be employed to generalize over context words, and other outlier detection approaches (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003) can be tested on this task.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"Marsland, 2003)","Our immediate goal is to use unknown sense detection in combination with WSD, to filter out items that the WSD system cannot handle due to missing senses.",One obvious possibility is the extension of Tax and Duin's method to more than one nearest training neighbor for a more accurate estimate of local density.,
1139043131,false,finalized,3,1/24/2017 05:25:21,Neut,1.0,N06-1017,"Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"(Widdows, 2003",,"Once items have been identified as unknown, they are available for further processing: If possible one would like to assign some measure of sense information even to these items.",
1139043132,false,finalized,3,1/24/2017 05:20:11,Neut,1.0,N06-1017,"Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"Curran, 2005",,"Once items have been identified as unknown, they are available for further processing: If possible one would like to assign some measure of sense information even to these items.",
1139043133,false,finalized,3,1/24/2017 06:04:01,Neut,1.0,N06-1017,"Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf,"Burchardt et al., 2005)",,"Once items have been identified as unknown, they are available for further processing: If possible one would like to assign some measure of sense information even to these items.",
1139043134,false,finalized,3,1/24/2017 05:38:51,Neut,1.0,P05-3014,"The system developed by (Decadt et al., 2004) uses two cascaded memory-based classifiers, combined with the use of a genetic algorithm for joint parameter optimization and feature selection.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf,"(Decadt et al., 2004)","A separate ""word expert"" is learned for each ambiguous word, using a concatenated corpus of English sense","In recent SENSEVAL-3 evaluations, the most successful approaches for all words word sense disambiguation relied on information drawn from annotated corpora.",
1139043135,false,finalized,3,1/24/2017 05:40:33,Pos,0.6781,P05-3014,"Another top ranked system is the one developed by (Yuret, 2004), which combines two Naive Bayes statistical models, one based on surrounding collocations and another one based on a bag of words around the target word.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf,"(Yuret, 2004)","The statistical models are built based on SemCor and WordNet, for an overall disambiguation accuracy of 64.1%.",The performance of this system on the SENSEVAL-3 English all words data set was evaluated at 65.2%.,
1139043136,false,finalized,3,1/24/2017 05:53:28,Pos,0.6617,P05-3014,"A different version of our own SENSELEARNER system (Mihalcea and Faruque, 2004), using three of the semantic models described in this paper, combined with semantic generalizations based on syntactic dependencies, achieved a performance of 64.6%.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf,"(Mihalcea and Faruque, 2004)",3 SenseLearner,"The statistical models are built based on SemCor and WordNet, for an overall disambiguation accuracy of 64.1%.",
1139043137,false,finalized,3,1/24/2017 05:12:16,Neut,0.6673,P05-3014,"We base our experiments on SemCor (Miller et al., 1993), a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf,"(Miller et al., 1993)",The input to the disambiguation algorithm consists of raw text.,"SENSELEARNER is attempting to learn general semantic models for various word categories, starting with a relatively small sense-annotated corpus.",
1139043138,false,finalized,3,1/24/2017 05:36:42,Neut,1.0,P05-3014,"The algorithm starts with a preprocessing stage, where the text is tokenized and annotated with part-ofspeech tags; collocations are identified using a sliding window approach, where a collocation is defined as a sequence of words that forms a compound concept defined in WordNet (Miller, 1995); named entities are also identified at this stager.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf,"(Miller, 1995)","Next, a semantic model is learned for all predefined word categories, which are defined as groups of words that share some common syntactic or semantic properties.",The output is a text with word meaning annotations for all open-class words.,
1139043139,false,finalized,3,1/24/2017 05:56:37,Pos,0.6887,P05-3014,"An alternative solution to this second step was suggested in (Mihalcea and Faruque, 2004), using semantic generalizations learned from dependencies identified between nodes in a conceptual network.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf,"(Mihalcea and Faruque, 2004)","Their approach however, although slightly more accurate, conflicted with our goal of creating an efficient WSD system, and therefore we opted for the simpler backoff method that employs WordNet sense frequencies.",The words that are not covered by these models (typically about 10-15% of the words in the test corpus) are assigned with the most frequent sense in WordNet.,
1139043140,false,finalized,3,1/24/2017 05:31:03,Neut,0.6477,P05-3014,"Although more general than models that are built individually for each word in a test corpus (Decadt et al., 2004), the applicability of the semantic models built as part of SENSELEARNER is still limited to those words previously seen in the training corpus, and therefore their overall coverage is not 100%.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf,"(Decadt et al., 2004)","Starting with an annotated corpus consisting of all annotated files in SemCor, a separate training data set is built for each model.",Different semantic models can be defined and trained for the disambiguation of different word categories.,
1139043141,false,finalized,3,1/24/2017 05:50:54,Pos,0.6707,P05-3014,"For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002), (Mihalcea, 2002).",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf,"(Daelemans et al., 2001)","Following the learning stage, each vector in the test data set is labeled with a predicted word and sense.","Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model.",
1139043142,false,finalized,3,1/24/2017 05:43:54,Pos,0.6683,P05-3014,"For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002), (Mihalcea, 2002).",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf,"(Hoste et al., 2002)","Following the learning stage, each vector in the test data set is labeled with a predicted word and sense.","Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model.",
1139043143,false,finalized,3,1/24/2017 05:34:08,Pos,0.6715,P05-3014,"For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002), (Mihalcea, 2002).",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf,"(Mihalcea, 2002)","Following the learning stage, each vector in the test data set is labeled with a predicted word and sense.","Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model.",
1139043144,false,finalized,3,1/24/2017 05:06:06,Pos,0.6608,P05-3014,"The best results reported on these data sets are 69.0% on SENSEVAL-2 data (Mihalcea and Moldovan, 2002),",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf,"(Mihalcea and Moldovan, 2002)",55,"A baseline, computed using the most frequent sense in WordNet, is also indicated.",
1139043145,false,finalized,3,1/24/2017 05:55:36,Neut,1.0,P05-3014,"and 65.2% on SENSEVAL-3 data (Decadt et al., 2004).",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf,"(Decadt et al., 2004)","Note however that both these systems rely on significantly larger training data sets, and thus the results are not directly comparable.",binations of contextual (model*1/2) and collocational (model*Coll) models are also included.,
1139043146,false,finalized,3,1/24/2017 05:51:29,Neut,1.0,W04-0208,"In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993).",http://www.aclweb.org/anthology/W/W04/W04-0208.pdf,"(Kamp and Reyle, 1993)","Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic.","In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses.",
1139043147,false,finalized,3,1/24/2017 05:53:00,Neut,0.6831,W04-0208,Spejewski (1994) developed a tree-based model of the temporal structure of a sequence of sentences.,http://www.aclweb.org/anthology/W/W04/W04-0208.pdf,(1994),"Her approach is based on relations of temporal coordination and subordination, and is thus a major motivation for our own approach.","In this respect, they differ from TDMs, which do not commit to specific rhetorical relations.",
1139043148,false,finalized,3,1/24/2017 05:56:37,Neut,0.6545,P08-1100,"Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"(Pereira and Shabes, 1992)","Since then, there has been a large body of work addressing the flaws of the EM-based approach.",Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima.,
1139043149,false,finalized,3,1/24/2017 05:28:25,Neut,0.6788,P08-1100,"Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"(Carroll and Charniak, 1992)","Since then, there has been a large body of work addressing the flaws of the EM-based approach.",Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima.,
1139043150,false,finalized,3,1/24/2017 05:51:26,Neut,1.0,P08-1100,"Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"(Clark, 2001",Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,"Since then, there has been a large body of work addressing the flaws of the EM-based approach.",
1139043151,false,finalized,3,1/24/2017 05:36:11,Neut,1.0,P08-1100,"Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"Klein and Manning, 2004)",Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,"Since then, there has been a large body of work addressing the flaws of the EM-based approach.",
1139043152,false,finalized,3,1/24/2017 05:56:37,Pos,0.6486,P08-1100,Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,(2005),"Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).","Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).",
1139043153,false,finalized,3,1/24/2017 05:53:00,Neut,0.6831,P08-1100,Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,(2006),"Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).","Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).",
1139043154,false,finalized,3,1/24/2017 05:55:36,Neut,0.6831,P08-1100,"Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"(Goldwater and Griffiths, 2007","Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches.",Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,
1139043155,false,finalized,3,1/24/2017 05:27:35,Pos,0.6601,P08-1100,"Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"Johnson, 2007","Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches.",Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,
1139043156,false,finalized,3,1/24/2017 04:50:37,Pos,0.6831,P08-1100,"Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"Kurihara and Sato, 2006)","Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches.",Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,
1139043157,false,finalized,3,1/24/2017 05:37:38,Neut,0.6568,P08-1100,"We present a metamodel of the changes that EM makes and show how this tool can shed some light on the undesired biases of the HMM, the PCFG, and the dependency model with valence (Klein and Manning, 2004).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"(Klein and Manning, 2004)",Identifiability error can be incurred when two distinct parameter settings yield the same probability distribution over sentences.,"Our key idea for understanding this mis-match is to ""cheat"" and initialize EM with the true relationship and then study the ways in which EM repurposes our desired syntactic structures to increase likelihood.",
1139043158,false,finalized,3,1/24/2017 05:58:45,Neut,0.6683,P08-1100,"In the dependency model with valence (DMV) (Klein and Manning, 2004), the input x = (x1, ... , xm) is a sequence of POS tags and the output y specifies the directed links of a projective dependency tree.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"(Klein and Manning, 2004)","The generative model is as follows: for each head xi, we generate an independent sequence of arguments to the left and to the right from a direction-dependent distribution over tags.","We represent y as a multiset of binary rewrites of the form (y -* y1 y2), where y is a nonterminal and y1, y2 can be either nonterminals or terminals.",
1139043159,false,finalized,3,1/24/2017 05:27:35,Neut,1.0,P08-1100,See Klein and Manning (2004) for a formal description.,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,(2004),"In all our experiments, we used the Wall Street Journal (WSJ) portion of the Penn Treebank.","At each point, we stop with a probability parametrized by the direction and whether any arguments have already been generated in that direction.",
1139043160,false,finalized,3,1/24/2017 05:20:11,Neut,0.656,P08-1100,"We trained 45-state HMMs on all 49208 sentences, 11-state PCFGs on WSJ-10 (7424 sentences) and DMVs on WSJ-20 (25523 sentences) (Klein and Manning, 2004).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"(Klein and Manning, 2004)",We ran EM for 100 iterations with the parameters initialized uniformly (always plus a small amount of random noise).,We binarized the PCFG trees and created gold dependency trees according to the Collins head rules.,
1139043161,false,finalized,3,1/24/2017 05:40:33,Neut,0.3488,P08-1100,"We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmax ]E log p(x, y), which acts as a surrogate for p. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"(Merialdo, 1994",We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models.,4 Approximation error,
1139043162,false,finalized,3,1/24/2017 05:51:29,Neut,1.0,P08-1100,"We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmax ]E log p(x, y), which acts as a surrogate for p. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"Smith and Eisner, 2005",We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models.,4 Approximation error,
1139043163,false,finalized,3,1/24/2017 05:32:14,Neut,1.0,P08-1100,"We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmax ]E log p(x, y), which acts as a surrogate for p. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"Haghighi and Klein, 2006)",We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models.,4 Approximation error,
1139043164,false,finalized,3,1/24/2017 05:00:10,Pos,0.6608,P08-1100,Grenager et al (2005) augments an HMM to allow emission from a generic stopword distribution at any position with probability q.,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,(2005),"Their model would definitely not be identifiable if q were a free parameter, since we can set q to 0 and just mix in the stopword distribution with each of the other emission distributions to obtain a different parameter setting yielding the same overall distribution.","While seemingly benign, this symmetry actually presents a serious challenge in measuring discrepancy (Section 5.1).",
1139043165,false,finalized,3,1/24/2017 04:50:37,Neut,1.0,P08-1100,"D(B  ||B0) can then be computed by finding a maximum weighted bipartite matching on M using the O(K3) Hungarian algorithm (Kuhn, 1955).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"(Kuhn, 1955)","For models such as the HMM and PCFG, computing D is NP-hard, since the summation in d (1) contains both first-order terms which depend on one label (e.g., emission parameters) and higher-order terms which depend on more than one label (e.g., transitions or rewrites).","We can form a K xK matrix M, where each entry MZj is the distance between the parameters involving label i of B and label j of B0.",
1139043166,false,finalized,3,1/24/2017 05:57:49,Weak,0.6486,P08-1100,Carroll and Charniak (1992) report that EM fared poorly with local optima.,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,(1992),"We do not claim that there are no local optima, but only that the likelihood surface that EM is optimizing can become smoother with more examples.","Figures 4(f,g) show how both likelihood and accuracy, which both start quite low, improve substantially over time for the HMM on artificial data.",
1139043167,false,finalized,3,1/24/2017 06:19:11,Neut,0.6531,P08-1100,Srebro et al (2006) made a similar observation in the context of learning Gaussian mixtures.,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,(2006),"They characterized three regimes: one where EM was successful in recovering the true clusters (given lots of data), another where EM failed but the global optimum was successful, and the last where both failed (without much data).","With more examples, there is less noise in the aggregate statistics, so it might be easier for EM to pick out the salient patterns.",
1139043168,false,finalized,3,1/24/2017 04:58:54,CoCo,0.6803,P08-1100,"Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"(Ron et al., 1998","But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.",There is also a rich body of theoretical work on learning latent-variable models.,
1139043169,false,finalized,3,1/24/2017 05:32:52,Neut,1.0,P08-1100,"Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"Clark and Thollard, 2005","But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.",There is also a rich body of theoretical work on learning latent-variable models.,
1139043170,false,finalized,3,1/24/2017 05:38:14,Neut,1.0,P08-1100,"Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"Adriaans, 1999)","But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.",There is also a rich body of theoretical work on learning latent-variable models.,
1139043171,false,finalized,3,1/24/2017 05:36:11,Neut,0.6887,P08-1100,"Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"(Dasgupta, 1999","But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.",There is also a rich body of theoretical work on learning latent-variable models.,
1139043172,false,finalized,3,1/24/2017 05:57:49,Neut,1.0,P08-1100,"Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,"Feldman et al., 2005)","But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.",There is also a rich body of theoretical work on learning latent-variable models.,
1139043173,false,finalized,3,1/24/2017 05:37:38,Neut,0.6545,P08-1100,"But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf,(2007),8 Conclusion,"Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).",
1139043174,false,finalized,3,1/24/2017 05:42:12,Neut,0.6831,W10-3506,"The volume of information available to users on the World Wide Web is growing at an exponential rate (Lyman and Varian, 2003).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Lyman and Varian, 2003)","Current keyword-matching information retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears (Metzler and Croft, 2006).",1 Introduction,
1139043175,false,finalized,3,1/24/2017 05:20:55,Weak,0.6608,W10-3506,"Current keyword-matching information retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears (Metzler and Croft, 2006).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Metzler and Croft, 2006)","In recent years, much research attention has therefore been given to semantic techniques of information retrieval.","The volume of information available to users on the World Wide Web is growing at an exponential rate (Lyman and Varian, 2003).",
1139043176,false,finalized,3,1/24/2017 05:05:37,Neut,1.0,W10-3506,"Such systems allow for sophisticated semantic search, however, require the use of a more difficult-to-understand querysyntax (Tran et al., 2008).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Tran et al., 2008)","Furthermore, these methods require specially encoded (and thus costly) ontologies to describe the particular domain knowledge in which the system operates, and the specific interrelations of concepts within that domain.","In recent years, much research attention has therefore been given to semantic techniques of information retrieval.",
1139043177,false,finalized,3,1/24/2017 05:32:14,Neut,0.6739,W10-3506,"It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) methods respectively.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Witten and Milne, 2008)","Our methods outperform WLM in computing inter-concept similarity, and match ESA for inter-document similarity.","New measures for computing similarity between individual concepts (inter-concept similarity, such as ""France"" and ""Great Britain""), as well as between documents (inter-document similarity) are proposed and tested.",
1139043178,false,finalized,3,1/24/2017 05:30:40,CoCo,0.6742,W10-3506,"It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) methods respectively.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Gabrilovich and Markovitch, 2007)","Our methods outperform WLM in computing inter-concept similarity, and match ESA for inter-document similarity.","New measures for computing similarity between individual concepts (inter-concept similarity, such as ""France"" and ""Great Britain""), as well as between documents (inter-document similarity) are proposed and tested.",
1139043179,false,finalized,3,1/24/2017 06:04:06,Neut,0.3412,W10-3506,"Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Collins and Loftus, 1975)","Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.",2 Related Work and Overview,
1139043180,false,finalized,3,1/24/2017 05:20:11,Neut,0.656,W10-3506,"Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Preece, 1982)","Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.",2 Related Work and Overview,
1139043181,false,finalized,3,1/24/2017 05:53:00,Neut,1.0,W10-3506,"Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Crestani, 1997)","Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.",2 Related Work and Overview,
1139043182,false,finalized,3,1/24/2017 06:00:44,Neut,0.6818,W10-3506,"Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Witten and Milne, 2008)","WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them.","Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).",
1139043183,false,finalized,3,1/24/2017 05:10:19,Pos,0.7112,W10-3506,"Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Gabrilovich and Markovitch, 2007)","WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them.","Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).",
1139043184,false,finalized,3,1/24/2017 05:58:52,CoCo,0.6836,W10-3506,"Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Budanitsky and Hirst, 2001)","WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them.","Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).",
1139043185,false,finalized,3,1/24/2017 04:51:57,Neut,0.6495,W10-3506,"The most similar work to ours is Yeh (2009) in which the authors derive a graph structure from the inter-article links in Wikipedia pages, and then perform random walks over the graph to compute relatedness.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,(2009),"In Wikipedia, users create links between articles which are seen to be related to some degree.",Text is categorised as vectors in this concept space and similarity is computed as the cosine similarity of their ESA vectors.,
1139043186,false,finalized,3,1/24/2017 05:57:49,Neut,1.0,W10-3506,"Since links relate one article to its neighbours, and by extension to their neighbours, we extract and process this hyperlink structure (using SA) as an Associative Network (AN) (Berger et al., 2004) of concepts and links relating them to one another.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Berger et al., 2004)","The SA algorithm can briefly be described as an iterative process of propagating real-valued energy from one or more source nodes, via weighted links over an associative network (each such a propagation is called a pulse).","In Wikipedia, users create links between articles which are seen to be related to some degree.",
1139043187,false,finalized,3,1/24/2017 05:25:21,Neut,0.6542,W10-3506,"Links into pages are used, since this leads to better results (Witten and Milne, 2008).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Witten and Milne, 2008)","The Wikipedia graph structure is represented in an adjacency list structure, i.e.",47,
1139043188,false,finalized,3,1/24/2017 05:39:36,Neut,1.0,W10-3506,"Each pulse in the Spreading Activation (SA) process consists of three stages: 1) pre-adjustment, 2) spreading, and 3) post-adjustment (Crestani, 1997).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Crestani, 1997)","During pre- and post-adjustment, some form of activation decay is optionally applied to the active nodes.",4 Adapting Spreading Activation for Wikipedia's Hyperlink Structure,
1139043189,false,finalized,3,1/24/2017 04:51:57,Weak,0.6922,W10-3506,"This pure model of SA has several significant problems, the most notable being that activation can saturate the entire network unless certain constraints are imposed, namely limiting how far activation can spread from the initially activated nodes (distance constraint), and limiting the effect of very highly-connected nodes (fanout constraint) (Crestani, 1997).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Crestani, 1997)",In the following three sections we discuss how these constraints were implemented in our model for SA.,(1) vjN(vi),
1139043190,false,finalized,3,1/24/2017 05:28:25,Neut,1.0,W10-3506,"Inverse Link-Frequency (ILF) is inspired by the term-frequency inverse document-frequency (tf-idf) heuristic (Salton and McGill, 1983) in which a term's weight is reduced as it is contained in more documents in the corpus.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Salton and McGill, 1983)","It is based on the idea that the more a term appears in documents across the corpus, the less it can discriminate any one of those documents.","For instance, we consider a path connecting two nodes via a general article such as USA (connected to 322,000 articles) not nearly as indicative of a semantic relationship, as a path connecting them via a very specific concept, such as Hair Pin (only connected to 20 articles).",
1139043191,false,finalized,3,1/24/2017 05:30:41,Error,0.6803,W10-3506,"We therefore divide by log |G |to normalise its range to [0,1].",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,[0],Threshold constraint,ILF reaches a maximum of log |G |when |N(vi) |= 1 (see Equation 2).,
1139043192,false,finalized,3,1/24/2017 05:51:26,Error,1.0,W10-3506,"We therefore divide by log |G |to normalise its range to [0,1].",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,[1],Threshold constraint,ILF reaches a maximum of log |G |when |N(vi) |= 1 (see Equation 2).,
1139043193,false,finalized,3,1/24/2017 05:12:16,Neut,1.0,W10-3506,"After spreading has terminated, relatedness is computed as the amount of overlap between the individual nodes' activation vectors, using either the cosine similarity (AA-cos), or an adapted version of the information theory based WLM (Witten and Milne, 2008) measure.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Witten and Milne, 2008)",Assume the same set of initial nodes vi and vj.,The second approach is called the Agglomerative Approach since we agglomerate all activations into one score resembling relatedness.,
1139043194,false,finalized,3,1/24/2017 10:20:20,Neut,0.6503,W10-3506,"For our adaptation of the Wikipedia Link-based Measure (WLM) approach to spreading activation, we define the WLM Agglomerative Approach (henceforth called AA-wlm2) as 2AA-wlm is our adaptation of WLM (Witten and Milne, 2008) for SA, not to be confused with their method, which we simply call WLM.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Witten and Milne, 2008)","simAA,cos(Ai, Aj) A Ai  Aj",(4) ||Ai |Aj||,
1139043195,false,finalized,3,1/24/2017 11:05:48,Neut,0.6599,W10-3506,"In order to compare our method with results reported by Gabrilovich and Markovitch (2007) and Witten and Milne (2008), we followed the same approach by randomly selecting Algorithm 1 Pseudo code to spread activation depth-first from node vi up to level Lp,max, using global decay d, and threshold T, given an adjacency list graph structure G and a weighting scheme W such that 0 < wig E W < 1.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,(2007),"Require: G, Lp,-ax, d, T function SPREAD UNIDIR(vi, A, P) if (vi, ai) E/ A or ai < T then > Threshold return end if Add vi to P > To avoid cycles for vj E N(vi) do > Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ < Lp,-ax then aj =aj+ai*wij*d Replace (vj, aj) E A with (vj, aj ) SPREAD UNIDIR(vj, A, P) end if end for return end function",Experimental Method,
1139043196,false,finalized,3,1/24/2017 11:43:08,Neut,0.6531,W10-3506,"In order to compare our method with results reported by Gabrilovich and Markovitch (2007) and Witten and Milne (2008), we followed the same approach by randomly selecting Algorithm 1 Pseudo code to spread activation depth-first from node vi up to level Lp,max, using global decay d, and threshold T, given an adjacency list graph structure G and a weighting scheme W such that 0 < wig E W < 1.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,(2008),"Require: G, Lp,-ax, d, T function SPREAD UNIDIR(vi, A, P) if (vi, ai) E/ A or ai < T then > Threshold return end if Add vi to P > To avoid cycles for vj E N(vi) do > Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ < Lp,-ax then aj =aj+ai*wij*d Replace (vj, aj) E A with (vj, aj ) SPREAD UNIDIR(vj, A, P) end if end for return end function",Experimental Method,
1139043197,false,finalized,3,1/24/2017 09:16:33,Pos,0.6657,W10-3506,"50 word-pairs from the WordSimilarity-353 dataset (Gabrilovich, 2002) and correlating our method's scores with the human-assigned scores.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Gabrilovich, 2002)","To reduce the possibility of overestimating the performance of our technique on a sample set that happens to be favourable to our technique, we furthermore implemented a technique of repeated holdout (Witten and Frank, 2005): Given a sample test set of N pairs of words with human-assigned ratings of relatedness, randomly divide this set into k parts of roughly equal size3.","Require: G, Lp,-ax, d, T function SPREAD UNIDIR(vi, A, P) if (vi, ai) E/ A or ai < T then > Threshold return end if Add vi to P > To avoid cycles for vj E N(vi) do > Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ < Lp,-ax then aj =aj+ai*wij*d Replace (vj, aj) E A with (vj, aj ) SPREAD UNIDIR(vj, A, P) end if end for return end function",
1139043198,false,finalized,3,1/24/2017 08:59:10,Pos,0.671,W10-3506,"To reduce the possibility of overestimating the performance of our technique on a sample set that happens to be favourable to our technique, we furthermore implemented a technique of repeated holdout (Witten and Frank, 2005): Given a sample test set of N pairs of words with human-assigned ratings of relatedness, randomly divide this set into k parts of roughly equal size3.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Witten and Frank, 2005)",Hold out one part of the data and iteratively evaluate the performance of the algorithm on the remaining k1 parts until all k parts have been held out once.,"50 word-pairs from the WordSimilarity-353 dataset (Gabrilovich, 2002) and correlating our method's scores with the human-assigned scores.",
1139043199,false,finalized,3,1/24/2017 11:44:32,CoCo,0.67,W10-3506,"These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008) (p = 0.69) and ESA (Gabrilovich and Markovitch, 2007) (p = 0.75).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Witten and Milne, 2008)","Maximum path length Lp,max is related to how far one node can spread its activation in the network.","Secondly, the AA-cos spreading strategy significantly outperforms the AA-wlm strategy over this sample set (pmax,wlm = 0.60 vs pmax,cos = 0.70).",
1139043200,false,finalized,3,1/24/2017 10:32:57,CoCo,0.6715,W10-3506,"These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008) (p = 0.69) and ESA (Gabrilovich and Markovitch, 2007) (p = 0.75).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Gabrilovich and Markovitch, 2007)","Maximum path length Lp,max is related to how far one node can spread its activation in the network.","Secondly, the AA-cos spreading strategy significantly outperforms the AA-wlm strategy over this sample set (pmax,wlm = 0.60 vs pmax,cos = 0.70).",
1139043201,false,finalized,3,1/24/2017 10:32:04,Neut,1.0,W10-3506,"This process is known as wikification (Csomai and Mihalcea, 2008), and we used an implementation of Milne and Witten (2008).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Csomai and Mihalcea, 2008)","This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document.","To compute document similarity, we first extract key representative Wikipedia concepts from a document to produce document concept vectors4.",
1139043202,false,finalized,3,1/24/2017 11:06:35,Neut,0.6581,W10-3506,"This process is known as wikification (Csomai and Mihalcea, 2008), and we used an implementation of Milne and Witten (2008).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,(2008),"This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document.","To compute document similarity, we first extract key representative Wikipedia concepts from a document to produce document concept vectors4.",
1139043203,false,finalized,3,1/24/2017 10:53:55,Neut,0.6617,W10-3506,"We next present two algorithms, MAXSIM and WIKISPREAD, for computing document similarity, and test these over the Lee (2005) document similarity dataset, a set of 50 documents between 51 and 126 words each, with the averaged gold standard similarity ratings produced by 83 test subjects (see (Lee et al., 2005)).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,(2005),The first metric we propose is called MAXSIM (see Algorithm 2) and is based on the idea of measuring document similarity by pairing up each Wikipedia concept in one document's concept vector with its most similar concept in the other document.,"This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document.",
1139043204,false,finalized,3,1/24/2017 12:41:40,Neut,0.6867,W10-3506,"We next present two algorithms, MAXSIM and WIKISPREAD, for computing document similarity, and test these over the Lee (2005) document similarity dataset, a set of 50 documents between 51 and 126 words each, with the averaged gold standard similarity ratings produced by 83 test subjects (see (Lee et al., 2005)).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Lee et al., 2005)",The first metric we propose is called MAXSIM (see Algorithm 2) and is based on the idea of measuring document similarity by pairing up each Wikipedia concept in one document's concept vector with its most similar concept in the other document.,"This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document.",
1139043205,false,finalized,3,1/24/2017 11:09:22,Neut,0.6599,W10-3506,ESA score from Gabrilovich and Markovitch (2007).,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,(2007),Pearson p Cosine VSM (with tf-idf) only 0.56 MaxSim method 0.68 WikiSpread method 0.62 ESA 0.72 Combined (Cosine + MaxSim) 0.72,Table 4: Summary of final document similarity correlations over the Lee & Pincombe document similarity dataset.,
1139043206,false,finalized,3,1/24/2017 11:42:01,Neut,1.0,W10-3506,"Therefore, the final document similarity metric we evaluate (COMBINED) is a linear combination of the best-performing Wikipedia-based methods described above, and the well-known Vector Space Model (VSM) with cosine similarity and tf-idf (Salton and McGill, 1983).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Salton and McGill, 1983)",Results,We therefore hypothesise that combining the two approaches will lead to more robust document similarity performance.,
1139043207,false,finalized,3,1/24/2017 12:41:40,Neut,1.0,W10-3506,"The results obtained on the Lee (2005) document similarity dataset using the three document similarity metrics (MAXSIM, WIKISPREAD, and COMBINED) are summarised in Table 4.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,(2005),"Of the two Wikipedia-only methods, the MaxSim method achieves the best correlation score of p = 0.68.",Results,
1139043208,false,finalized,3,1/24/2017 11:00:54,Neut,0.6599,W10-3506,Figure 1: Parameter sweep over A showing contributions from cosine (A) and Wikipedia-based MAXSIM method (1  A) to the final performance over the Lee (2005) dataset.,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,(2005),9 Conclusion,This suggests that selective knowledge-based augmentation of simple VSM methods can lead to more accurate document similarity performance.,
1139043209,false,finalized,3,1/24/2017 11:43:45,Neut,1.0,W10-3506,"The final result of p = 0.72 is equal to that reported for ESA (Gabrilovich and Markovitch, 2007), while requiring less than 10% of the Wikipedia database required for ESA.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf,"(Gabrilovich and Markovitch, 2007)",Table 4 summarises the document-similarity results.,"Finally, we show that using our best Wikipedia-based method to augment the cosine VSM method using tf-idf, leads to the best results.",
1139043210,false,finalized,3,1/24/2017 11:46:02,Neut,1.0,P05-1058,"Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Brown et al., 1993)","In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.",1 Introduction,
1139043211,false,finalized,3,1/24/2017 10:32:57,Neut,0.6788,P05-1058,"In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu, 1997","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.","Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).",
1139043212,false,finalized,3,1/24/2017 10:18:27,Neut,0.689,P05-1058,"In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"Och and Ney, 2003","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.","Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).",
1139043213,false,finalized,3,1/24/2017 11:48:42,Neut,1.0,P05-1058,"In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"Cherry and Lin, 2003)","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.","Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).",
1139043214,false,finalized,3,1/24/2017 11:32:14,Neut,1.0,P05-1058,"In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Smadja et al., 1996","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.","Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).",
1139043215,false,finalized,3,1/24/2017 11:56:16,Neut,1.0,P05-1058,"In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"Ahrenberg et al., 1998","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.","Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).",
1139043216,false,finalized,3,1/24/2017 11:51:57,Neut,1.0,P05-1058,"In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"Tufis and Barbu, 2002)","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.","Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).",
1139043217,false,finalized,3,1/24/2017 11:45:00,Neut,1.0,P05-1058,"When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Ker and Chang, 1997)","However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available.","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.",
1139043218,false,finalized,3,1/24/2017 09:17:42,Pos,0.6486,P05-1058,"However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu and Wang, 2004)","In this paper, we address the problem of word alignment in a specific domain, in which only a small-scale corpus is available.","When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997).",
1139043219,false,finalized,3,1/24/2017 09:08:36,Neut,1.0,P05-1058,"Although the adaptation technology is widely used for other tasks such as language modeling (Iyer et al., 1997), only a few studies, to the best of our knowledge, directly address word alignment adaptation.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Iyer et al., 1997)",Wu and Wang (2004) adapted the alignment results obtained with the out-of-domain corpus to the results obtained with the in-domain corpus.,We implement this by using alignment model adaptation.,
1139043220,false,finalized,3,1/24/2017 09:48:56,Neut,1.0,P05-1058,Wu and Wang (2004) adapted the alignment results obtained with the out-of-domain corpus to the results obtained with the in-domain corpus.,http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,(2004),"This method first trained two models and two translation dictionaries with the in-domain corpus and the out-of-domain corpus, respectively.","Although the adaptation technology is widely used for other tasks such as language modeling (Iyer et al., 1997), only a few studies, to the best of our knowledge, directly address word alignment adaptation.",
1139043221,false,finalized,3,1/24/2017 09:07:26,Neut,1.0,P05-1058,"According to the IBM models (Brown et al., 1993), the statistical word alignment model can be generally represented as in Equation (1).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Brown et al., 1993)","f e , ) = | a '  |e)  p(a' ,  |) f e (1) , a ( f p ( a p",2 Statistical Word Alignment,
1139043222,false,finalized,3,1/24/2017 11:56:16,Neut,0.6656,P05-1058,"In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999), which is shown in Equation (2).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Al-Onaizan et al., 1999)","This simplified version does not take word classes into account as described in (Brown et al., 1993).","f e , ) = | a '  |e)  p(a' ,  |) f e (1) , a ( f p ( a p",
1139043223,false,finalized,3,1/24/2017 09:46:51,Pos,0.3522,P05-1058,"This simplified version does not take word classes into account as described in (Brown et al., 1993).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Brown et al., 1993)","p(a,  |e) =  Pr( ,  |) f ( , )   e   l m , , 1 j = j : 0 a j =  n(i  |ei ) i i =1 i=1 m m e) m 0 l l |  ,  e)Pr(=  p(a, | f ( , )     m 0 = 0   p   20 0 p1  ! )","In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999), which is shown in Equation (2).",
1139043224,false,finalized,3,1/24/2017 09:12:25,Neut,0.6939,P05-1058,"d j a ( | j t (fj   |e j a (3) 1 A cept is defined as the set of target words connected to a source word (Brown et al., 1993). )",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Brown et al., 1993)",") ( |   t f e j a j i 1, 0 aj= = l m  )  n(i  |e = 1 = i j 1 m j j= 1, 0 a j = m 0 ( ([ ( )] ( j h a d j c =   )) j 1 aj m ([ ( )] ( ( )))) j h a d j p j =   j > 1   + 0 =   p   m 0  2  0 0 p1 } i = max{i ': i' >","p(a,  |e) =  Pr( ,  |) f ( , )   e   l m , , 1 j = j : 0 a j =  n(i  |ei ) i i =1 i=1 m m e) m 0 l l |  ,  e)Pr(=  p(a, | f ( , )     m 0 = 0   p   20 0 p1  ! )",
1139043225,false,finalized,3,1/24/2017 10:28:48,Neut,1.0,P05-1058,"In order to deal with this problem, we perform word alignment in two directions (source to target, and target to source) as described in (Och and Ney, 2000).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Och and Ney, 2000)",The GIZA++ toolkit2 is used to perform statistical word alignment.,"Thus, some multi-word units in the domain-specific corpus cannot be correctly aligned.",
1139043226,false,finalized,3,1/24/2017 10:51:02,Error,1.0,P05-1058,"more frequently in a specific domain than in the general domain, it can usually be considered as a domain-specific word et al., 2001).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"more frequently in a specific domain than in the general domain, it can usually be considered as a domain-specific word et al., 2001)",For,Equation (8) indicates that if a word occurs,
1139043227,false,finalized,3,1/24/2017 10:41:36,Pos,0.6583,P05-1058,"The alignment links in this intersection set are extended by iteratively adding word alignment links into it as described in (Och and Ney, 2000).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Och and Ney, 2000)","Based on the extended alignment links, we build a translation dictionary.","By taking the intersection of the two word alignment results, we build a new alignment set.",
1139043228,false,finalized,3,1/24/2017 09:22:53,Pos,0.6583,P05-1058,"In order to filter the noise caused by the error alignment links, we only retain those translation pairs whose log-likelihood ratio scores (Dunning, 1993) are above a threshold.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Dunning, 1993)",Based on the alignment results on the d one-to-many alignments.,"Based on the extended alignment links, we build a translation dictionary.",
1139043229,false,finalized,3,1/24/2017 11:43:11,Neut,1.0,P05-1058,"The first method is descri ed in (Wu and Wang, 2004).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu and Wang, 2004)","We call it ""Result Adaptation (ResAdapt)"".",6 Evaluation,
1139043230,false,finalized,3,1/24/2017 12:26:18,Neut,1.0,P05-1058,"For each sentence pair, there are two different word alignment results, from which the final alignment links are selected according to their translation probabilities in the dictionary D. The selection order is similar to that in the competitive linking algorithm (Melamed, 1997).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Melamed, 1997)",The difference is that we allow many-to-one an We compare our method with four other methods.,The detailed algorithm is shown in Figure 2.,
1139043231,false,finalized,3,1/24/2017 09:48:56,Neut,0.6503,P05-1058,"We use the same evaluation metrics as described in (Wu and Wang, 2004).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu and Wang, 2004)","If we use to represent SG the set of alignment links identified by the proposed methods and to denote the reference SC alignment set, the methods to calculate the precision, recall, f-measure, and alignment error rate (AER) are shown in Equation (13), (14), (15), and (16).",6.2 Evaluation Metrics,
1139043232,false,finalized,3,1/24/2017 09:39:38,Neut,0.6815,P05-1058,"In order to further compare our method with the method described in (Wu and Wang, 2004).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu and Wang, 2004)","We do another experiment using almost the same-scale in-domain training corpus as described in (Wu and Wang, 2004).",Results,
1139043233,false,finalized,3,1/24/2017 09:02:18,Neut,0.6733,P05-1058,"We do another experiment using almost the same-scale in-domain training corpus as described in (Wu and Wang, 2004).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu and Wang, 2004)","From the in-domain training corpus, we randomly select about 500 sentence pairs to build the smaller training set.","In order to further compare our method with the method described in (Wu and Wang, 2004).",
1139043234,false,finalized,3,1/24/2017 10:32:57,Neut,0.6496,P05-1058,"while the method ""ResAdapt"" described in (Wu and Wang, 2004) only achieves an error rate reduction of 8.59%.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu and Wang, 2004)","Compared with the method ""ResAdapt"", our method achieves an error rate reduction of 10.15%.",472,
1139043235,false,finalized,3,1/24/2017 11:41:49,CoCo,1.0,P05-1058,"This result is different from that in (Wu and Wang, 2004), where their method achieved an error rate reduction of 21.96% as compared with the method ""Gen+Spec"".",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu and Wang, 2004)","The main reason is that the in-domain training corpus and testing corpus in this paper are different from those in (Wu and Wang, 2004).","Compared with the method ""ResAdapt"", our method achieves an error rate reduction of 10.15%.",
1139043236,false,finalized,3,1/24/2017 11:48:42,Neut,0.6649,P05-1058,"The main reason is that the in-domain training corpus and testing corpus in this paper are different from those in (Wu and Wang, 2004).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu and Wang, 2004)","The training data and the testing data described in (Wu and Wang, 2004) are from a single manual.","This result is different from that in (Wu and Wang, 2004), where their method achieved an error rate reduction of 21.96% as compared with the method ""Gen+Spec"".",
1139043237,false,finalized,3,1/24/2017 11:48:38,Neut,1.0,P05-1058,"The training data and the testing data described in (Wu and Wang, 2004) are from a single manual.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu and Wang, 2004)",The data in our corpus are from several manuals describing how to use the diagnostic ultrasound systems.,"The main reason is that the in-domain training corpus and testing corpus in this paper are different from those in (Wu and Wang, 2004).",
1139043238,false,finalized,3,1/24/2017 11:29:19,Neut,0.6649,P05-1058,"In addition to the above evaluations, we also evaluate our model adaptation method using the ""refined"" combination in Och and Ney (2000) instead of the translation dictionary.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,(2000),"Using the ""refined"" method to select the alignments produced by our model adaptation method (AER: 0.2371) still yields better result than directly combining out-of-domain and in-domain corpora as training data of the ""refined"" method (AER: 0.2290).",The data in our corpus are from several manuals describing how to use the diagnostic ultrasound systems.,
1139043239,false,finalized,3,1/24/2017 11:12:23,CoCo,0.671,P05-1058,"It also achieves a relative error rate reduction of 6.56% as compared with the previous work in (Wu and Wang, 2004).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu and Wang, 2004)","In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004), our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004).",Our method achieves a relative error rate reduction of 17.43% as compared with the method directly combining the out-of-domain corpus and the in-domain corpus as training data.,
1139043240,false,finalized,3,1/24/2017 09:38:34,CoCo,1.0,P05-1058,"In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004), our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu and Wang, 2004)",We also use in-domain corpora and out-of-domain corpora of different sizes to perform adaptation experiments.,"It also achieves a relative error rate reduction of 6.56% as compared with the previous work in (Wu and Wang, 2004).",
1139043241,false,finalized,3,1/24/2017 09:33:50,CoCo,0.6611,P05-1058,"In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004), our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf,"(Wu and Wang, 2004)",We also use in-domain corpora and out-of-domain corpora of different sizes to perform adaptation experiments.,"It also achieves a relative error rate reduction of 6.56% as compared with the previous work in (Wu and Wang, 2004).",
1139043242,false,finalized,3,1/24/2017 09:15:03,Neut,1.0,W12-1635,"Zadrozny (1995) proposes Q-Complexity, which roughly corresponds to vocabulary size, and is essentially the number of questions that can be asked about a database.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf,(1995),"Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space.",Little work has been reported on measures of the relationship between dialogue complexity and the semantic structure of a DS application's database.,
1139043243,false,finalized,3,1/24/2017 09:18:33,CoCo,0.3625,W12-1635,"Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf,(2000),Gorin et al (2000) distinguish between semantic and linguistic complexity of calls to a spoken DS.,"Zadrozny (1995) proposes Q-Complexity, which roughly corresponds to vocabulary size, and is essentially the number of questions that can be asked about a database.",
1139043244,false,finalized,3,1/24/2017 08:59:10,Neut,1.0,W12-1635,Gorin et al (2000) distinguish between semantic and linguistic complexity of calls to a spoken DS.,http://www.aclweb.org/anthology/W/W12/W12-1635.pdf,(2000),"Semantic complexity is measured by inheritance relations between call types, the number of type labels per call, and how often calls are routed to human agents.","Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space.",
1139043245,false,finalized,3,1/24/2017 12:00:42,Neut,1.0,W12-1635,"Popescu et al (2003) identify a class of ""semantically tractable"" natural language questions that can be mapped to an SQL query to return the question's unique correct answer.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf,(2003),Ambiguous questions with multiple correct answers are not considered semantically tractable.,"Linguistic complexity is measured by utterance length, vocabulary size and perplexity.",
1139043246,false,finalized,3,1/24/2017 09:12:10,Neut,1.0,W12-1635,"Polifroni and Walker (2008) address how to present informative options to users who are exploring a database, for example, to choose a restaurant.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf,(2008),"When a query returns many options, their system summarizes the return using attribute value pairs shared by many of the members.",Ambiguous questions with multiple correct answers are not considered semantically tractable.,
1139043247,false,finalized,3,1/24/2017 10:16:28,Error,1.0,W12-1635,"To produce specificity values in the range [0, 1], w(j) should decrease as j increases, but not penalize any query that returns a single instance, that is, w(1) = 1.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf,[0],"The faster w decreases, the more it penalizes an ambiguous attribute.","A larger query return size indicates a more ambiguous attribute, one less able to distinguish among instances in I.",
1139043248,false,finalized,3,1/24/2017 11:39:52,Error,0.6571,W12-1635,"To produce specificity values in the range [0, 1], w(j) should decrease as j increases, but not penalize any query that returns a single instance, that is, w(1) = 1.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf,[1],"The faster w decreases, the more it penalizes an ambiguous attribute.","A larger query return size indicates a more ambiguous attribute, one less able to distinguish among instances in I.",
1139043249,false,finalized,3,1/24/2017 11:40:49,Neut,0.6905,W12-1635,"To avoid addressing information presentation issues such as those explored in (Polifroni and Walker, 2008), CheckItOut followed a simple strategy of offering each next candidate book in a query return, and user studies with CheckItOut restricted query return size to a maximum of three books.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf,"(Polifroni and Walker, 2008)","For the simulations, we expect an inverse relationship between specificity and dialogue length.","When the system cannot uniquely identify a requested book, it begins a disambiguation subdialogue, an example of which is shown in Figure 1.",
1139043250,false,finalized,3,1/24/2017 11:43:45,Neut,0.6677,W07-1402,"The system is based to a large extent on Burchardt and Frank's system (2006) used in the second RTE challenge (Bar-Haim et al., 2006); it relies on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,(2006),"As the system has been described earlier, we concentrate on a more systematic discussion of the system behaviour, aiming at spotting promising anchors for future extensions and improvements.",This paper reports on the system we used in the third PASCAL challenge on Recognizing Textual Entailment.,
1139043251,false,finalized,3,1/24/2017 11:25:39,Neut,0.6649,W07-1402,"The system is based to a large extent on Burchardt and Frank's system (2006) used in the second RTE challenge (Bar-Haim et al., 2006); it relies on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Bar-Haim et al., 2006)","As the system has been described earlier, we concentrate on a more systematic discussion of the system behaviour, aiming at spotting promising anchors for future extensions and improvements.",This paper reports on the system we used in the third PASCAL challenge on Recognizing Textual Entailment.,
1139043252,false,finalized,3,1/24/2017 11:51:57,Neut,0.6568,W07-1402,"It has been observed for related systems that a combination of separately trained features in the machine learning component can lead to an overall improvement in system performance, in particular if features from a more ""informed"" component and shallow ones are combined (Hickl et al., 2006; Bos and Markert, 2006).",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Hickl et al., 2006",We provide a detailed analysis of our system's behaviour on different training and test sets.,"As the system has been described earlier, we concentrate on a more systematic discussion of the system behaviour, aiming at spotting promising anchors for future extensions and improvements.",
1139043253,false,finalized,3,1/24/2017 11:51:29,Neut,0.6909,W07-1402,"It has been observed for related systems that a combination of separately trained features in the machine learning component can lead to an overall improvement in system performance, in particular if features from a more ""informed"" component and shallow ones are combined (Hickl et al., 2006; Bos and Markert, 2006).",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"Bos and Markert, 2006)",We provide a detailed analysis of our system's behaviour on different training and test sets.,"As the system has been described earlier, we concentrate on a more systematic discussion of the system behaviour, aiming at spotting promising anchors for future extensions and improvements.",
1139043254,false,finalized,3,1/24/2017 09:22:48,Neut,0.6948,W07-1402,"More details can be found in (Burchardt and Frank, 2006).",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Burchardt and Frank, 2006)",2.1 Architecture,"In this Section, we review the basic architecture of the SALSA RTE system, and report on some improvements and extensions.",
1139043255,false,finalized,3,1/24/2017 11:54:11,Neut,1.0,W07-1402,"The SALSA RTE system is based on three main components: (i) a linguistic analysis of text and hypothesis based primarily on LFG and Frame Semantics (Baker et al., 1998), (ii) the computation of a match graph that encodes the ""semantic overlap"" between text and hypothesis, and (iii) a statistical entailment decision.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Baker et al., 1998)","Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 10-15, Prague, June 2007.",2.1 Architecture,
1139043256,false,finalized,3,1/24/2017 08:43:33,Neut,1.0,W07-1402,"The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al., 2005).",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Riezler et al., 2002)",Frame semantic analysis is especially interesting for the task of recognising textual entailment as it offers a robust yet relatively precise measure for semantic overlap.,Linguistic analysis.,
1139043257,false,finalized,3,1/24/2017 11:15:03,Neut,1.0,W07-1402,"The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al., 2005).",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Erk and Pado, 2006)",Frame semantic analysis is especially interesting for the task of recognising textual entailment as it offers a robust yet relatively precise measure for semantic overlap.,Linguistic analysis.,
1139043258,false,finalized,3,1/24/2017 09:12:25,Neut,0.6939,W07-1402,"The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al., 2005).",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Burchardt et al., 2005)",Frame semantic analysis is especially interesting for the task of recognising textual entailment as it offers a robust yet relatively precise measure for semantic overlap.,Linguistic analysis.,
1139043259,false,finalized,3,1/24/2017 11:08:21,Neut,1.0,W07-1402,"Semantic nodes are further projected into an ontological analysis layer containing WordNet (Fellbaum, 1997) senses and SUMO (Niles and Pease, 2001) classes.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Fellbaum, 1997)","Semantic phenomena not treated by FrameNet like anaphora, negation or modality are (approximately) encoded with special operators.","The predicate discover is associated with the frame ACxIEVING_FIRST, the semantic role COGNIZER points to the pseudo-predicate Leloir and NEW_IDEA points to the PROCESS frame evoked by metabolism.",
1139043260,false,finalized,3,1/24/2017 10:17:49,Neut,0.6503,W07-1402,"Semantic nodes are further projected into an ontological analysis layer containing WordNet (Fellbaum, 1997) senses and SUMO (Niles and Pease, 2001) classes.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Niles and Pease, 2001)","Semantic phenomena not treated by FrameNet like anaphora, negation or modality are (approximately) encoded with special operators.","The predicate discover is associated with the frame ACxIEVING_FIRST, the semantic role COGNIZER points to the pseudo-predicate Leloir and NEW_IDEA points to the PROCESS frame evoked by metabolism.",
1139043261,false,finalized,3,1/24/2017 09:30:33,Pos,1.0,W07-1402,"To cope with longer texts, we integrated the sentence splitter of the JTok tokeniser (Schafer, 2005) into the system.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Schafer, 2005)",WordNet interface.,Sentence splitter.,
1139043262,false,finalized,3,1/24/2017 12:20:27,Neut,1.0,W07-1402,"Both text and hypothesis are tagged and lemmatised using Tree Tagger (Schmid, 1994), taking only nouns, non-auxiliary verbs, adjectives and adverbs into account.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Schmid, 1994)","Training a decision tree on the relative word-overlap as single feature yields a system which performs comparable to earlier word-overlap based systems, achieving an accuracy of 60.6 % if trained and tested on the RTE 2 development and test set, respectively (using Weka's J48 classifier), or 57.5 % if we use Weka's LogitBoost classifier.",The shallow system measures the relative number of words in the hypothesis that also occur in the text.,
1139043263,false,finalized,3,1/24/2017 11:45:36,Neut,1.0,W07-1402,"Finally, we improved the machine learning back-end which feeds our extracted features into the Weka toolkit (Witten and Frank, 2005).",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Witten and Frank, 2005)","This allows to train features in arbitrary combinations, with different machine learners.'",Weka Interface.,
1139043264,false,finalized,3,1/24/2017 11:46:02,Neut,1.0,W07-1402,"The improvement observed by (Hickl et al., 2006) was achieved by going to 10.000 items.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Hickl et al., 2006)","As can be seen from Table 2, in most of the cases, IV performs best, e.g.","3In terms of machine learning, extending a training set by factor 2 (from 800 to 1.600 items) does not make a qualitative difference.",
1139043265,false,finalized,3,1/24/2017 08:42:48,Neut,0.6815,W07-1402,"The observation that standard logical entailment and textual entailment deviate in certain respects is not surprising and has also been addressed in a discussion initiated by (Zaenen et al., 2005).",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Zaenen et al., 2005)","Still, there is no consensus regarding the precise mechanisms involved in the latter such as ""general principles of plausibility"" or pragmatic principles.",Although the hypothesis is logically entailed by the text (if we ignore the report context) - 'kill' implies 'possibly kill' - pragmatic principles seem to block entailment here.,
1139043266,false,finalized,3,1/24/2017 09:34:40,Pos,0.6496,W07-1402,"In another current effort, we work on an interface to upper-level ontologies (Reiter, 2007) in order to access more ""world-knowledge"" which is a desideratum in natural language processing in general, as in many approaches to textual entailment.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf,"(Reiter, 2007)",Acknowledgements,"As we cannot expect the necessary amount of training data to be available in the near future, we currently investigate the data more closely in order to arrive at a more controlled model of textual entailment.",
1139043267,false,finalized,3,1/24/2017 11:25:20,Neut,0.6581,W10-4231,"An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al (2009).",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf,(2009),"Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b).","The corpus is a collection of 2,000 introductory sections from Wikipedia articles about individual people in which all mentions of person entities have been annotated.",
1139043268,false,finalized,3,1/24/2017 11:33:16,Neut,1.0,W10-4231,"Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b).",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf,"(Greenbacker and McCoy, 2009a)","Although our system performed reasonably-well in predicting REG08Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task.","An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al (2009).",
1139043269,false,finalized,3,1/24/2017 11:43:08,Neut,1.0,W10-4231,"Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b).",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf,"(Greenbacker and McCoy, 2009b)","Although our system performed reasonably-well in predicting REG08Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task.","An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al (2009).",
1139043270,false,finalized,3,1/24/2017 09:33:50,Neut,1.0,W10-4231,"As suggested by the evaluators (Belz et al., 2009), this was due in large part to our reliance on the list of REs being in a particular order, which had changed for the NEG task.",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf,"(Belz et al., 2009)",2 Method,"Although our system performed reasonably-well in predicting REG08Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task.",
1139043271,false,finalized,3,1/24/2017 12:12:57,Neut,1.0,W10-4231,"In 2009, we trained a series of decision trees to predict REG08Type based on our psycholinguistically-inspired feature set (described in (Greenbacker and McCoy, 2009c)), and then simply chose the first option in the list of REs matching the predicted type.",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf,"(Greenbacker and McCoy, 2009c)","For 2010, we incorporated the case of each RE into our target attribute so that the decision tree classifier would predict both the type and case for the given reference.",The first improvement we made to our existing methods related to the manner by which we selected the specific RE to employ.,
1139043272,false,finalized,3,1/24/2017 11:32:37,Neut,0.6649,W10-4231,"Then, we applied a series of rules governing the length of initial and subsequent REs involving a person's name (following Nenkova and McKeown (2003)), as well as 'backoffs' if the predicted type or case were not available.",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf,(2003),Another improvement we made involved our method of determining whether the use of a pronoun would introduce ambiguity in a given context.,"For 2010, we incorporated the case of each RE into our target attribute so that the decision tree classifier would predict both the type and case for the given reference.",
1139043273,false,finalized,3,1/24/2017 09:07:26,CoCo,0.6741,W10-4231,"We remain several points below the bestperforming team from 2009 (ICSI-Berkeley), possibly due to the inclusion of additional items in their feature set, or the use of Conditional Random Fields as their learning technique (Favre and Bohnet, 2009).",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf,"(Favre and Bohnet, 2009)",5 Future Work,Our efforts during this iteration of the NEG task were primarily focused on enhancing our methods of choosing the best RE once the reference type was selected.,
1139043274,false,finalized,3,1/24/2017 09:37:01,Pos,1.0,W10-4231,"Moving forward, we hope to expand our feature set by including the morphology of words immediately surrounding the reference, as well as a more extensive reference history, as suggested by (Favre and Bohnet, 2009).",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf,"(Favre and Bohnet, 2009)","We suspect that these features may play a significant role in determining the type of referenced used, the prediction of which acts as a 'bottleneck' in generating exact REs.",5 Future Work,
1139043275,false,finalized,3,1/24/2017 09:17:42,Error,0.6486,W14-6402,"21' Traitement Automatique des Langues Naturelles, Marseille, 2014 [FondamenTAL-O.2]",http://www.aclweb.org/anthology/W/W14/W14-6402.pdf,"Traitement Automatique des Langues Naturelles, Marseille, 2014",Representation ontologique du LVF et son utilisation en traitement automatique de la langue,,
1139043276,false,finalized,3,1/24/2017 11:35:15,Neut,0.6677,W14-6402,"Certains travaux ont rendu le LVF plus accessible en termes d'encodage et de format de donn&es : Denis Le Pesant en a cr&& une version sous format Excel pour faciliter sa consultation manuelle, mais ce mode d'acc&s ne s'est pas av&r& pratique pour les applications informatiques ; Guy Lapalme en a alors propos& une version XML qui en facilite l'exploitation par les applications de traitement automatique de la langue et Hadouche et Lapalme (2010) l'ont compar&  d'autres ressources lexicales.",http://www.aclweb.org/anthology/W/W14/W14-6402.pdf,(2010),"Ces derni&res ann&es, il y a eu un regain d'int&ret pour la notion d'ontologie, sous l'impulsion du web s&mantique.","E cause de problemes de diffusion et de distribution, le LVF n'a malheureusement pas pu etre exploit& par les chercheurs et les linguistes qui, pour plusieurs, en ignoraient meme l'existence.",
1139043277,false,finalized,3,1/24/2017 09:30:33,Pos,0.6863,W14-6402,"Les ontologies repr&sentent des ressources de mod&lisation et de conceptualisation tr&s importantes (Noy et McGuinness, 2000).",http://www.aclweb.org/anthology/W/W14/W14-6402.pdf,"(Noy et McGuinness, 2000)","Elles constituent en soi un modele de donn&es repr&sentatif d'un ensemble de concepts dans un domaine, ainsi que des relations entre ces concepts.",On fournit ainsi des ontologies qui sont des ressources conceptuelles repr&sent&es par ces langages mod&lisant les domaines des connaissances et on facilite leur acc&s et leur partage.,
1139043278,false,finalized,3,1/24/2017 09:33:50,Neut,0.6733,W14-6402,"Contrairement  cela, d'autres approches consid&rent qu'il y a une s&mantique dans les documents XML qui peut etre d&couverte  partir de la structure des documents, en l'occurrence l'approche de Melnik (1999).",http://www.aclweb.org/anthology/W/W14/W14-6402.pdf,(1999),"Meme si XML n'est pas cens& repr&senter d'informations s&mantiques ou de s&mantique entre les donn&es, les balises imbriqu&es peuvent repr&senter une relation is-a ou part-of ou subType-of.","Certaines approches proposent une m&thode g&n&rique de transformation XML en modele OWL  partir d'un sch&ma XML et des donn&es du fichier XML, d'autres pensent qu'il est impossible de proposer une approche automatique convenable pour une transformation automatique complete de XML vers OWL, car XML ne d&finit aucune contrainte s&mantique.",
1139043279,false,finalized,3,1/24/2017 11:15:03,Neut,0.6642,W11-1403,"Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"(Enright et al., 2002",The goal of our item generation project is to develop a model to support optimal problem and test construction.,national project on item generation for testing student competencies in solving probability problems.,
1139043280,false,finalized,3,1/24/2017 09:53:01,Pos,0.689,W11-1403,"Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"Deane and Sheehan, 2003",The goal of our item generation project is to develop a model to support optimal problem and test construction.,national project on item generation for testing student competencies in solving probability problems.,
1139043281,false,finalized,3,1/24/2017 09:12:25,Pos,1.0,W11-1403,"Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"Arendasy et al., 2006",The goal of our item generation project is to develop a model to support optimal problem and test construction.,national project on item generation for testing student competencies in solving probability problems.,
1139043282,false,finalized,3,1/24/2017 11:27:35,Neut,1.0,W11-1403,"Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"Holling et al., 2009)",The goal of our item generation project is to develop a model to support optimal problem and test construction.,national project on item generation for testing student competencies in solving probability problems.,
1139043283,false,finalized,3,1/24/2017 09:15:03,Neut,1.0,W11-1403,"iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"iron and Williamson, 2002","A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).","(Left: German original, right: English translation.)",
1139043284,false,finalized,3,1/24/2017 09:45:36,Neut,1.0,W11-1403,"iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"Arendasy et al., 2006","A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).","(Left: German original, right: English translation.)",
1139043285,false,finalized,3,1/24/2017 12:28:21,Neut,1.0,W11-1403,"iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"Holling et al., 2009)","A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).","(Left: German original, right: English translation.)",
1139043286,false,finalized,3,1/24/2017 11:44:41,Neut,1.0,W11-1403,"A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"(Deane and Sheehan, 2003","However, this system focuses on semantic factors influencing the expression of events with different participants (e.g., different types of vehicles) rather than on generating linguistic variations.","iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).",
1139043287,false,finalized,3,1/24/2017 10:29:04,Neut,0.6788,W11-1403,"A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"Higgins et al., 2005)","However, this system focuses on semantic factors influencing the expression of events with different participants (e.g., different types of vehicles) rather than on generating linguistic variations.","iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).",
1139043288,false,finalized,3,1/24/2017 11:44:41,Neut,1.0,W11-1403,See Boer Rookhuiszen (2011) for more details on how probability problems are constructed.,http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,(2011),3 Language Generation,A warning is also issued if the edited problem contains properties for which no lexical information is available.,
1139043289,false,finalized,3,1/24/2017 10:41:36,Pos,0.6583,W11-1403,"Its architecture reflects the language generation pipeline of Reiter and Dale (2000), with three modules: Document Planner, Microplanner and Surface Realizer.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,(2000),"Information between the modules is exchanged in the form of a list of sentence trees, each defining the content and grammatical structure of a sentence.",An overview of the NLG component of Genpex is given in Figure 3.,
1139043290,false,finalized,3,1/24/2017 11:45:00,Neut,1.0,W11-1403,"According to Deane and Sheehan (2003), it is possible to change the wording of a text without changing its difficulty.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,(2003),"Reiter and Dale (2000) state that for example aggregating multiple sentences does not change the information they express, but improves the readability and fluency of the text.","When introducing variation in the narrative form of the exercise, it is important that variations of the same exercise should all have the same meaning and approximately the same difficulty.",
1139043291,false,finalized,3,1/24/2017 11:09:22,Neut,1.0,W11-1403,"Reiter and Dale (2000) state that for example aggregating multiple sentences does not change the information they express, but improves the readability and fluency of the text.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,(2000),This is what we want to achieve: adding variation to the text without affecting its interpretation.,"According to Deane and Sheehan (2003), it is possible to change the wording of a text without changing its difficulty.",
1139043292,false,finalized,3,1/24/2017 11:54:35,Neut,1.0,W11-1403,Fairon and Williams (2002).,http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,(2002),Aggregation.,"Given that understanding the question is crucial for solving the exercise, and that varying the way the questions are asked might cause confusion, we chose to adhere to a fixed format for the questions, cf.",
1139043293,false,finalized,3,1/24/2017 10:30:50,Neut,1.0,W11-1403,"This is the removal of duplicate words from sentences, which typically applies to aggregated sentences (Harbusch and Kempen, 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"(Harbusch and Kempen, 2009)","Genpex can apply different types of ellipsis, such as Gapping and Conjunction Reduction.",Ellipsis.,
1139043294,false,finalized,3,1/24/2017 09:29:13,Pos,0.646,W11-1403,"The existence of such preferred formulations is in line with the results of Cahill and Forst (2010), who carried out an experiment in which native speakers of German evaluated a number of alternative realisations of the same sentence.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,(2010),"Their subjects accepted some variation in word order, but showed a clear preference for some of the alternatives.","As a consequence, the system frequently applies too much or too little ellipsis to the generated sentences, with less than ideal (though not ungrammatical) results.",
1139043295,false,finalized,3,1/24/2017 09:54:59,Pos,1.0,W11-1403,"This makes it relatively easy to test the influence of those factors on the difficulty of the exercise, for example by carrying out the kind of statistical and cognitive analysis advocated by Graf et al (2005).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,(2005),"The effect of the main parameters of the probability problems in Genpex (i.e., the type of question being asked) was already statistically analyzed by Holling et al (2009) and Zeuch (In preparation).","The underlying probability problem is saved together with the text as well, so all factors that certainly or potentially influence item difficulty are known.",
1139043296,false,finalized,3,1/24/2017 11:46:02,Neut,1.0,W11-1403,"The effect of the main parameters of the probability problems in Genpex (i.e., the type of question being asked) was already statistically analyzed by Holling et al (2009) and Zeuch (In preparation).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,(2009),"They used automatically generated items similar to the exercises generated by Genpex, except that their exercises did not have variations in wording apart from context-related ones.","This makes it relatively easy to test the influence of those factors on the difficulty of the exercise, for example by carrying out the kind of statistical and cognitive analysis advocated by Graf et al (2005).",
1139043297,false,finalized,3,1/24/2017 09:04:17,Neut,0.6829,W11-1403,"Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,(2009),"Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).","They used automatically generated items similar to the exercises generated by Genpex, except that their exercises did not have variations in wording apart from context-related ones.",
1139043298,false,finalized,3,1/24/2017 11:50:33,Neut,1.0,W11-1403,"Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"(Enright et al., 2002","The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort.","Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.",
1139043299,false,finalized,3,1/24/2017 09:33:50,Pos,0.6611,W11-1403,"Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"Fairon and Williamson, 2002","The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort.","Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.",
1139043300,false,finalized,3,1/24/2017 10:51:02,Neut,0.6899,W11-1403,"Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"Deane and Sheehan, 2003","The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort.","Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.",
1139043301,false,finalized,3,1/24/2017 11:31:35,Neut,1.0,W11-1403,"Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"Arendasy et al., 2006","The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort.","Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.",
1139043302,false,finalized,3,1/24/2017 08:41:35,Pos,0.6444,W11-1403,"Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"Holling et al., 2008","The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort.","Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.",
1139043303,false,finalized,3,1/24/2017 11:51:29,Pos,0.3476,W11-1403,"Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf,"Holling et al., 2009)","The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort.","Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.",
1139043304,false,finalized,3,1/24/2017 10:18:53,Neut,1.0,P97-1062,"As the basic mechanism for parsing text into a shallow semantic representation, we choose a shiftreduce type parser (Marcus, 1980).",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf,"(Marcus, 1980)",It breaks parsing into an ordered sequence of small and manageable parse actions such as shift and reduce.,2 Basic Parsing Paradigm,
1139043305,false,finalized,3,1/24/2017 10:41:36,Neut,0.6478,P97-1062,"We extended the standard ID3 model (Quinlan, 1986) to more general hybrid decision structures.",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf,"(Quinlan, 1986)","In our tests, the best performing structure was a decision list (Rivest, 1987) of hierarchical decision trees, whose simplified basic structure is illustrated in figure 3.","A set of parse examples, as already described in the previous section, is then fed into an 1D3-based learning routine that generates a decision structure, which can then 'classify' any given parse state by proposing what parse action to perform next.",
1139043306,false,finalized,3,1/24/2017 10:44:35,Pos,0.689,P97-1062,"In our tests, the best performing structure was a decision list (Rivest, 1987) of hierarchical decision trees, whose simplified basic structure is illustrated in figure 3.",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf,"(Rivest, 1987)","Note that in the 'reduce operation tree', the system first decides whether or not to perform a reduction before deciding on a specific reduction.","We extended the standard ID3 model (Quinlan, 1986) to more general hybrid decision structures.",
1139043307,false,finalized,3,1/24/2017 10:59:22,Neut,0.6581,P97-1062,"We believe that an extensive collection of complex translation pairs in the bilingual dictionary is critical for translation quality and we are confident that its acquisition can be at least partially automated by using techniques like those described in (Smadja et al., 1996).",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf,"(Smadja et al., 1996)",Complex translation entries are preprocessed using the same parser as for normal text.,"ambiguous with respect to German, but the English compound conclusively maps to the German compound ""Zinssatz"".",
1139043308,false,finalized,3,1/24/2017 11:48:38,Neut,0.6966,P97-1062,"Our basic parsing and interactive training paradigm is based on (Simmons and Yu, 1992).",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf,"(Simmons and Yu, 1992)","We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component.",7 Related Work,
1139043309,false,finalized,3,1/24/2017 09:34:40,Neut,0.6525,P97-1062,"(Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993).",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf,"(Magerman, 1995)","Questioning the traditional n-grams, Magerman already advocates a heavier reliance on contextual information.","We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component.",
1139043310,false,finalized,3,1/24/2017 10:44:35,Neut,1.0,P97-1062,"(Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993).",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf,"(Marcus et al., 1993)","Questioning the traditional n-grams, Magerman already advocates a heavier reliance on contextual information.","We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component.",
1139043311,false,finalized,3,1/24/2017 11:54:11,Neut,1.0,P97-1062,"(Collins, 1996) focuses on bigram lexical dependencies (BLD).",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf,"(Collins, 1996)","Trained on the same 40,000 sentences as Spatter, it relies on a much more limited type of context than our system and needs little background knowledge.","While this necessitates the involvement of a parsing supervisor for training, we are able to perform deterministic parsing and get already very good test results for only 256 training sentences.",
1139043312,false,finalized,3,1/24/2017 11:31:10,Neut,1.0,W07-2203,Collins (1999) is a detailed exposition of one such ongoing line of research which utilizes the Wall Street Journal (WSJ) sections of the Penn Treebank (PTB).,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1999),"However, there are disadvantages to this approach.","Extant statistical parsers require extensive and detailed treebanks, as many of their lexical and structural parameters are estimated in a fullysupervised fashion from treebank derivations.",
1139043313,false,finalized,3,1/24/2017 10:51:02,Neut,1.0,W07-2203,"For example, Hockenmeier (2003) trains a statistical parser based on Combinatory Categorial Grammar (CCG) on the WSJ PTB, but first maps the treebank to CCG derivations semi-automatically.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(2003),"Thirdly, many (lexical) parameter estimates do not generalize well between domains.","Secondly, the richer the annotation required, the harder it is to adapt the treebank to train parsers which make different assumptions about the structure of syntactic analyses.",
1139043314,false,finalized,3,1/24/2017 11:42:56,Neut,0.6966,W07-2203,"For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(2001),"Tadayoshi et al (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format.","Thirdly, many (lexical) parameter estimates do not generalize well between domains.",
1139043315,false,finalized,3,1/24/2017 09:53:49,Neut,0.6741,W07-2203,"For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1999),"Tadayoshi et al (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format.","Thirdly, many (lexical) parameter estimates do not generalize well between domains.",
1139043316,false,finalized,3,1/24/2017 09:18:33,Neut,0.6776,W07-2203,"Tadayoshi et al (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(2005),"To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain.","For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.",
1139043317,false,finalized,3,1/24/2017 09:10:09,Weak,1.0,W07-2203,"Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001).",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,"(Baker, 1979","However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB.","To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain.",
1139043318,false,finalized,3,1/24/2017 11:07:20,Weak,0.67,W07-2203,"Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001).",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,"Prescher, 2001)","However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB.","To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain.",
1139043319,false,finalized,3,1/24/2017 11:16:08,Neut,1.0,W07-2203,"However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1992),They constrain the training data (parses) considered within the IOA to those consistent with the constituent boundaries defined by the bracketing.,"Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001).",
1139043320,false,finalized,3,1/24/2017 09:41:25,Pos,1.0,W07-2203,"More recently, both Riezler et al (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(2002),"In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing.",Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank.,
1139043321,false,finalized,3,1/24/2017 11:47:00,Neut,0.696,W07-2203,"More recently, both Riezler et al (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(2004),"In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing.",Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank.,
1139043322,false,finalized,3,1/24/2017 11:27:28,CoCo,0.6749,W07-2203,"We contrast an IOA-based EM method for training a PGLR parser (Inui et al., 1997), similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,"(Inui et al., 1997)","The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs).",We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data.,
1139043323,false,finalized,3,1/24/2017 09:33:50,Neut,1.0,W07-2203,Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1994),"These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models.","The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs).",
1139043324,false,finalized,3,1/24/2017 11:26:49,Neut,0.6677,W07-2203,Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1994),"These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models.","The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs).",
1139043325,false,finalized,3,1/24/2017 11:42:56,Neut,0.6966,W07-2203,"These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1992),Another motivation to explore alternative non-iterative methods is that the derivation space over partiallybracketed data can remain large (>1K) while the confidence-based methods we explore have a total processing overhead equivalent to one iteration of an IOA-based EM algorithm.,Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting.,
1139043326,false,finalized,3,1/24/2017 08:43:33,Neut,1.0,W07-2203,"constructed from this backbone (Tomita, 1987).",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,"(Tomita, 1987)",The residue of features not incorporated into the backbone are unified on each reduce action and if unification fails the associated derivation paths also fail.,24,
1139043327,false,finalized,3,1/24/2017 09:34:40,Pos,1.0,W07-2203,"The parser creates a packed parse forest represented as a graph-structured stack.2 The parse selection model ranks complete derivations in the parse forest by computing the product of the probabilities of the (shift/reduce) parse actions (given LR state and lookahead item) which created each derivation (Inui et al., 1997).",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,"(Inui et al., 1997)","Estimating action probabilities, consists of a) recording an action history for the correct derivation in the parse forest (for each sentence in a treebank), b) computing the frequency of each action over all action histories and c) normalizing these frequencies to determine probability distributions over conflicting (i.e.",The residue of features not incorporated into the backbone are unified on each reduce action and if unification fails the associated derivation paths also fail.,
1139043328,false,finalized,3,1/24/2017 11:12:23,Neut,1.0,W07-2203,"Inui et al (1997) describe the probability model utilized in the system where a transition is represented by the probability of moving from one stack state, i1, (an instance of the graph structured stack) to another, i.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1997),"They estimate this probability using the stack-top state si1, next input symbol li and next action ai.",shift/reduce or reduce/reduce) actions.,
1139043329,false,finalized,3,1/24/2017 09:12:49,Neut,0.6815,W07-2203,"In addition, Laplace estimation can be used to ensure that all actions in the 2The parse forest is an instance of a feature forest as defined by Miyao and Tsujii (2002).",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(2002),We will use the term 'node' herein to refer to an element in a derivation tree or in the parse forest that corresponds to a (sub-)analysis whose label is the mother's label in the corresponding CF 'backbone' rule.,"Therefore, normalization is performed over all lookaheads for a state or over each lookahead for the state depending on whether the state is a member of Ss or Sr, respectively (hereafter the I function).",
1139043330,false,finalized,3,1/24/2017 11:05:48,Neut,0.6599,W07-2203,"Following Pereira and Schabes (1992) given t = (s, U), a node's span in the parse forest is valid if it does not overlap with any span outlined in U, and hence, a derivation is correct if the span of every node in the derivation is valid in U.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1992),"That is, if no crossing brackets are present in the derivation.","In this case, equality between the derivation tree and the treebank annotation A identifies the correct derivation.",
1139043331,false,finalized,3,1/24/2017 10:16:42,Pos,0.661,W07-2203,"The Susanne Treebank (Sampson, 1995) is utilized to create fully annotated training data.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,"(Sampson, 1995)",25,3.2 The Susanne Treebank and Baseline Training Data,
1139043332,false,finalized,3,1/24/2017 10:19:40,Neut,1.0,W07-2203,"King et al (2003) describe the development of the PARC 700 Dependency Bank, a goldstandard set of relational dependencies for 700 sentences (from the PTB) drawn at random from section 23 of the WSJ (the de facto standard test set for statistical parsing).",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(2003),"In all the evaluations reported in this paper we test our parser over a gold-standard set of relational dependencies compatible with our parser output derived (Briscoe and Carroll, 2006) from the PARC 700 Dependency Bank (DepBank, henceforth).",3.4 The DepBank Test Data,
1139043333,false,finalized,3,1/24/2017 09:08:36,Neut,0.6486,W07-2203,"In all the evaluations reported in this paper we test our parser over a gold-standard set of relational dependencies compatible with our parser output derived (Briscoe and Carroll, 2006) from the PARC 700 Dependency Bank (DepBank, henceforth).",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,"(Briscoe and Carroll, 2006)",The Susanne Corpus is a (balanced) subset of the Brown Corpus which consists of 15 broad categories of American English texts.,"King et al (2003) describe the development of the PARC 700 Dependency Bank, a goldstandard set of relational dependencies for 700 sentences (from the PTB) drawn at random from section 23 of the WSJ (the de facto standard test set for statistical parsing).",
1139043334,false,finalized,3,1/24/2017 10:17:49,Neut,0.6503,W07-2203,"We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-ofdomain training data.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(2001),4 The Evaluation Scheme,All but one category (reportage text) is drawn from different domains than the WSJ.,
1139043335,false,finalized,3,1/24/2017 11:00:54,Neut,0.6599,W07-2203,"The parser's output is evaluated using a relational dependency evaluation scheme (Carroll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,"(Carroll, et al., 1998",Relations are organized into a hierarchy with the root node specifying an unlabeled dependency.,4 The Evaluation Scheme,
1139043336,false,finalized,3,1/24/2017 11:39:37,Neut,1.0,W07-2203,"The parser's output is evaluated using a relational dependency evaluation scheme (Carroll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,"Lin, 1998)",Relations are organized into a hierarchy with the root node specifying an unlabeled dependency.,4 The Evaluation Scheme,
1139043337,false,finalized,3,1/24/2017 11:27:28,Neut,0.6649,W07-2203,"The microaveraged F1 score for the baseline system using this evaluation scheme is 75.61%, which - over similar sets of relational dependencies - is broadly comparable to recent evaluation results published by King and collaborators with their state-of-theart parsing system (Briscoe et al., 2006).",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,"(Briscoe et al., 2006)","3The pipeline is the same as that used for creating S though we do not automatically map the bracketing to be more consistent with the system grammar, instead, we simply removed unary brackets.","The microaveraged precision, recall and F1 scores are calculated from the counts for all relations in the hierarchy which subsume the parser output.",
1139043338,false,finalized,3,1/24/2017 09:30:28,Pos,0.6741,W07-2203,As the number of samples (sentences) is large we use the normal approximation for z. Siegel and Castellan (1988) describe and motivate this test.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1988),"We use a 0.05 level of significance, and provide z-value probabilities for significant results reported below.","For example, to compare the accuracy of two parsers over the same data set.",
1139043339,false,finalized,3,1/24/2017 11:29:19,Neut,0.6649,W07-2203,"An alternative approach that we have also explored is to utilize a similar bootstrapping approach with data partially-annotated for grammatical relations (Watson and Briscoe, 2007).",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,"(Watson and Briscoe, 2007)",5.1 Confidence-Based Approaches,"We leave for the future a more extensive investigation of these cases which, in principle, would allow us to make more use of this training data.",
1139043340,false,finalized,3,1/24/2017 09:42:21,Neut,1.0,W07-2203,"corresponding normalized inside-outside weight for each node (Watson et al., 2005).",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,"(Watson et al., 2005)","We perform EM starting from two initial models; either a uniform probability model, IL(), or from models derived from unambiguous training data, 'y.",$represents the statistical significance of the system against the baseline model.,
1139043341,false,finalized,3,1/24/2017 10:32:04,Neut,1.0,W07-2203,The graphs of EM performance from iteration 1 illustrate the same 'classical' and 'initial' patterns observed by Elworthy (1994).,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1994),"When EM is initialized from a relatively poor model, such as that built from S (Figure 2), a 'classical'","In most cases, these results are significant, even when we manually select the best model (iteration) for EM.",
1139043342,false,finalized,3,1/24/2017 09:17:42,Neut,1.0,W07-2203,"The steep drop in performance (down to 69.93% F1) after the first iteration is probably due to loss of information from S. However, this run also eventually converges to similar performance, suggesting that the information in S is effectively disregarded as it forms only a small portion of SW, and that these runs effectively converge to a local maximum over W. Bacchiani et al (2006), working in a similar framework, explore weighting the contribution (frequency counts) of the in-domain and out-ofdomain training datasets and demonstrate that this can have beneficial effects.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(2006),"Furthermore, they also tried unsupervised tuning to the indomain corpus by weighting parses for it by their normalized probability.","In this case, the graph illustrates a combination of Elworthy's 'initial' and 'classical' patterns.",
1139043343,false,finalized,3,1/24/2017 10:20:20,Pos,0.689,W07-2203,"An immediate benefit of this work is that (re)training parsers with incrementally-modified grammars based on different linguistic frameworks should be much more straightforward - see, for example Oepen et al (2002) for a good discussion of the problem.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(2002),"Furthermore, it suggests that it may be possible to usefully tune",Mapping an incompatible annotated treebank to a compatible partially-bracketed corpus is relatively easy compared to mapping to a compatible fully-annotated corpus.,
1139043344,false,finalized,3,1/24/2017 11:53:59,Neut,1.0,W07-2203,Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available).,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1994),The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents.,a parser to a new domain with less annotation effort.,
1139043345,false,finalized,3,1/24/2017 11:31:10,Pos,0.3381,W07-2203,Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available).,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1994),The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents.,a parser to a new domain with less annotation effort.,
1139043346,false,finalized,3,1/24/2017 10:48:06,Neut,1.0,W07-2203,These findings may not hold if the level of bracketing available does not adequately constrain the parses considered - see Hwa (1999) for a related investigation with EM.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(1999),"In future work we intend to further investigate the problem of tuning to a new domain, given that minimal manual effort is a major priority.",The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents.,
1139043347,false,finalized,3,1/24/2017 12:28:21,Neut,1.0,W07-2203,"For instance, Bacchiani et al (2006) demonstrate imrpovements in parsing accuracy with unsupervised adaptation from unannotated data and explore the effect of different weighting of counts derived from the supervised and unsupervised data.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf,(2006),Acknowledgements,"Finally, further experiments on weighting the contribution of each dataset might be beneficial.",
1139043348,false,finalized,3,1/24/2017 09:33:50,Neut,1.0,W12-5209,"Ontology is defined as 'Explicit specification of conceptualization' (Gruber, 1993).",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"(Gruber, 1993)","As a knowledge representation formalism, ontologies have found a wide range of applications in the areas like knowledge management, information retrieval and information extraction.",1 Introduction,
1139043349,false,finalized,3,1/24/2017 11:43:08,Neut,0.689,W12-5209,"Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"(Ahmad et al., 1999",Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.,The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.,
1139043350,false,finalized,3,1/24/2017 08:44:30,Neut,0.6815,W12-5209,"Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"Kozakov et al., 2004",Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.,The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.,
1139043351,false,finalized,3,1/24/2017 09:41:25,Neut,0.6849,W12-5209,"Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"Sclano and Velardi, 2007",Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.,The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.,
1139043352,false,finalized,3,1/24/2017 10:28:10,Neut,0.6788,W12-5209,"Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"Frantzi et al., 1998",Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.,The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.,
1139043353,false,finalized,3,1/24/2017 09:33:50,Neut,0.6733,W12-5209,"Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"Gacitua et al., 2011)",Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.,The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.,
1139043354,false,finalized,3,1/24/2017 11:40:49,Neut,1.0,W12-5209,"As noted by Fountain and Lapata (2012), 'Most of the existing approaches construct flat structure rather than a taxonomy.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(2012),"Also, the automatically constructed ontologies often create false association between terms and result in erroneous concept hierarchy (Zhou, 2007).",Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.,
1139043355,false,finalized,3,1/24/2017 11:27:28,Neut,1.0,W12-5209,"Also, the automatically constructed ontologies often create false association between terms and result in erroneous concept hierarchy (Zhou, 2007).",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"(Zhou, 2007)","In order to handle the above mentioned issues, we propose a graph-based ontology learning algorithm.","As noted by Fountain and Lapata (2012), 'Most of the existing approaches construct flat structure rather than a taxonomy.",
1139043356,false,finalized,3,1/24/2017 11:54:35,Neut,1.0,W12-5209,"'Terms with high information content remain lower in the concept hierarchy and terms with low information content remain higher in the concept hierarchy' (Resnik, 1999).",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"(Resnik, 1999)",Caraballo and Charniak (1999) have shown that the term frequency is a good indicator of determining specificity of a term.,Our approach is based on the information content of the term.,
1139043357,false,finalized,3,1/24/2017 11:45:00,Neut,1.0,W12-5209,Caraballo and Charniak (1999) have shown that the term frequency is a good indicator of determining specificity of a term.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(1999),We divide the initial set of terms into different partitions based on the term frequency and then construct k-partite graph by finding subsumption relation between the terms of different partitions.,"'Terms with high information content remain lower in the concept hierarchy and terms with low information content remain higher in the concept hierarchy' (Resnik, 1999).",
1139043358,false,finalized,3,1/24/2017 11:46:02,Neut,0.6649,W12-5209,"The proposed approach combines evidences from linguistic patterns and WordNet (Fellbaum, 1998) to detect subsumption relation.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"(Fellbaum, 1998)",The patterns used in the system are generic and can be used across languages.,This early identification of hierarchy creates a better taxonomic structure and avoids false association between the terms.,
1139043359,false,finalized,3,1/24/2017 11:53:02,Neut,1.0,W12-5209,"Wordnets of Indian languages are linked with each other and English WordNet through a common index (Bhattacharyya, 2010), which makes it possible to share concept definitions across languages.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"(Bhattacharyya, 2010)",Following are the major features of the proposed system:,The patterns used in the system are generic and can be used across languages.,
1139043360,false,finalized,3,1/24/2017 10:48:06,Neut,1.0,W12-5209,"As noted by Leenheer and Moor (2005), 'No matter how expressive ontologies might be, they are all in fact lexical representations of concepts'.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(2005),The linguistic basis of formal ontology is such that a significant portion of domain ontology can be extracted automatically from the domain related texts using language processing techniques.,2 Related work,
1139043361,false,finalized,3,1/24/2017 10:29:35,Neut,0.6788,W12-5209,"Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"(Hearst, 1992",Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text.,"Ontology learning approaches can be divided into three categories: heuristic based, statistical and hybrid techniques.",
1139043362,false,finalized,3,1/24/2017 09:22:48,Neut,1.0,W12-5209,"Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"Berland and Charniak, 1999",Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text.,"Ontology learning approaches can be divided into three categories: heuristic based, statistical and hybrid techniques.",
1139043363,false,finalized,3,1/24/2017 11:37:54,Neut,1.0,W12-5209,"Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"Girju et al., 2003)",Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text.,"Ontology learning approaches can be divided into three categories: heuristic based, statistical and hybrid techniques.",
1139043364,false,finalized,3,1/24/2017 11:20:13,Neut,1.0,W12-5209,Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(1992),"She described a syntagmatic technique for identifying hyponymy relations in free text by using frequently occurring patterns like 'NP0 such as NP1, NP2, ...,NPn'.","Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.",
1139043365,false,finalized,3,1/24/2017 08:45:38,Neut,1.0,W12-5209,"Berland and Charniak (1999) used a pattern-based approach to find out part-whole relationships (such as between car and door, or car and engine) in a text.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(1999),Heuristic approaches rely on language-specific rules which cannot be transferred from one language to another.,"She described a syntagmatic technique for identifying hyponymy relations in free text by using frequently occurring patterns like 'NP0 such as NP1, NP2, ...,NPn'.",
1139043366,false,finalized,3,1/24/2017 10:30:50,Neut,1.0,W12-5209,"Statistical methods relate concepts based on distributional hypothesis (Harris, 1968), that is 'similar terms appear in the similar context.'",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"(Harris, 1968)",Hindle (1990) performed semantic clustering to find semantically similar nouns.,Statistical approaches model ontology learning as a classification or clustering problem.,
1139043367,false,finalized,3,1/24/2017 10:16:28,Neut,1.0,W12-5209,Hindle (1990) performed semantic clustering to find semantically similar nouns.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(1990),They calculated the co-occurrence weight for each verb-subject and verb-object pair.,"Statistical methods relate concepts based on distributional hypothesis (Harris, 1968), that is 'similar terms appear in the similar context.'",
1139043368,false,finalized,3,1/24/2017 10:19:40,Neut,1.0,W12-5209,Pereira et al (1993) proposed a divisive clustering method to induce noun hierarchy from an encyclopedia.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(1993),"Hybrid approaches leverage the strengths of both statistical and heuristic based approaches and often use evidences from existing knowledge bases such as wordnet, wikipedia, etc.",Verb-wise similarity of two nouns is calculated as the minimum shared weight and the similarity of two nouns is the sum of all verb-wise similarities.,
1139043369,false,finalized,3,1/24/2017 09:30:28,Neut,1.0,W12-5209,Caraballo (1999) combined the lexico-syntactic patterns and distributional similarity based methods to construct ontology.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(1999),Similarity between two nouns is calculated by computing the cosine between their respective vectors and used for hierarchical bottom-up clustering.,"Hybrid approaches leverage the strengths of both statistical and heuristic based approaches and often use evidences from existing knowledge bases such as wordnet, wikipedia, etc.",
1139043370,false,finalized,3,1/24/2017 11:50:36,Neut,0.6722,W12-5209,"In a similar approach, Cimiano et al (2005) clustered nouns based on distributional similarity and used Hearst-patterns, WordNet (Fellbaum, 1998) and patterns on the web as a hypernymy oracle for constructing a hierarchy.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(2005),"Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy.",Hearst-patterns are used to detect hypernymy relation between similar nouns.,
1139043371,false,finalized,3,1/24/2017 11:48:42,Neut,0.6649,W12-5209,"In a similar approach, Cimiano et al (2005) clustered nouns based on distributional similarity and used Hearst-patterns, WordNet (Fellbaum, 1998) and patterns on the web as a hypernymy oracle for constructing a hierarchy.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"(Fellbaum, 1998)","Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy.",Hearst-patterns are used to detect hypernymy relation between similar nouns.,
1139043372,false,finalized,3,1/24/2017 08:41:35,Neut,0.3556,W12-5209,"Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"(Caraballo, 1999)",Dominguez Garcia et al (2012) used wikipedia to extract ontology for different languages.,"In a similar approach, Cimiano et al (2005) clustered nouns based on distributional similarity and used Hearst-patterns, WordNet (Fellbaum, 1998) and patterns on the web as a hypernymy oracle for constructing a hierarchy.",
1139043373,false,finalized,3,1/24/2017 11:32:37,Neut,1.0,W12-5209,Dominguez Garcia et al (2012) used wikipedia to extract ontology for different languages.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(2012),"Like Cimiano et al (2005), we follow a hybrid approach and construct a concept hierarchy using distributional similarity, patterns and WordNet.","Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy.",
1139043374,false,finalized,3,1/24/2017 09:58:46,Pos,0.6836,W12-5209,"Like Cimiano et al (2005), we follow a hybrid approach and construct a concept hierarchy using distributional similarity, patterns and WordNet.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(2005),"However, instead of performing top-down or bottom-up clustering, we pose ontology learning as a k-partite graph construction problem.",Dominguez Garcia et al (2012) used wikipedia to extract ontology for different languages.,
1139043375,false,finalized,3,1/24/2017 10:44:35,Neut,0.6503,W12-5209,"Ryu and Choi (2006) also used term frequency as a measure of domain specificity, but instead of partitioning they combined term frequency and distributional similarity to construct hierarchy.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(2006),Other method similar to our work is proposed in Fountain and Lapata (2012).,We use term frequency to determine the position of a concept in the hierarchy.,
1139043376,false,finalized,3,1/24/2017 10:59:22,Neut,0.6581,W12-5209,Other method similar to our work is proposed in Fountain and Lapata (2012).,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(2012),Fountain and Lapata (2012) proposed a graph based approach that does not require a separate term extraction step.,"Ryu and Choi (2006) also used term frequency as a measure of domain specificity, but instead of partitioning they combined term frequency and distributional similarity to construct hierarchy.",
1139043377,false,finalized,3,1/24/2017 12:00:42,Neut,1.0,W12-5209,Fountain and Lapata (2012) proposed a graph based approach that does not require a separate term extraction step.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,(2012),"However, their approach works with a predefined set of seed terms.",Other method similar to our work is proposed in Fountain and Lapata (2012).,
1139043378,false,finalized,3,1/24/2017 11:35:47,Neut,0.6525,W12-5209,"Terms are filtered out using weirdness measure (Ahmad et al., 1999).",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"(Ahmad et al., 1999)","Feature vector for each term is created by including co-occurring nouns, verbs and adjectives.",Relevance of the key term in the corpus is calculated by counting the frequency of the term.,
1139043379,false,finalized,3,1/24/2017 11:41:49,Neut,1.0,W12-5209,"Head word heuristic (Cimiano, 2006) based pattern (NP)  (NP) is used to identify subsumption relation.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf,"(Cimiano, 2006)","As per head word heuristic (NP1)(NP2) implies (NP2) subsumes (NP1NP2), e.g.",Two patterns are used to detect subsumption and neighbor relations.,
1139043380,false,finalized,3,1/24/2017 10:29:04,Neut,0.6788,W14-5605,"Traditional readability formulas normally take into account the number of words per sentence or/and the number of ""hard"", be it long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 1998; Greenfield, 2004).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Kincaid, Fishburne, Rogers, & Chissom, 1975","Both the first and the second ratio will be equal to the number of words in a claim sentence where practically all words are ""hard"" terms, some of them used for the first time.",Anybody who has seen patent claims at least once will find it unnecessary to calculate claim readability indices to get persuaded that the claim text is extremely low readable.,
1139043381,false,finalized,3,1/24/2017 09:04:17,Neut,0.6829,W14-5605,"Traditional readability formulas normally take into account the number of words per sentence or/and the number of ""hard"", be it long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 1998; Greenfield, 2004).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"Brown, 1998","Both the first and the second ratio will be equal to the number of words in a claim sentence where practically all words are ""hard"" terms, some of them used for the first time.",Anybody who has seen patent claims at least once will find it unnecessary to calculate claim readability indices to get persuaded that the claim text is extremely low readable.,
1139043382,false,finalized,3,1/24/2017 11:09:22,Neut,1.0,W14-5605,"Traditional readability formulas normally take into account the number of words per sentence or/and the number of ""hard"", be it long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 1998; Greenfield, 2004).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"Greenfield, 2004)","Both the first and the second ratio will be equal to the number of words in a claim sentence where practically all words are ""hard"" terms, some of them used for the first time.",Anybody who has seen patent claims at least once will find it unnecessary to calculate claim readability indices to get persuaded that the claim text is extremely low readable.,
1139043383,false,finalized,3,1/24/2017 11:35:15,Neut,1.0,W14-5605,"2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just ""regular"" readers (Graesser et al., 2004).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"Orletta et al., 2011)",Text simplification is most often performed on the sentence level.,"The mainstream of text simplification is developing methodologies and tools for general types of texts that address people with special needs, such as poor literacy readers (Aluisio et al.",
1139043384,false,finalized,3,1/24/2017 09:12:10,Neut,1.0,W14-5605,"2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just ""regular"" readers (Graesser et al., 2004).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Bott et al., 2012)",Text simplification is most often performed on the sentence level.,"The mainstream of text simplification is developing methodologies and tools for general types of texts that address people with special needs, such as poor literacy readers (Aluisio et al.",
1139043385,false,finalized,3,1/24/2017 09:37:01,Neut,1.0,W14-5605,"2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just ""regular"" readers (Graesser et al., 2004).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Crossley and McNamara, 2008)",Text simplification is most often performed on the sentence level.,"The mainstream of text simplification is developing methodologies and tools for general types of texts that address people with special needs, such as poor literacy readers (Aluisio et al.",
1139043386,false,finalized,3,1/24/2017 09:08:36,Neut,1.0,W14-5605,"2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just ""regular"" readers (Graesser et al., 2004).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Graesser et al., 2004)",Text simplification is most often performed on the sentence level.,"The mainstream of text simplification is developing methodologies and tools for general types of texts that address people with special needs, such as poor literacy readers (Aluisio et al.",
1139043387,false,finalized,3,1/24/2017 09:16:33,Pos,0.6611,W14-5605,"An intuitive approach relies mainly on the developers' intuition and experience (Allen, 2009) that leads to using less lexical diversity, less sophisticated words, less syntactic complexity, and greater cohesion.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Allen, 2009)","A structural approach depends on the use of structure and word lists that are predefined by the intelligence level, as typically found in targeted readers.",Simplifying texts to provide more comprehensible input to a targeted audience the developers generally work within two approaches: an intuitive approach and a structural approach.,
1139043388,false,finalized,3,1/24/2017 10:30:50,Neut,1.0,W14-5605,"(Siddharthan, 2002) describe the implementation of the three stages - analysis, transforma",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Siddharthan, 2002)",42,"Automated text simplification tools are trying to achieve this purpose by combining linguistic and statistical techniques and penalize writers for polysyllabic words and long, complex sentences.",
1139043389,false,finalized,3,1/24/2017 11:05:48,Neut,1.0,W14-5605,"Some works on text simplification use parallel corpora of original and simplified sentences (Petersen & Ostendorf, 2007).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Petersen & Ostendorf, 2007)","There are works where text simplification is treated as a ""translation task within a RBMT (Takao and Sumita.","tion and regeneration, system that lay particular emphasis on the discourse level aspects of syntactic simplification.",
1139043390,false,finalized,3,1/24/2017 11:25:20,Neut,1.0,W14-5605,"In (Specia, 2010) text simplification is developed in the Statistical Machine Translation framework, given a parallel corpus of original and simplified texts, aligned at the sentence level.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Specia, 2010)","In (Poornima et al.2011) a rule based technique is proposed to simplify the complex sentences based on connectives like relative pronouns, coordinating and subordinating conjunctions.",2003).,
1139043391,false,finalized,3,1/24/2017 09:41:07,Neut,1.0,W14-5605,"(Bott, et al., 2012) describe a hybrid automatic text simplification system which combines a rule based core module with a statistical support module that controls the application of rules in the wrong contexts.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Bott, et al., 2012)",The approaches to patent claim simplification can be roughly put into two groups.,Sentence simplification is expressed as the list of sub-sentences that are portions of the original sentence.,
1139043392,false,finalized,3,1/24/2017 08:42:48,Neut,1.0,W14-5605,"For example, in (Shinmori et al., 2003) the discourse structure of the patent claim is built by means of a rule-based technique; each discourse segment is then paraphrased.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Shinmori et al., 2003)","In (Mille and Wanner, 2008) the claim sentence (by means of lexical and punctuation clues) is segmented into clausal units, that are then compressed into a summary.","Studies of the first group try to adapt to the patent domain general text simplification techniques and involve lexical and/or structural substitution, pruning, paraphrasing, etc.",
1139043393,false,finalized,3,1/24/2017 11:34:25,Neut,1.0,W14-5605,"In (Mille and Wanner, 2008) the claim sentence (by means of lexical and punctuation clues) is segmented into clausal units, that are then compressed into a summary.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Mille and Wanner, 2008)","The simplification methods proposed by this group of researches to some extent change the original content of the claim that might not always be desirable, especially for patent experts.","For example, in (Shinmori et al., 2003) the discourse structure of the patent claim is built by means of a rule-based technique; each discourse segment is then paraphrased.",
1139043394,false,finalized,3,1/24/2017 10:32:39,Neut,1.0,W14-5605,"For example, in one of the earlier works a rule-based technique was developed for decomposing the complex sentence of a claim into a set of simple sentences while preserving the initial content (Sheremetyeva, 2003).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Sheremetyeva, 2003)","Most recently (Shinmori et al., 2012) suggested aligning claim phrases with explanatory text from the description section, while (Ferraro et al., 2014) proposed an approach that involves highlighting the claim segments borders and reformatting the original text so as to emphasis segments with the identified border marker.","Another group of studies focuses on segmenting, reformatting or highlighting certain parts of the patent claim without changing the content of the original.",
1139043395,false,finalized,3,1/24/2017 10:17:49,Neut,0.689,W14-5605,"Most recently (Shinmori et al., 2012) suggested aligning claim phrases with explanatory text from the description section, while (Ferraro et al., 2014) proposed an approach that involves highlighting the claim segments borders and reformatting the original text so as to emphasis segments with the identified border marker.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Shinmori et al., 2012)","This approach does not involve any syntactic restructuring, just visualization of claim segments.","For example, in one of the earlier works a rule-based technique was developed for decomposing the complex sentence of a claim into a set of simple sentences while preserving the initial content (Sheremetyeva, 2003).",
1139043396,false,finalized,3,1/24/2017 10:28:48,Neut,0.6788,W14-5605,"Most recently (Shinmori et al., 2012) suggested aligning claim phrases with explanatory text from the description section, while (Ferraro et al., 2014) proposed an approach that involves highlighting the claim segments borders and reformatting the original text so as to emphasis segments with the identified border marker.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Ferraro et al., 2014)","This approach does not involve any syntactic restructuring, just visualization of claim segments.","For example, in one of the earlier works a rule-based technique was developed for decomposing the complex sentence of a claim into a set of simple sentences while preserving the initial content (Sheremetyeva, 2003).",
1139043397,false,finalized,3,1/24/2017 08:43:33,Neut,1.0,W14-5605,"2006; Radack, 1995) on how to read patent claims and conducted extensive interviews with patent experts of several companies in the US and Europe handling intellectual property1.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"Radack, 1995)",The recommendations are as follows.,In preparing for this research we have investigated professional instructions (Pressman.,
1139043398,false,finalized,3,1/24/2017 11:00:54,Neut,1.0,W14-5605,"This knowledge is structured following the methodology described in (Sheremetyeva, 1999; Sheremetyeva, 2003) and is mostly coded in the system lexicon as well as in analysis and generation rules.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Sheremetyeva, 1999",Different modules of the micro-level simplification component use specific parts and types of linguistic knowledge included in the lexicon and their own specific sets of rules.,Micro-level simplification at each of its stages is done by means of a specific combination of rulebased and statistical techniques and relies on linguistic knowledge of different depth.,
1139043399,false,finalized,3,1/24/2017 09:39:38,Pos,0.6741,W14-5605,"This knowledge is structured following the methodology described in (Sheremetyeva, 1999; Sheremetyeva, 2003) and is mostly coded in the system lexicon as well as in analysis and generation rules.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"Sheremetyeva, 2003)",Different modules of the micro-level simplification component use specific parts and types of linguistic knowledge included in the lexicon and their own specific sets of rules.,Micro-level simplification at each of its stages is done by means of a specific combination of rulebased and statistical techniques and relies on linguistic knowledge of different depth.,
1139043400,false,finalized,3,1/24/2017 11:30:03,Neut,1.0,W14-5605,"To extract (and then highlight) nominal terminology we use the NP extractor described in (Sheremetyeva, 2009).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Sheremetyeva, 2009)","The extraction methodology combines statistical techniques, heuristics and a very shallow linguistic knowledge extracted from the main system lexicon (see Section 5.1).",This task is performed based on the results of a shallow analysis performed by a hybrid NP extractor and NP and predicate term chunkers which in succession run on the same claim text.,
1139043401,false,finalized,3,1/24/2017 08:45:38,Neut,0.6785,W14-5605,"We here used the automatic text planner of the claim generator that was developed as a module of a patent MT system (Sheremetyeva, 2007).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Sheremetyeva, 2007)",Figure 4.,Simplification of a claim text into a diagram is performed based of the internal claim representation as shown in Section 5.3.,
1139043402,false,finalized,3,1/24/2017 12:00:42,Neut,1.0,W14-5605,"Some of the researchers admit avoiding qualitative evaluation due to the lack of resources that would have made it possible (Mille and Wanner, 2008).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Mille and Wanner, 2008)","The number of patents the authors use to evaluate their methodologies might seem quite limited, e.g., (Mille and Wanner, 2008) report evaluation results based on 30 patents; in (Bouayad-Agha et al.)",Given that no reliable evaluation metrics exist so far for text simplification we performed a preliminary qualitative evaluation of our methodology based on human judgment (as in all cited works on claim simplification).,
1139043403,false,finalized,3,1/24/2017 11:44:25,Neut,0.6677,W14-5605,"The number of patents the authors use to evaluate their methodologies might seem quite limited, e.g., (Mille and Wanner, 2008) report evaluation results based on 30 patents; in (Bouayad-Agha et al.)",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf,"(Mille and Wanner, 2008)",the test corpus consisted of 29 patents; (Ferraro et al.,"Some of the researchers admit avoiding qualitative evaluation due to the lack of resources that would have made it possible (Mille and Wanner, 2008).",
1139043404,false,finalized,3,1/24/2017 11:50:44,Neut,1.0,W10-1719,"The baseline systems for the translation directions German-English and English-German are both developed using Discriminative Word Alignment (Niehues and Vogel, 2008) and the Moses Toolkit (Koehn et al., 2007) for extracting phrase pairs and generating the phrase table from the discriminative word alignments.",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,"(Niehues and Vogel, 2008)",The difficult reordering between German and English was modeled using POS-based reordering rules.,2 Baseline System,
1139043405,false,finalized,3,1/24/2017 11:53:02,Neut,1.0,W10-1719,"The baseline systems for the translation directions German-English and English-German are both developed using Discriminative Word Alignment (Niehues and Vogel, 2008) and the Moses Toolkit (Koehn et al., 2007) for extracting phrase pairs and generating the phrase table from the discriminative word alignments.",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,"(Koehn et al., 2007)",The difficult reordering between German and English was modeled using POS-based reordering rules.,2 Baseline System,
1139043406,false,finalized,3,1/24/2017 11:54:11,Neut,1.0,W10-1719,"The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages.",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,"(Schmid, 1994)","Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al (2005).",These rules were learned using a word-aligned parallel corpus.,
1139043407,false,finalized,3,1/24/2017 08:46:56,Neut,1.0,W10-1719,"Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al (2005).",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,"(Vogel, 2003)","2.1 Training, Development and Test Data","The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages.",
1139043408,false,finalized,3,1/24/2017 09:48:56,Pos,0.6607,W10-1719,"Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al (2005).",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,(2005),"2.1 Training, Development and Test Data","The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages.",
1139043409,false,finalized,3,1/24/2017 08:45:38,Pos,0.6356,W10-1719,"When translating from German to English, we apply compound splitting as described in Koehn and Knight (2003) to the German corpus.",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,(2003),As a last preprocessing step we remove sentences that are too long and empty lines to obtain the final training corpus.,"If the new word is a correct word according to the hunspell lexicon using the new spelling rules, we map the words.",
1139043410,false,finalized,3,1/24/2017 09:16:33,Pos,0.6657,W10-1719,"For short-range reorderings, continuous reordering rules were applied to the test sentences (Rottmann and Vogel, 2007).",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,"(Rottmann and Vogel, 2007)","To model the long-range reorderings between German and English, different types of noncontinuous reordering rules were applied depending on the translation direction.",These possible reorderings were learned based on the POS of the source language words in the training corpus and the information about alignments between source and target language words in the corpus.,
1139043411,false,finalized,3,1/24/2017 11:43:08,Neut,0.6503,W10-1719,"(Niehues and Kolss, 2009).",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,"(Niehues and Kolss, 2009)","When translating from English to German, most of the changes in word order consist in a shift to the right while typical word shifts in German to English translations take place in the reverse direction.","To model the long-range reorderings between German and English, different types of noncontinuous reordering rules were applied depending on the translation direction.",
1139043412,false,finalized,3,1/24/2017 10:59:22,Neut,0.6581,W10-1719,"In addition, we applied phrase table smoothing as described in Foster et al (2006).",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,(2006),"Furthermore, we extended the translation model by additional features for unaligned words and introduced bilingual language models.","The phrase table was trained using the Moses training scripts, but for the German to English system we used a different phrase extraction method described in detail in Section 4.2.",
1139043413,false,finalized,3,1/24/2017 11:32:37,Pos,0.672,W10-1719,We used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead.,http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,(2008),This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++2 Toolkit and POS information.,Then these alignments are used to extract the phrase pairs.,
1139043414,false,finalized,3,1/24/2017 12:26:18,Neut,1.0,W10-1719,For more details see Niehues and Vogel (2008).,http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,(2008),4.2 Lattice Phrase Extraction,"The model was trained in three steps, first using maximum likelihood optimization and afterwards it was optimized towards the alignment error rate.",
1139043415,false,finalized,3,1/24/2017 09:22:53,Pos,0.6583,W10-1719,"Therefore, we build lattices that encode the different reorderings for every training sentence, as described in Niehues et al (2009).",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,(2009),"Then we can not only extract phrase pairs from the monotone source path, but also from the reordered paths.",139,
1139043416,false,finalized,3,1/24/2017 11:40:11,Neut,1.0,W10-1719,Guzman et al (2009) analyzed the role of the word alignment in the phrase extraction process.,http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,(2009),"To better model the relation between word alignment and the phrase extraction process, they introduced two new features into the log-linear model.",4.3 Unaligned Word Feature,
1139043417,false,finalized,3,1/24/2017 12:41:40,Neut,0.6867,W10-1719,"Motivated by the improvements in translation quality that could be achieved by using the n-gram based approach to statistical machine translation, for example by Allauzen et al (2009), we tried to integrate a bilingual language model into our phrase-based translation system.",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf,(2009),"To be able to integrate the approach easily into a standard phrase-based SMT system, a token in the bilingual language model is defined to consist of a target word and all source words it is aligned to.",4.4 Bilingual Word language model,
1139043418,false,finalized,3,1/24/2017 09:41:25,Pos,0.6776,W12-0108,"As a parser, any available tool may be used (the TreeTagger (Schmid, 1994) is used in the present implementation for English).",http://www.aclweb.org/anthology/W/W12/W12-0108.pdf,"(Schmid, 1994)","PAM processes a bilingual corpus of SL - TL sentence pairs, taking into account the parsing information in one language (in the current implementation the TL side) and making use of a bilingual lexicon and information on potential phrase heads; the output being the bilingual corpus aligned at word, phrase and clause level.",The phrases are assumed to be flat and linguistically valid.,
1139043419,false,finalized,3,1/24/2017 09:39:38,Neut,0.6815,W12-0108,Further PAM details are reported in Tambouratzis et al (2011).,http://www.aclweb.org/anthology/W/W12/W12-0108.pdf,(2011),"The PAM output in terms of SL phrases is then handed over to the Phrasing model generator (PMG), which is trained to determine the phrasal structure of an input sentence.","For instance, based on a sentence pair from the parallel corpus, the SL sentence with structure A-B-C-D is transformed into A'-C'-D'-B', where X is a phrase in SL and X' is a phrase in TL.",
1139043420,false,finalized,3,1/24/2017 10:20:53,Pos,1.0,W12-0108,"PMG is based on the Conditional Random Fields model (Lafferty et al., 1999) which has been found to provide the highest accuracy.",http://www.aclweb.org/anthology/W/W12/W12-0108.pdf,"(Lafferty et al., 1999)",The SL text segmented into phrases by PMG is then input to the 1st translation phase.,This phrasing model is then applied in segmenting any arbitrary SL text being input to the PRESEMT system for translation.,
1139043421,false,finalized,3,1/24/2017 09:22:48,Neut,0.6948,W12-0108,"The implementation is based on the SmithWaterman algorithm (Smith and Waterman, 1981), initially proposed for determining similar regions between two protein or DNA sequences.",http://www.aclweb.org/anthology/W/W12/W12-0108.pdf,"(Smith and Waterman, 1981)",The algorithm is guaranteed to find the optimal local alignment between the two input sequences at clause level.,"), the values of which are optimised by employing the optimisation module.",
1139043422,false,finalized,3,1/24/2017 11:07:20,Neut,1.0,W12-0108,"A matching algorithm selects the most similar from the set of the retrieved TL phrases through a comparison process, which is viewed as an assignment problem, using the Gale-Shapley algorithm (Gale and Shapley, 1962).",http://www.aclweb.org/anthology/W/W12/W12-0108.pdf,"(Gale and Shapley, 1962)",6.,"In the second task, the most similar phrases to the TL structure phrases are retrieved from the monolingual corpus to provide local structural information such as word-reordering.",
1139043423,false,finalized,3,1/24/2017 12:12:57,Neut,0.6716,W12-0108,"In the case of the language pair English-toGerman, these results are contrasted to the ones obtained when translating the same test set with Moses (Koehn et al., 2007).It is observed that for the English-to-German language pair, PRESEMT achieved approximately 50% of the MOSES BLEU score and 80% of the MOSES with respect to the Meteor and TER scores.",http://www.aclweb.org/anthology/W/W12/W12-0108.pdf,"(Koehn et al., 2007)",These are reasonably competitive results compared to an established system such as Moses.,"Table 1 illustrates an indicative set of results obtained by running automatic evaluation metrics on test data translated by the 1st PRESEMT prototype for a selection of language pairs, due to space restrictions.",
1139043424,false,finalized,3,1/24/2017 11:08:21,Neut,1.0,J03-3001,"1 Across the Atlantic, a resurgence in empiricism was led by the success of the noisy-channel model in speech recognition (see Church and Mercer [1993] for references).",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,[1993],334,"Arguments raged, and it was not clear whether corpus work was an acceptable",
1139043425,false,finalized,3,1/24/2017 09:02:07,Neut,1.0,J03-3001,"Rada Mihalcea and Dan Moldovan (1999) used hit counts for carefully constructed search engine queries to identify rank orders for word sense frequencies, as an input to a word sense disambiguation engine.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(1999),Philip Resnik (1999) showed that parallel corpora-until then a promising research avenue but largely constrained to the English-French Canadian Hansard-could be found on the Web: We can grow our own parallel corpus using the many Web pages that exist in parallel in local and in major languages.,The Web walked in on ACL meetings starting in 1999.,
1139043426,false,finalized,3,1/24/2017 11:44:25,Neut,1.0,J03-3001,Philip Resnik (1999) showed that parallel corpora-until then a promising research avenue but largely constrained to the English-French Canadian Hansard-could be found on the Web: We can grow our own parallel corpus using the many Web pages that exist in parallel in local and in major languages.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(1999),We are glad to have the further development of this work (co-authored by Noah Smith) presented in this special issue.,"Rada Mihalcea and Dan Moldovan (1999) used hit counts for carefully constructed search engine queries to identify rank orders for word sense frequencies, as an input to a word sense disambiguation engine.",
1139043427,false,finalized,3,1/24/2017 10:52:04,Neut,1.0,J03-3001,"In the student session of ACL 2000, Rosie Jones and Rayid Ghani (2001) showed how, using the Web, one can build a language-specific corpus from a single document in that language.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2001),"In the main session Atsushi Fujii and Tetsuya Ishikawa (2000) demonstrated that descriptive, definition-like collections can be acquired from the Web.",We are glad to have the further development of this work (co-authored by Noah Smith) presented in this special issue.,
1139043428,false,finalized,3,1/24/2017 11:53:02,Neut,1.0,J03-3001,"In the main session Atsushi Fujii and Tetsuya Ishikawa (2000) demonstrated that descriptive, definition-like collections can be acquired from the Web.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2000),2.1 Some Current Themes,"In the student session of ACL 2000, Rosie Jones and Rayid Ghani (2001) showed how, using the Web, one can build a language-specific corpus from a single document in that language.",
1139043429,false,finalized,3,1/24/2017 09:02:18,Pos,0.6657,J03-3001,"In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al (2003) ""balance"" their corpus using Web documents.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2001),The information retrieval community now has a Web track as a component of its TREC evaluation initiative.,The Web is being used to address data sparseness for language modeling.,
1139043430,false,finalized,3,1/24/2017 09:44:07,CoCo,0.6478,J03-3001,"In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al (2003) ""balance"" their corpus using Web documents.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2003),The information retrieval community now has a Web track as a component of its TREC evaluation initiative.,The Web is being used to address data sparseness for language modeling.,
1139043431,false,finalized,3,1/24/2017 11:26:49,Neut,1.0,J03-3001,"Varantola (2000) shows how translators can use ""just-in-time"" sublanguage corpora to choose correct target language terms for areas in which they are not expert.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2000),Fletcher (2002) demonstrates methods for gathering and using Web corpora in a language-teaching context.,2000) are exploring the automatic population of existing ontologies using the Web as a source for new instances.,
1139043432,false,finalized,3,1/24/2017 10:52:04,Neut,1.0,J03-3001,Fletcher (2002) demonstrates methods for gathering and using Web corpora in a language-teaching context.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2002),2.2 The 100M Words of the BNC,"Varantola (2000) shows how translators can use ""just-in-time"" sublanguage corpora to choose correct target language terms for areas in which they are not expert.",
1139043433,false,finalized,3,1/24/2017 11:50:33,Neut,1.0,J03-3001,Another argument is made vividly by Banko and Brill (2001).,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2001),They explore the performance of a number of machine learning algorithms (on a representative disambiguation task) as the size of the training corpus grows from a million to a billion words.,"They find that probabilistic models of language based on very large quantities of data, even if those data are noisy, are better than ones based on estimates (using sophisticated smoothing techniques) from smaller, cleaner data sets.",
1139043434,false,finalized,3,1/24/2017 10:28:29,Neut,1.0,J03-3001,"Lawrence and Giles (1999) compared the overlap between page lists returned by different Web browsers over the same set of queries and estimated that, in 1999, there were 800 million indexable Web pages available.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(1999),"By sampling pages, and estimating an average page length of seven to eight kilobytes of nonmarkup text, they concluded that there might be six terabytes of text available then.",Linguistic aspects take a little more work and can be estimated only by sampling and extrapolation.,
1139043435,false,finalized,3,1/24/2017 11:42:56,Neut,1.0,J03-3001,"Xu (2000) estimated that 71% of the pages (453 million out of 634 million Web pages indexed by the Excite engine at that time) were written in English, followed by Japanese (6.8%), German (5.1%), French (1.8%), Chinese (1.5%), Spanish (1.1%), Italian (0.9%), and Swedish (0.7%).",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2000),"We have measured the counts of some English phrases according to various search engines over time and compared them with counts in the BNC, which we know has 100 million words.",How much of it is English?,
1139043436,false,finalized,3,1/24/2017 11:34:28,Neut,1.0,J03-3001, AltaVista covers only a fraction of the indexable Web pages available (the fraction was estimated at just 15% by Lawrence and Giles [1999]).,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,[1999],AltaVista may be biased toward North American (mainly English-language) pages by the strategy it uses to crawl the Web.,"The numbers presented in Table 3 are lower bounds, for a number of reasons:",
1139043437,false,finalized,3,1/24/2017 09:18:33,Neut,1.0,J03-3001,"This hidden Web is vast (consider MedLine,8 just one such database, with more than five billion words; see also Ipeirotis, Gravano, and Sahami [2001]), and it is not considered at all in the AltaVista estimates.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,[2001],"Repeating the procedure after an interval, the second author and Nioche showed that the proportion of non-English text to English is growing.","AltaVista indexes only pages that can be directly called by a URL and does not index text found in databases that are accessible through dialog windows on Web pages (the ""hidden Web"").",
1139043438,false,finalized,3,1/24/2017 11:52:47,Neut,1.0,J03-3001,A set of controlled experiments of this form is described in Grefenstette (1999).,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(1999),"In Grefenstette's study, a good translation was found in 87% of ambiguous cases from German to English and 86% of ambiguous cases from Spanish to English.",The phrase work group is 15 times more frequent than any other and is also the best translation among the tested possibilities.,
1139043439,false,finalized,3,1/24/2017 10:18:53,Neut,1.0,J03-3001,"(These issues, and related themes of cut-and-paste authorship, ownership, and plagiarism, are explored in Wilks [2003].)",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,[2003],4.2 Technology,"In the text domain, organizations such as Reuters produce news feeds that are typically adapted to the style of a particular newspaper and then republished: Is each republication a new writing event?",
1139043440,false,finalized,3,1/24/2017 09:37:01,Neut,0.6583,J03-3001,"Two studies in this area are Sekine (1997) and Gildea (2001), both of which show substantial variation in model performance",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(1997),341,There is little work on assessing how well one language model fares when applied to a text type that is different from that of the training corpus.,
1139043441,false,finalized,3,1/24/2017 11:54:35,Neut,1.0,J03-3001,"Two studies in this area are Sekine (1997) and Gildea (2001), both of which show substantial variation in model performance",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2001),341,There is little work on assessing how well one language model fares when applied to a text type that is different from that of the training corpus.,
1139043442,false,finalized,3,1/24/2017 11:46:16,Neut,0.6966,J03-3001,"Atkins, Clear, and Ostler (1992) describe the desiderata and criteria used for the BNC, and this stands as a good model for a general-purpose, general-language corpus.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(1992),"The word representative has tended to fall out of discussions, to be replaced by the meeker balanced.","To date, corpus developers have been obliged to make pragmatic decisions about the sorts of text to go into a corpus.",
1139043443,false,finalized,3,1/24/2017 09:18:33,Pos,1.0,J03-3001,"The recent history of mathematically sophisticated modeling of language variation begins with Biber (1988), who identifies and quantifies the linguistic features associated with different spoken and written text types.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(1988),Habert and colleagues (Folch et al.,Kilgarriff and Grefenstette Web as Corpus: Introduction,
1139043444,false,finalized,3,1/24/2017 10:18:27,Pos,0.3497,J03-3001,"In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2001),"As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).","2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others.",
1139043445,false,finalized,3,1/24/2017 08:42:48,Neut,0.6741,J03-3001,"In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2002),"As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).","2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others.",
1139043446,false,finalized,3,1/24/2017 10:13:40,Pos,0.3583,J03-3001,"As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(1997),Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.,"In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.",
1139043447,false,finalized,3,1/24/2017 10:16:42,Neut,1.0,J03-3001,"As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2001),Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.,"In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.",
1139043448,false,finalized,3,1/24/2017 11:40:49,Neut,0.6472,J03-3001,"As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2000),Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.,"In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.",
1139043449,false,finalized,3,1/24/2017 11:41:25,Neut,1.0,J03-3001,Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2001),"A practical discussion of a central technical concern is Vossen (2001), which tailors a general-language resource for a domain.","As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).",
1139043450,false,finalized,3,1/24/2017 11:20:13,Neut,1.0,J03-3001,"A practical discussion of a central technical concern is Vossen (2001), which tailors a general-language resource for a domain.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2001),"Baayen (2001) presents sophisticated mathematical models for word frequency distributions, and it is likely that his mixture models have potential for modeling sublanguage mixtures.",Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.,
1139043451,false,finalized,3,1/24/2017 09:42:21,Neut,0.6852,J03-3001,"Baayen (2001) presents sophisticated mathematical models for word frequency distributions, and it is likely that his mixture models have potential for modeling sublanguage mixtures.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(2001),"His models have been developed with a specific, descriptive goal in mind and using a small number of short texts: It is unclear whether they can be usefully applied in NLP.","A practical discussion of a central technical concern is Vossen (2001), which tailors a general-language resource for a domain.",
1139043452,false,finalized,3,1/24/2017 11:15:27,Neut,1.0,J03-3001,"Exceptions focusing on genre include Kessler, Nunberg, and Schutze (1997) and Karlgren and Cutting (1994).",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(1997),4.7 Representativeness: Conclusion,"Also, the focus is usually on content words and topics or domains, with other differences of genre or sublanguage remaining unexamined.",
1139043453,false,finalized,3,1/24/2017 11:15:03,Neut,1.0,J03-3001,"Exceptions focusing on genre include Kessler, Nunberg, and Schutze (1997) and Karlgren and Cutting (1994).",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf,(1994),4.7 Representativeness: Conclusion,"Also, the focus is usually on content words and topics or domains, with other differences of genre or sublanguage remaining unexamined.",
1139043454,false,finalized,3,1/24/2017 10:28:10,Neut,1.0,N13-1057,"For example, (Gollins and Sanderson, 2001) use lexical triangulation to translate in parallel across multiple intermediate languages and",http://www.aclweb.org/anthology/N/N13/N13-1057.pdf,"(Gollins and Sanderson, 2001)",1http://www.ethnologue.com/ 2http://dsal.uchicago.edu/dictionaries/list.html,Existing algorithms for creating new bilingual dictionaries use intermediate languages or intermediate dictionaries to find chains of words with the same meaning.,
1139043455,false,finalized,3,1/24/2017 11:56:16,Neut,1.0,N13-1057,"Another existing approach for creating bilingual dictionaries is using probabilistic inference (Mausam et al., 2010).",http://www.aclweb.org/anthology/N/N13/N13-1057.pdf,"(Mausam et al., 2010)",They organize dictionaries in a graph topology and use random walks and probabilistic graph sampling.,"They use four pivot languages, German, Spanish, Dutch and Italian, as intermediate languages.",
1139043456,false,finalized,3,1/24/2017 11:43:11,Neut,1.0,N13-1057,"(Shaw et al., 2011) propose a set of algorithms to create a reverse dictionary in the context of single language by using converse mapping.",http://www.aclweb.org/anthology/N/N13/N13-1057.pdf,"(Shaw et al., 2011)","In particular, given an English-English dictionary, they attempt to find the original words or terms given a synonymous word or phrase describing the meaning of a word.",They organize dictionaries in a graph topology and use random walks and probabilistic graph sampling.,
1139043457,false,finalized,3,1/24/2017 11:44:32,Neut,1.0,N13-1057,"A LexicalUnit is a word or a phrase being defined, also called definiendum (Landau, 1984).",http://www.aclweb.org/anthology/N/N13/N13-1057.pdf,"(Landau, 1984)",A list of entries sorted by the LexicalUnit is called a lexicon or a dictionary.,"A dictionary entry, called LexicalEntry, is a 2-tuple <LexicalUnit, Definition>.",
1139043458,false,finalized,3,1/24/2017 11:09:22,Neut,0.6599,N13-1057,"We require that at least one of two these languages has a Wordnet type lexical ontology (Miller, 1995).",http://www.aclweb.org/anthology/N/N13/N13-1057.pdf,"(Miller, 1995)",Our algorithms are used to create reverse dictionaries from them at various levels of accuracy and sophistication.,"In this section, we propose a series of algorithms, each one of which automatically creates a reverse dictionary, or ReverseDictionary, from a dictionary that translates a word in language L1 to a word or phrase in language L2.",
1139043459,false,finalized,3,1/24/2017 11:34:25,Neut,1.0,N13-1057,"The distance between each phrase pair is the average of the total distances between every word pair in the phrases (Wu and Palmer, 1994).",http://www.aclweb.org/anthology/N/N13/N13-1057.pdf,"(Wu and Palmer, 1994)","If the distance between two words or phrases is 1.00, there is no similarity between these words or phrases, but if they have the same meaning, the distance is 0.00.","The distance between the two LexicalEntrys is the distance between the two LexicalUnits if the LexicalUnits occur in Wordnet ontology; otherwise, it is the distance between the two Senses.",
1139043460,false,finalized,3,1/24/2017 10:16:42,Neut,0.661,W11-2921,"The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n') time.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(Cocke and Schwartz, 1970","While often ignored, the grammar constant |G |typically dominates the runtime in practice.","Most syntactic constituency parsers employ a weighted context-free grammar (CFG), that is learned from a treebank.",
1139043461,false,finalized,3,1/24/2017 09:37:01,Pos,0.6939,W11-2921,"The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n') time.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Kasami, 1965","While often ignored, the grammar constant |G |typically dominates the runtime in practice.","Most syntactic constituency parsers employ a weighted context-free grammar (CFG), that is learned from a treebank.",
1139043462,false,finalized,3,1/24/2017 11:53:59,Neut,0.6677,W11-2921,"The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n') time.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Younger, 1967)","While often ignored, the grammar constant |G |typically dominates the runtime in practice.","Most syntactic constituency parsers employ a weighted context-free grammar (CFG), that is learned from a treebank.",
1139043463,false,finalized,3,1/24/2017 11:25:39,Neut,1.0,W11-2921,"This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(Collins, 1999","Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006).","While often ignored, the grammar constant |G |typically dominates the runtime in practice.",
1139043464,false,finalized,3,1/24/2017 11:15:27,Neut,1.0,W11-2921,"This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Charniak, 2000","Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006).","While often ignored, the grammar constant |G |typically dominates the runtime in practice.",
1139043465,false,finalized,3,1/24/2017 11:43:11,Neut,1.0,W11-2921,"This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Petrov et al., 2006)","Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006).","While often ignored, the grammar constant |G |typically dominates the runtime in practice.",
1139043466,false,finalized,3,1/24/2017 11:05:48,Neut,0.6599,W11-2921,"Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(Asanovic et al., 2006)",This opens up new opportunities for increasing the speed of parsers.,"This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.",
1139043467,false,finalized,3,1/24/2017 11:39:52,Neut,1.0,W11-2921,We make no assumptions about the size of the grammar and we demonstrate the efficacy of our approach by implementing a decoder for the state-of-the-art latent variable grammars of Petrov et al (2006) (a.k.a.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,(2006),Berkeley Parser) on a Graphics Processor Unit (GPU).,We present a general approach for parallelizing the CKY algorithm that can handle arbitrary context-free grammars (Section 2).,
1139043468,false,finalized,3,1/24/2017 12:12:57,Neut,1.0,W11-2921,"We first present an overview of the general architecture of GPUs and the efficient synchronization provided by the Compute Unified Device Architecture (CUDA (Nickolls et al., 2008)) programming model (Section 3).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(Nickolls et al., 2008)",We then discuss how the hundreds of cores available on a GPU can enable a fine-grained parallel execution of the CKY algorithm.,Berkeley Parser) on a Graphics Processor Unit (GPU).,
1139043469,false,finalized,3,1/24/2017 11:00:54,Neut,0.6599,W11-2921,"In our experiments we will use a probabilistic latent variable CFG (Petrov et al., 2006).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(Petrov et al., 2006)","However, our algorithms can be used with any weighted CFG, including discriminative ones, such as the ones in Petrov and Klein (2007a) and",In this work we focus our attention on constituency parsing and assume that a weighted CFG is available to us.,
1139043470,false,finalized,3,1/24/2017 11:39:37,Neut,1.0,W11-2921,"However, our algorithms can be used with any weighted CFG, including discriminative ones, such as the ones in Petrov and Klein (2007a) and",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,(2007a),"Figure 2: The chart that visualizes the bottom-up process of CKY parsing for the sentence ""I love you .""","In our experiments we will use a probabilistic latent variable CFG (Petrov et al., 2006).",
1139043471,false,finalized,3,1/24/2017 11:47:00,Neut,1.0,W11-2921,Finkel et al (2008).1 The grammars in our experiments have on the order of thousands of nonterminals and millions of productions.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,(2008),Figure 1(a) shows a constituency parse tree.,"Figure 2: The chart that visualizes the bottom-up process of CKY parsing for the sentence ""I love you .""",
1139043472,false,finalized,3,1/24/2017 09:22:53,Neut,0.3522,W11-2921,Using the lazy evaluation algorithm of Huang and Chiang (2005) the extrac,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,(2005),1For feature-rich discriminative models a trivially parallelizable pass can be used to pre-compute the rule-potentials.,"We found this to be more efficient than keeping backpointers.2 One should also note that many real-world applications benefit from, or even expect n-best lists of possible parse trees.",
1139043473,false,finalized,3,1/24/2017 11:25:20,Neut,1.0,W11-2921,"In order to increase the efficiency by exploiting this parallelism, typical GPUs (Lindholm et al., 2008) have hundreds of processing cores.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(Lindholm et al., 2008)","For example, the NVIDIA GTX480 GPU has 480 processing cores called stream processors (SP).","Graphics Processor Units (GPUs) were originally designed for processing graphics applications, where millions of operations can be executed in parallel.",
1139043474,false,finalized,3,1/24/2017 10:53:55,Neut,1.0,W11-2921,"Recently, Nickolls et al (2008) introduced the Compute Unified Device Architecture (CUDA).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,(2008),It allows programmers to utilize GPUs to accelerate applications in domains other than graphics.,3.1 Compute Unified Device Architecture,
1139043475,false,finalized,3,1/24/2017 11:12:23,Neut,1.0,W11-2921,"Thus, to achieve good performance, it is important to increase the ratio of Compute to Global Memory Access (CGMA) (Kirk and Hwu, 2010), which can be done in part by cleverly utilizing the different types of shared onchip memory in each SM.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(Kirk and Hwu, 2010)",Threads in a thread block are mapped onto the same SM and can cooperate with one another by sharing data through the on-chip shared memory of the SM (shown in Figure 5).,"Generally speaking, manycore architectures (like GPUs) have more ALUs in place of on-chip caches, making arithmetic operations relatively cheaper and global memory accesses relatively more expensive.",
1139043476,false,finalized,3,1/24/2017 09:17:42,Neut,0.6486,W11-2921,The grammar we used is extracted from the publicly available parser of Petrov et al (2006).,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,(2006),"It has 1,120 nonterminal symbols including 636 preterminal symbols.",The detailed specifications of the experimental platforms are listed in Table 1.,
1139043477,false,finalized,3,1/24/2017 10:13:40,Neut,0.6417,W11-2921,"We used the first 1000 sentences from Section 22 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993) as our benchmark set.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(Marcus et al., 1993)",We verified for each sentence that our parallel implementation obtains exactly the same parse tree and score as the sequential implementation.,"The grammar has 852,591 binary rules and 114,419 unary rules.",
1139043478,false,finalized,3,1/24/2017 09:41:25,CoCo,1.0,W11-2921,This is comparable to the better implementations presented in Dunlop et al (2011).,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,(2011),"As can be seen in Figure 11, the fastest configuration on the GTX285 is Block+PR+SS+tex:scores, which shows a 17.4x speedup against the sequential parser.","The exhaustive sequential CKY parser was written in C and is reasonably optimized, taking 5.5 seconds per sentence (or 5,505 seconds for the 1000 benchmark sentences).",
1139043479,false,finalized,3,1/24/2017 09:08:36,Neut,0.6735,W11-2921,"The GTX480 is the Fermi architecture (NVIDIA, 2009), with many features added to the GTX285.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(NVIDIA, 2009)","The number of cores doubled from 240 to 480, but the number of SMs was halved from 30 to 15.","However, placing the rule information in texture memory improves the performance little as there are many more accesses to the scores array than to the rule information.",
1139043480,false,finalized,3,1/24/2017 10:18:27,Neut,1.0,W11-2921,"A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(van Lohuizen, 1999","However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization.",6 Related Work,
1139043481,false,finalized,3,1/24/2017 10:16:42,Neut,1.0,W11-2921,"A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Giachin and Rullent, 1989","However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization.",6 Related Work,
1139043482,false,finalized,3,1/24/2017 09:16:33,CoCo,0.3389,W11-2921,"A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Pontelli et al., 1998","However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization.",6 Related Work,
1139043483,false,finalized,3,1/24/2017 11:51:57,Neut,1.0,W11-2921,"A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Manousopoulou et al., 1997)","However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization.",6 Related Work,
1139043484,false,finalized,3,1/24/2017 12:02:14,Neut,0.6998,W11-2921,"For example, van Lohuizen (1999) reports a 1.8x speedup, while Manousopoulou et al (1997) claims a 7-8x speedup.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,(1999),"In contrast, our parallel parser is implemented on a manycore system with an abundant number of threads and pro","The parallel parsers in past work are implemented on multicore systems, where the limited parallelization possibilities provided by the systems restrict the speedups that can be achieved.",
1139043485,false,finalized,3,1/24/2017 12:02:14,CoCo,0.6477,W11-2921,"For example, van Lohuizen (1999) reports a 1.8x speedup, while Manousopoulou et al (1997) claims a 7-8x speedup.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,(1997),"In contrast, our parallel parser is implemented on a manycore system with an abundant number of threads and pro","The parallel parsers in past work are implemented on multicore systems, where the limited parallelization possibilities provided by the systems restrict the speedups that can be achieved.",
1139043486,false,finalized,3,1/24/2017 11:31:35,Neut,1.0,W11-2921,"Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(van Lohuizen, 1999",Agenda-based parsers maintain a queue of prioritized intermediate results and iteratively refine and combine these until the whole sentence is processed.,We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.,
1139043487,false,finalized,3,1/24/2017 10:59:22,Neut,1.0,W11-2921,"Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Giachin and Rullent, 1989",Agenda-based parsers maintain a queue of prioritized intermediate results and iteratively refine and combine these until the whole sentence is processed.,We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.,
1139043488,false,finalized,3,1/24/2017 11:50:36,Neut,0.6722,W11-2921,"Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Pontelli et al., 1998",Agenda-based parsers maintain a queue of prioritized intermediate results and iteratively refine and combine these until the whole sentence is processed.,We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.,
1139043489,false,finalized,3,1/24/2017 11:40:11,Neut,0.6905,W11-2921,"Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Manousopoulou et al., 1997)",Agenda-based parsers maintain a queue of prioritized intermediate results and iteratively refine and combine these until the whole sentence is processed.,We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.,
1139043490,false,finalized,3,1/24/2017 11:52:47,Neut,1.0,W11-2921,Bordim et al (2002) present a CKY parser that is implemented on a field-programmable gate array (FPGA) and report a speedup of up to 750x.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,(2002),"However, this hardware approach suffers from insufficient memory or logic elements and limits the number of rules in the grammar to 2,048 and the number of non-terminal symbols.",Chart-based parsing on the other hand allows us to expose and exploit the abundant parallelism of the dynamic program.,
1139043491,false,finalized,3,1/24/2017 10:29:04,Neut,1.0,W11-2921,"Ninomiya et al (1997) propose a parallel CKY parser on a distributed-memory parallel machine consisting of 256 nodes, where each node contains a single processor.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,(1997),"Using their parallel language, they parallelize over cells in the chart, assigning each chart cell to each node in the machine.","Their approach thus cannot be applied to real-world, state-of-theart grammars.",
1139043492,false,finalized,3,1/24/2017 10:32:39,Neut,0.6822,W11-2921,"Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(Goodman, 1997",These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest.,It should be noted that there are a also number of orthogonal approaches for accelerating natural language parsers.,
1139043493,false,finalized,3,1/24/2017 10:17:49,Neut,0.689,W11-2921,"Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Charniak and Johnson, 2005",These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest.,It should be noted that there are a also number of orthogonal approaches for accelerating natural language parsers.,
1139043494,false,finalized,3,1/24/2017 11:36:02,Neut,1.0,W11-2921,"Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Petrov and Klein, 2007b)",These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest.,It should be noted that there are a also number of orthogonal approaches for accelerating natural language parsers.,
1139043495,false,finalized,3,1/24/2017 12:26:18,Neut,1.0,W11-2921,"There are also some optimality preserving approaches based on A*-search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(Klein and Manning, 2003","We suspect that most of the ideas therein are orthogonal to our approach, and therefore leave their integration into our GPU-based parser for future work.",Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements.,
1139043496,false,finalized,3,1/24/2017 09:15:03,Neut,0.6503,W11-2921,"There are also some optimality preserving approaches based on A*-search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"Pauls and Klein, 2009)","We suspect that most of the ideas therein are orthogonal to our approach, and therefore leave their integration into our GPU-based parser for future work.",Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements.,
1139043497,false,finalized,3,1/24/2017 11:27:35,Neut,1.0,W11-2921,"There are also some optimality preserving approaches based on A*-search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf,"(Dunlop et al., 2011)","We suspect that most of the ideas therein are orthogonal to our approach, and therefore leave their integration into our GPU-based parser for future work.",Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements.,
1139043498,false,finalized,3,1/24/2017 10:32:04,Neut,1.0,W13-5507,"In recent years, many linguistic resources have been released as Linked Data (Chiarcos et al., 2011).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Chiarcos et al., 2011)","Most of the datasets that are part of the so called Linguistic Linked Open Data (LLOD) cloud consist of dictionaries, written corpora or lexica.",1 Introduction,
1139043499,false,finalized,3,1/24/2017 10:20:20,Pos,0.6607,W13-5507,"In order to express both established and new data categories and properties, from linguistics as well as from nonlinguistic communication, we developed a new data category registry, which contains links to other resources in the LLOD cloud, in particular to the ISOcat data category repository (Windhouwer and Wright, 2012), but also serves as a place where categories from novel research fields (mainly multimodal communication) can be collected, discussed, until they have settled down and are stable enough for an integration into more authoritative category registries, such as ISOcat.",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Windhouwer and Wright, 2012)",By means of this we aim to make the re,"Due to the challenging nature of the data, in particular that it contains annotations on multiple timelines, we developed a new model for the representation of this data, which we call FiESTA.",
1139043500,false,finalized,3,1/24/2017 09:22:48,Pos,0.6593,W13-5507,"Furthermore, we describe a software toolchain for easy extraction of RDF data from existing information structures, such as classes or database records, and delivery of this data via web applications and services based on the popular framework Rails (Ruby et al., 2011).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Ruby et al., 2011)","This tool chain is designed to be easy to integrate with existing libraries in a plugin-like fashion, in order to reduce the effort of integrating existing systems into Linked Data networks and infrastructures.",source more widely available and to enable a long and successful lifecycle for the resource.,
1139043501,false,finalized,3,1/24/2017 08:43:33,Neut,1.0,W13-5507,"We use these terms in Lehmann's reading: ""Primary linguistic data are [...] representations of [...] speech events with their spatio-temporal coordinates"" (Lehmann, 2005, p. 187).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Lehmann, 2005, ","However, his distinction between raw (=non-symbolic) and processed (=symbolic) data (Lehmann, 2005, pp.","Based on this automatically generated data, several annotations have been created: 1Terms like primary and secondary data are problematic when we go beyond classical face-to-face dialogues preserved in audio and video recordings.",
1139043502,false,finalized,3,1/24/2017 11:44:25,Neut,1.0,W13-5507,"However, his distinction between raw (=non-symbolic) and processed (=symbolic) data (Lehmann, 2005, pp.",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Lehmann, 2005, ",205ff.),"We use these terms in Lehmann's reading: ""Primary linguistic data are [...] representations of [...] speech events with their spatio-temporal coordinates"" (Lehmann, 2005, p. 187).",
1139043503,false,finalized,3,1/24/2017 10:44:35,Pos,0.3497,W13-5507,"Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Klein and Manning, 2002",3.,2.,
1139043504,false,finalized,3,1/24/2017 11:47:00,Neut,1.0,W13-5507,"Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"Klein and Manning, 2003)",3.,2.,
1139043505,false,finalized,3,1/24/2017 11:48:42,Neut,0.6649,W13-5507,"Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Rafferty and Manning, 2008)",3.,2.,
1139043506,false,finalized,3,1/24/2017 11:42:34,Pos,0.3528,W13-5507,"We developed FiESTA (an acronym for ""format for extensive spatiotemporal annotations""), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Bird and Liberman, 2001)",There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.,3.1 Internal representation,
1139043507,false,finalized,3,1/24/2017 11:34:25,Neut,0.6588,W13-5507,"We developed FiESTA (an acronym for ""format for extensive spatiotemporal annotations""), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Evert et al., 2003)",There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.,3.1 Internal representation,
1139043508,false,finalized,3,1/24/2017 09:41:07,Pos,0.6583,W13-5507,"We developed FiESTA (an acronym for ""format for extensive spatiotemporal annotations""), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(TEI Consortium, 2008)",There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.,3.1 Internal representation,
1139043509,false,finalized,3,1/24/2017 11:50:36,Pos,0.6579,W13-5507,"We developed FiESTA (an acronym for ""format for extensive spatiotemporal annotations""), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Ide et al., 2000)",There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.,3.1 Internal representation,
1139043510,false,finalized,3,1/24/2017 11:56:16,Neut,0.6545,W13-5507,"These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Chiarcos, 2012)","One of the most pressing problems is the restriction to a single, flat stream or sequence ofprimary data (called ""text"" in some approaches), or a single, flat timeline.",There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.,
1139043511,false,finalized,3,1/24/2017 10:20:53,Pos,0.3488,W13-5507,"These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Ide et al., 2003)","One of the most pressing problems is the restriction to a single, flat stream or sequence ofprimary data (called ""text"" in some approaches), or a single, flat timeline.",There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.,
1139043512,false,finalized,3,1/24/2017 11:31:10,Weak,0.672,W13-5507,"For instance, the annotation tool EXMARaLDA provides a mechanism for creating time forks (Schmidt and Worner, 2005), but this is useful only for shorter stretches of simultaneous events surrounded by synchronised time points (e. g., for shorter segments of simultaneous speech), and not for timelines that might be completely independent from each other in the beginning and need to be merged and aligned later.",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Schmidt and Worner, 2005)","Also, there are various potential reasons in a scientific workflow that call for the use of an annotation tool different from EXMARaLDA.","With most of the given models, such an undertaking is either impossible, or it involves the alienation of model components (e.g., creation of phantom annotations being used as fake time points), which both inflates the resulting data structure and makes it less comprehensible.",
1139043513,false,finalized,3,1/24/2017 11:16:08,Neut,1.0,W13-5507,"and a different level of measurement, following (Stevens, 1946).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Stevens, 1946)","Scales can be left independent, or a synchronisation betweeen them can be expressed (e. g., a linear transformation between a video-frame-based scale and a millisecond-based one, or a manual alignment using explicit alignment points).",Figure 4: UML class diagram (simplified) of the FiESTA data model.,
1139043514,false,finalized,3,1/24/2017 09:58:46,Pos,0.6836,W13-5507,"We added some convenience methods for easy linking to some vocabularies or concept registries, among them, ISOcat (Windhouwer and Wright, 2012), XML Schema, Dublin Core, FOAF, and others.",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf,"(Windhouwer and Wright, 2012)","At the moment, the ontology describing the FiESTA data model (cf.","A category consists of (1) an identifier (which automatically is suffixed to the ontology URI to create an URI for the category), (2) a humanreadable label, (3) a human-readable definition (typically consisting of one or two sentences), (4) information about the class hierarchy, (S) information about possible domains and ranges, and (6) a number of relations, which express equivalence and similarity relations to other categories already existing outside the system (using appropriate vocabulary, such as rdfs:seeAlso or owl:sameAs).",
1139043515,false,finalized,3,1/24/2017 11:46:16,Neut,1.0,P93-1003," Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Kay and Roscheisen, 1988, ","Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].",Areas of investigation using bilingual corpora have included the following:,
1139043516,false,finalized,3,1/24/2017 08:44:30,Neut,1.0,P93-1003," Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Brown et al., 1991a, ","Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].",Areas of investigation using bilingual corpora have included the following:,
1139043517,false,finalized,3,1/24/2017 11:45:00,Neut,1.0,P93-1003," Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Gale and Church, 1991b]","Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].",Areas of investigation using bilingual corpora have included the following:,
1139043518,false,finalized,3,1/24/2017 11:32:37,Neut,1.0,P93-1003,"Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Dagan et al., 1991, ","Extracting word correspondences [Gale and Church, 1991a]."," Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].",
1139043519,false,finalized,3,1/24/2017 09:15:03,Neut,1.0,P93-1003,"Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Brown et al., 1991b, ","Extracting word correspondences [Gale and Church, 1991a]."," Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].",
1139043520,false,finalized,3,1/24/2017 09:30:28,Neut,1.0,P93-1003,"Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Church and Gale, 1991]","Extracting word correspondences [Gale and Church, 1991a]."," Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].",
1139043521,false,finalized,3,1/24/2017 09:30:28,Neut,1.0,P93-1003,"Extracting word correspondences [Gale and Church, 1991a].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Gale and Church, 1991a]","Finding bilingual collocations [Smadja, 1992].","Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].",
1139043522,false,finalized,3,1/24/2017 10:18:53,Neut,1.0,P93-1003,"Finding bilingual collocations [Smadja, 1992].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Smadja, 1992]","Estimating parameters for statistically-based machine translation [Brown et al., 1992].","Extracting word correspondences [Gale and Church, 1991a].",
1139043523,false,finalized,3,1/24/2017 11:27:42,Neut,1.0,P93-1003,"Estimating parameters for statistically-based machine translation [Brown et al., 1992].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Brown et al., 1992]","The work described here makes use of the aligned Canadian Hansards [Gale and Church, 1991b] to obtain noun phrase correspondences between the English and French text.","Finding bilingual collocations [Smadja, 1992].",
1139043524,false,finalized,3,1/24/2017 09:30:33,Pos,1.0,P93-1003,"The work described here makes use of the aligned Canadian Hansards [Gale and Church, 1991b] to obtain noun phrase correspondences between the English and French text.",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Gale and Church, 1991b]","The term ""correspondence"" is used here to signify a mapping between words in two aligned sentences.","Estimating parameters for statistically-based machine translation [Brown et al., 1992].",
1139043525,false,finalized,3,1/24/2017 08:41:35,Neut,1.0,P93-1003,"Single word correspondences have been investigated [Gale and Church, 1991a] using a statistic operating on contingency tables.",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Gale and Church, 1991a]","An algorithm for producing collocational correspondences has also been described [Smadja, 1992].",A word sequence in Ei is defined here as the correspondence of another sequence in Fi if the words of one sequence are considered to represent the words in the other.,
1139043526,false,finalized,3,1/24/2017 11:34:28,Neut,1.0,P93-1003,"An algorithm for producing collocational correspondences has also been described [Smadja, 1992].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Smadja, 1992]",The algorithm involves several steps.,"Single word correspondences have been investigated [Gale and Church, 1991a] using a statistic operating on contingency tables.",
1139043527,false,finalized,3,1/24/2017 10:28:48,Pos,0.3504,P93-1003,"The taggers are robust and operate with a low error rate [Kupiec, 19921.",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Kupiec, 1992",Simple noun phrases (excluding pronouns and digits) are then extracted from the sentences by finite-state recognizers that are specified by regular expressions defined in terms of part-ofspeech categories.,"Each tagger contains a hidden Markov model (HMM), which is trained using samples of raw text from the Hansards for each language.",
1139043528,false,finalized,3,1/24/2017 09:45:36,Pos,0.6486,P93-1003,"The algorithm described here is an instance of a general approach to statistical estimation, represented by the EM algorithm [Dempster et al., 1977].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Dempster et al., 1977]","In contrast to reservations that have been expressed [Gale and Church, 1991a] about using the EM algorithm to provide word correspondences, there have been no indications that prohibitive amounts of memory might be required, or that the approach lacks robustness.",An arbitrarily large corpus can be accommodated by segmenting it appropriately.,
1139043529,false,finalized,3,1/24/2017 10:32:39,Neut,0.6427,P93-1003,"In contrast to reservations that have been expressed [Gale and Church, 1991a] about using the EM algorithm to provide word correspondences, there have been no indications that prohibitive amounts of memory might be required, or that the approach lacks robustness.",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Gale and Church, 1991a]","Unlike the other methods that have been mentioned, the approach has the capability to accommodate more context to improve performance.","The algorithm described here is an instance of a general approach to statistical estimation, represented by the EM algorithm [Dempster et al., 1977].",
1139043530,false,finalized,3,1/24/2017 11:53:02,Neut,1.0,P93-1003,"[Cutting et al., 1992]).",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf,"Cutting et al., 1992]",CONCLUSION,"This situation is very similar to that involved in training HMM text taggers, where joint probabilities are computed that a particular word corresponds to a particular part-ofspeech, and the rest of the words in the sentence are also generated (e.g.",
1139043531,false,finalized,3,1/24/2017 09:46:51,Pos,1.0,W10-2912,In this work we develop a segmentation model from the constraints suggested by Yang (2004) and evaluate it in idealized conditions and conditions that better approximate the environment of a child learner.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,(2004),We seek to determine how these limitations in the learner's input and memory affect the learner's performance and to demonstrate that the presented learner is robust even under non-ideal conditions.,"Computational models present an opportunity to test the potentially innate constraints, structures, and algorithms that a child may be using to guide her acquisition.",
1139043532,false,finalized,3,1/24/2017 11:43:08,Neut,0.6876,W10-2912,"Most recent work in word segmentation of childdirected speech has operated within statistical optimization frameworks, particularly Bayesian approaches (Goldwater et al., 2009; Johnson and Goldwater, 2009).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Goldwater et al., 2009",These models have established the state-of-the-art for the task of selecting appropriate word boundaries from a stream of unstructured phonemes.,2 Related Work,
1139043533,false,finalized,3,1/24/2017 12:28:21,Neut,1.0,W10-2912,"Most recent work in word segmentation of childdirected speech has operated within statistical optimization frameworks, particularly Bayesian approaches (Goldwater et al., 2009; Johnson and Goldwater, 2009).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"Johnson and Goldwater, 2009)",These models have established the state-of-the-art for the task of selecting appropriate word boundaries from a stream of unstructured phonemes.,2 Related Work,
1139043534,false,finalized,3,1/24/2017 11:08:21,Neut,1.0,W10-2912,"Children are incremental learners (Brown, 1973), and learners relying on statistical optimization are generally not.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Brown, 1973)","A child's competence grows gradually as she hears and produces more and more utterances, going through predictable changes to her working grammar (Marcus et al., 1992) that statistical optimization techniques typically do not go through and do not intend to replicate.",Trying to find cognitive insight from these types of models is difficult because of the inherent mismatch in the quality and types of hypotheses they maintain during learning.,
1139043535,false,finalized,3,1/24/2017 11:33:16,Neut,1.0,W10-2912,"A child's competence grows gradually as she hears and produces more and more utterances, going through predictable changes to her working grammar (Marcus et al., 1992) that statistical optimization techniques typically do not go through and do not intend to replicate.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Marcus et al., 1992)","Statistical models provide excellent information about the features, distributional cues, and priors that can be used in learning, but provide little information about how a child learner can use this information and how her knowledge of language develops as the learning process evolves.","Children are incremental learners (Brown, 1973), and learners relying on statistical optimization are generally not.",
1139043536,false,finalized,3,1/24/2017 12:28:21,Neut,1.0,W10-2912,"Previous simulations in word segmentation using the same type of distributional information as many statistical optimization-based learners but without an optimization model suggest that statistics alone are not sufficient for learning to succeed in a computationally efficient online manner; further constraints on the search space are needed (Yang, 2004).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Yang, 2004)",Previous computational models have demanded tremendous memory and computational capacity from human learners.,"Statistical models provide excellent information about the features, distributional cues, and priors that can be used in learning, but provide little information about how a child learner can use this information and how her knowledge of language develops as the learning process evolves.",
1139043537,false,finalized,3,1/24/2017 09:12:49,Pos,0.6444,W10-2912,"of Brent & Cartwright (1996) produces a set of possible lexicons that describe the learning corpus, each of which is evaluated as the learner iterates until no further improvement is possible.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,(1996),It is unlikely that an algorithm of this type is something a human learner is capable of using given the requirement to remember at the very least a long history of recent utterances encountered and constantly reanalyze them to find a optimal segmentation.,"Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 88-97, Uppsala, Sweden, 15-16 July 2010. c2010 Association for Computational Linguistics",
1139043538,false,finalized,3,1/24/2017 11:39:52,Neut,1.0,W10-2912,"Most of these models are ""synthetic"" in the sense of Brent (1999): the raw material for segmentation is a stream of segments, which are then successively grouped into larger units and eventually, conjectured words.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,(1999),"This assumption may make the child learner's job unnecessarily hard; since syllables are hierarchical structures consisting of segments, treating the linguistic data as unstructured segment sequences makes the problem harder than it actually is.","On the other hand, previous computational models often underestimate the human learner's knowledge of linguistic representations.",
1139043539,false,finalized,3,1/24/2017 09:53:49,Neut,0.6815,W10-2912,"provide the learner the most flexibility, allowing the learner to build structure from the individual phonemes and apply distributions at each level of abstraction (Johnson and Goldwater, 2009).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Johnson and Goldwater, 2009)","While this results in state-of-the-art performance for segmentation performed at the phoneme level, this approach requires significant computational resources as each additional level of representation increases the complexity of learning.","Modeling the corpus using hierarchical grammars that can model the input at varying levels (word collocations, words, syllables, onsets, etc.)",
1139043540,false,finalized,3,1/24/2017 11:15:27,Neut,1.0,W10-2912,"A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Saffran et al., 1996a","The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.","In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner.",
1139043541,false,finalized,3,1/24/2017 10:20:53,Neut,1.0,W10-2912,"A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"Saffran et al., 1996b)","The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.","In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner.",
1139043542,false,finalized,3,1/24/2017 11:44:32,Neut,1.0,W10-2912,"A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Swingley, 2005)","The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.","In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner.",
1139043543,false,finalized,3,1/24/2017 12:02:14,Neut,0.3473,W10-2912,"The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Bijeljac-Babic et al., 1993)","Precision is 41.6%, and recall is 23.3%, which we will show is worse than a simple baseline of assuming every syllable is a word.","A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).",
1139043544,false,finalized,3,1/24/2017 11:39:37,Neut,0.6905,W10-2912,"The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Yang, 2004)","Precision is 41.6%, and recall is 23.3%, which we will show is worse than a simple baseline of assuming every syllable is a word.","A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).",
1139043545,false,finalized,3,1/24/2017 10:29:04,Neut,1.0,W10-2912,"Corpus statistics reveal that on average a monosyllabic word is followed by another monosyllabic word 85% of time (Yang, 2004), and thus learners that use only local transitional probabilities without any global optimization are unlikely to succeed.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Yang, 2004)","This problem does not affect online approaches that use global information, such as computing the maximum likelihood of the corpus incrementally (Venkataraman, 2001).","While the pseudo-words used in infant studies measuring the ability to use transitional probability information are uniformly three-syllables long, much of child-directed English consists of sequences of monosyllabic words.",
1139043546,false,finalized,3,1/24/2017 11:36:43,Neut,1.0,W10-2912,"This problem does not affect online approaches that use global information, such as computing the maximum likelihood of the corpus incrementally (Venkataraman, 2001).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Venkataraman, 2001)","Since these approaches do not require each boundary be a local minimum, they are able to correctly handle a sequence of monosyllable words.","Corpus statistics reveal that on average a monosyllabic word is followed by another monosyllabic word 85% of time (Yang, 2004), and thus learners that use only local transitional probabilities without any global optimization are unlikely to succeed.",
1139043547,false,finalized,3,1/24/2017 11:43:08,Neut,1.0,W10-2912,"Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Gold, 1967","If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand.",3 Constraining the Learning Space,
1139043548,false,finalized,3,1/24/2017 08:44:30,Neut,0.6741,W10-2912,"Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"Valiant, 1984","If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand.",3 Constraining the Learning Space,
1139043549,false,finalized,3,1/24/2017 08:46:56,Neut,0.6779,W10-2912,"Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"Vapnik, 2000)","If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand.",3 Constraining the Learning Space,
1139043550,false,finalized,3,1/24/2017 11:08:21,Neut,1.0,W10-2912,"A particularly useful constraint for word segmentation, introduced to the problem of word segmentation by Yang (2004) but previously discussed by Halle and Vergnaud (1987), is as follows: Unique Stress Constraint (USC): A word can bear at most one primary stress.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,(2004),A simple example of how adult learners might use the USC is upon hearing novel names or words.,"It is important to identify such constraints to see to what extent they complement, or even replace, domain neutral learning mechanisms.",
1139043551,false,finalized,3,1/24/2017 11:52:47,Neut,1.0,W10-2912,"A particularly useful constraint for word segmentation, introduced to the problem of word segmentation by Yang (2004) but previously discussed by Halle and Vergnaud (1987), is as follows: Unique Stress Constraint (USC): A word can bear at most one primary stress.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,(1987),A simple example of how adult learners might use the USC is upon hearing novel names or words.,"It is important to identify such constraints to see to what extent they complement, or even replace, domain neutral learning mechanisms.",
1139043552,false,finalized,3,1/24/2017 11:07:20,Neut,1.0,W10-2912,Yang (2004) evaluates the effectiveness of the USC in conjunction with a simple approach to using transitional probabilities.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,(2004),The performance of the approach presented there improves dramatically if the learner is equipped with the assumption that each word can have only one primary stress.,"Moreover, it also knows that in the window between S1 and S2 there must be one or more word boundaries.",
1139043553,false,finalized,3,1/24/2017 09:53:01,Neut,0.6503,W10-2912,"This assumption is plausible given that 7.5-month-old infants are sensitive to strong/weak prosodic distinctions (Jusczyk, 1999).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Jusczyk, 1999)","Yang's stress-delimited algorithm achieves the precision of 73.5% and recall of 71.2%, a significant improvement over using TPs alone, but still below the baseline presented in our results.","If the learner knows this, then it may limit the search for local minima to only the window between two syllables that both bear primary stress, e.g., between the two a's in the sequence languageacquisition.",
1139043554,false,finalized,3,1/24/2017 09:46:51,Neut,0.6939,W10-2912,"Stress is better organized into hierarchical patterns constructed on top of syllables that vary in relative prominence based on the domain of each level of the hierarchy, and generally languages avoid adjacent strong syllables (Liberman and Prince, 1977).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Liberman and Prince, 1977)",We later discuss a manipulation of the corpus used by Yang (2004) to address this concern.,"It should be noted that the classification of every syllable as ""weak"" or ""strong"" is a significant simplification.",
1139043555,false,finalized,3,1/24/2017 11:26:49,Neut,1.0,W10-2912,We later discuss a manipulation of the corpus used by Yang (2004) to address this concern.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,(2004),"Additionally, there are significant challenges in reconstructing stress from an acoustic signal (Van Kuijk and Boves, 1999).","Stress is better organized into hierarchical patterns constructed on top of syllables that vary in relative prominence based on the domain of each level of the hierarchy, and generally languages avoid adjacent strong syllables (Liberman and Prince, 1977).",
1139043556,false,finalized,3,1/24/2017 11:40:49,Neut,1.0,W10-2912,"Additionally, there are significant challenges in reconstructing stress from an acoustic signal (Van Kuijk and Boves, 1999).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Van Kuijk and Boves, 1999)","For a child learner to use the algorithm presented here, she would need to have mechanisms for detecting stress in the speech signal and categorizing the gradient stress in utterances into a discrete level for each syllable.",We later discuss a manipulation of the corpus used by Yang (2004) to address this concern.,
1139043557,false,finalized,3,1/24/2017 11:41:25,Neut,1.0,W10-2912,"formation to detect word boundaries (Jusczyk et al., 1999), we believe that it is reasonable to assume that identifying syllabic stress is a task an infant learner can perform at the developmental stage of word segmentation.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Jusczyk et al., 1999)",4 A Simple Algorithm for Word Segmentation,90,
1139043558,false,finalized,3,1/24/2017 11:42:34,Neut,1.0,W10-2912,We now present a simple algebraic approach to word segmentation based on the constraints suggested by Yang (2004).,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,(2004),The learner we present is algebraic in that it has a lexicon which stores previously segmented words and identifies the input as a combination of words already in the lexicon and novel words.,4 A Simple Algorithm for Word Segmentation,
1139043559,false,finalized,3,1/24/2017 09:10:09,Neut,0.6815,W10-2912,"This type of memory function is a simplified representation of models of humans' memory recall capabilities (Anderson et al., 1998; Gillund and Shiffrin, 1984).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Anderson et al., 1998","This memory function for the value of  = 0.05, the value used in our experiments, is given in Figure 1.","pr(word) is the probability of a word being retrieved,  is a constant, and c(word) is the number of times the word has been identified in segmentations thus far.",
1139043560,false,finalized,3,1/24/2017 09:54:59,Neut,0.6733,W10-2912,"This type of memory function is a simplified representation of models of humans' memory recall capabilities (Anderson et al., 1998; Gillund and Shiffrin, 1984).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"Gillund and Shiffrin, 1984)","This memory function for the value of  = 0.05, the value used in our experiments, is given in Figure 1.","pr(word) is the probability of a word being retrieved,  is a constant, and c(word) is the number of times the word has been identified in segmentations thus far.",
1139043561,false,finalized,3,1/24/2017 09:02:07,Neut,0.6829,W10-2912,"While recent work from Bayesian approaches has used a Dirichlet Process to generate these distributions (Goldwater et al., 2006), in this learner the reuse of frequent items in learning is a result of the memory model rather than an explicit process of reusing old outcomes or generating new ones.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Goldwater et al., 2006)",This growth is an inherent property of the cognitive model of memory used here rather than an externally imposed computational technique.,"Probabilistic word recall results in a ""rich get richer"" phenomenon as the learner segments; words that are used more often in segmentations are more likely to be reused in later segmentations.",
1139043562,false,finalized,3,1/24/2017 09:54:59,Pos,0.6611,W10-2912,The corpus we use to evaluate it is the same corpus used by Yang (2004).,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,(2004),"Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.",Our computational model is designed to process child-directed speech.,
1139043563,false,finalized,3,1/24/2017 11:31:35,Neut,1.0,W10-2912,"Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,(1973),"We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word.",The corpus we use to evaluate it is the same corpus used by Yang (2004).,
1139043564,false,finalized,3,1/24/2017 10:48:06,Neut,0.6485,W10-2912,"Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(MacWhinney, 2000)","We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word.",The corpus we use to evaluate it is the same corpus used by Yang (2004).,
1139043565,false,finalized,3,1/24/2017 11:42:56,Neut,1.0,W10-2912,"We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Weide, 1998)","In CMUdict, lexical stress information is preserved by numbers: 0 for unstressed, 1 for primary stress, 2 for secondary stress.","Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.",
1139043566,false,finalized,3,1/24/2017 09:58:46,Neut,0.6836,W10-2912,"9month old infants are believed to have learned some phonotactic constraints of their native language (Mattys and Jusczyk, 2001), and learning these constraints can be done with only minimal exposure (Onishi et al., 2002).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Mattys and Jusczyk, 2001)","Finally, spaces and punctuation between words were removed, but the boundaries between utterances-as indicated by line breaks in CHILDES-are retained.","While we performed syllabification as a preprocessing step outside of learning, a child learner would presumably learn the required phonotactics as a part of learning to segment words.",
1139043567,false,finalized,3,1/24/2017 09:10:09,Neut,0.6815,W10-2912,"9month old infants are believed to have learned some phonotactic constraints of their native language (Mattys and Jusczyk, 2001), and learning these constraints can be done with only minimal exposure (Onishi et al., 2002).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Onishi et al., 2002)","Finally, spaces and punctuation between words were removed, but the boundaries between utterances-as indicated by line breaks in CHILDES-are retained.","While we performed syllabification as a preprocessing step outside of learning, a child learner would presumably learn the required phonotactics as a part of learning to segment words.",
1139043568,false,finalized,3,1/24/2017 10:51:02,Neut,1.0,W10-2912,"Cognitive literature suggests that limited memory during learning may be essential to a learner in its early stages (Elman, 1993).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf,"(Elman, 1993)","But we do not see any notable improvement as a result of the probabilistic memory model used in our experiments, although the learner does do better in the Reduced Stress condition with Probabilistic Memory than Perfect Memory.",But this slower learning is unlikely to be a concern for a child learner who would be exposed to much larger amounts of data than the corpora here provide.,
1139043569,false,finalized,3,1/24/2017 11:34:28,Neut,1.0,D15-1142,"Because hand-labeling individual words and word boundaries is very difficult (Jiao et al., 2006), producing segmented Chinese texts is very time-consuming and expensive.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Jiao et al., 2006)","Although a number of manually segmented datasets have been constructed by various organizations, it is not feasible to combine them into a single complete dataset because of their incompatibility due to the use of various segmenting standards.","However, the reliability of CWS that can be achieved using machine learning techniques relies heavily on the availability of a large amount of high-quality, manually segmented data.",
1139043570,false,finalized,3,1/24/2017 11:42:34,Neut,1.0,D15-1142,"These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Jiao et al., 2006)","In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed.","To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years.",
1139043571,false,finalized,3,1/24/2017 09:34:40,Neut,0.6525,D15-1142,"These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Liang et al., 2005)","In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed.","To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years.",
1139043572,false,finalized,3,1/24/2017 09:44:07,Neut,0.6939,D15-1142,"These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Sun and Xu, 2011)","In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed.","To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years.",
1139043573,false,finalized,3,1/24/2017 08:45:38,Neut,1.0,D15-1142,"These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Zeng et al., 2013a)","In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed.","To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years.",
1139043574,false,finalized,3,1/24/2017 11:36:43,Neut,0.6619,D15-1142,"In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Zeng et al., 2013b)","However, because monolingual unlabeled data contain limited natural segmenting information, in most semisupervised methods, the objective function tends to be optimized based on the personal experience and knowledge of the researchers.","These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).",
1139043575,false,finalized,3,1/24/2017 11:29:19,Neut,1.0,D15-1142,"In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Xu et al., 2008)","However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation.","Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.",
1139043576,false,finalized,3,1/24/2017 11:36:43,Neut,1.0,D15-1142,"In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Chang et al., 2008)","However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation.","Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.",
1139043577,false,finalized,3,1/24/2017 12:26:18,Neut,1.0,D15-1142,"In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Ma and Way, 2009)","However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation.","Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.",
1139043578,false,finalized,3,1/24/2017 11:44:25,Neut,1.0,D15-1142,"In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Chung et al., 2009)","However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation.","Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.",
1139043579,false,finalized,3,1/24/2017 09:46:51,Neut,1.0,D15-1142,"In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Xi et al., 2012)","However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation.","Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.",
1139043580,false,finalized,3,1/24/2017 09:38:34,Neut,0.6939,D15-1142,"The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Xue et al., 2003)","Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et",Considerable efforts have been made in the NLP community in the study of Chinese word segmentation.,
1139043581,false,finalized,3,1/24/2017 09:44:07,Pos,0.6478,D15-1142,"Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Peng et al., 2004)",1208,"The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003).",
1139043582,false,finalized,3,1/24/2017 11:34:28,Neut,1.0,D15-1142,"al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Zhang and Clark, 2007)","However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce.",1208,
1139043583,false,finalized,3,1/24/2017 09:29:13,Neut,0.671,D15-1142,"al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Zhao et al., 2010)","However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce.",1208,
1139043584,false,finalized,3,1/24/2017 11:44:41,Neut,1.0,D15-1142,"For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CRF model.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Sun and Xu, 2011)","(Zeng et al., 2013a) introduced a graph-based semi-supervised joint model of Chinese word segmentation and part-of-speech tagging and regularized the learning of a linear CRF model based on the label distributions derived from unlabeled data.","To address this problem, a number of semisupervised CWS approaches have been intensively investigated in recent years.",
1139043585,false,finalized,3,1/24/2017 10:31:30,Neut,1.0,D15-1142,"(Zeng et al., 2013a) introduced a graph-based semi-supervised joint model of Chinese word segmentation and part-of-speech tagging and regularized the learning of a linear CRF model based on the label distributions derived from unlabeled data.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Zeng et al., 2013a)","However, because monolingual unlabeled data lack natural segmenting information, most previous semi-supervised CWS methods have required certain assumptions to be made regarding their objective functions based on the researchers' personal experiences.","For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CRF model.",
1139043586,false,finalized,3,1/24/2017 08:46:56,Neut,1.0,D15-1142,"Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Xu et al., 2008)","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",2.2 Bilingual Semi-supervised CWS Methods,
1139043587,false,finalized,3,1/24/2017 12:02:14,Neut,1.0,D15-1142,"Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Chang et al., 2008)","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",2.2 Bilingual Semi-supervised CWS Methods,
1139043588,false,finalized,3,1/24/2017 11:12:23,Neut,1.0,D15-1142,"Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Ma and Way, 2009)","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",2.2 Bilingual Semi-supervised CWS Methods,
1139043589,false,finalized,3,1/24/2017 10:19:40,Neut,0.689,D15-1142,"Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Chung et al., 2009)","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",2.2 Bilingual Semi-supervised CWS Methods,
1139043590,false,finalized,3,1/24/2017 11:20:13,Neut,0.6857,D15-1142,"Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Xi et al., 2012)","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",2.2 Bilingual Semi-supervised CWS Methods,
1139043591,false,finalized,3,1/24/2017 10:31:30,Neut,0.6788,D15-1142,"These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Xu et al., 2004)","(Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT.","Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).",
1139043592,false,finalized,3,1/24/2017 11:51:29,Neut,0.6909,D15-1142,"These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Peng et al., 2004)","(Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT.","Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).",
1139043593,false,finalized,3,1/24/2017 11:43:45,Neut,1.0,D15-1142,"(Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Zeng et al., 2014)","However, because most of these approaches focus on SMT performance, they emphasize decreasing the perplexity of the bilingual data and word alignment rather than improving the CWS accuracy.","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",
1139043594,false,finalized,3,1/24/2017 11:06:35,Neut,1.0,D15-1142,"The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model's effectiveness in detecting OOV words.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Lafferty et al., 2001)","In this paper, we score the character-level feature in the same manner defined by (Xue et al., 2003).",3.1 Character-level Feature,
1139043595,false,finalized,3,1/24/2017 11:29:07,Neut,1.0,D15-1142,"The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model's effectiveness in detecting OOV words.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Xue et al., 2003)","In this paper, we score the character-level feature in the same manner defined by (Xue et al., 2003).",3.1 Character-level Feature,
1139043596,false,finalized,3,1/24/2017 10:28:10,Neut,1.0,D15-1142,"In this paper, we score the character-level feature in the same manner defined by (Xue et al., 2003).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Xue et al., 2003)","For the jth character cj in the sentence cJ1 = c1...cJ, the score can be calculated as follows:","The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model's effectiveness in detecting OOV words.",
1139043597,false,finalized,3,1/24/2017 10:28:29,Neut,0.6788,D15-1142,"We do not introduce the CRF-based CWS model in detail here, but more information can be obtained from (Lafferty et al., 2001) and (Xue et al., 2003).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Lafferty et al., 2001)",3.2 Phrase-level Features,"yj1 and yj represent the tags of the previous and current characters, respectively.",
1139043598,false,finalized,3,1/24/2017 11:48:38,Neut,1.0,D15-1142,"We do not introduce the CRF-based CWS model in detail here, but more information can be obtained from (Lafferty et al., 2001) and (Xue et al., 2003).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Xue et al., 2003)",3.2 Phrase-level Features,"yj1 and yj represent the tags of the previous and current characters, respectively.",
1139043599,false,finalized,3,1/24/2017 09:02:18,Neut,0.6733,D15-1142,"By representing the English and Chinese sentences as eI1 = e1e2...eI and cJ1 = c1c2...cJ, respectively, where ei and cj represent single elements of the sentences, we define their alignment as aK1 , of which each element is a span ak =< s, t > and represents the alignment of the English word es with the Chinese character ct. Then, the corpus of unlabeled bilingual data can be represented as the set of sentence tuples <eI1, cJ1 , aK1 > To obtain the character-based alignment, we employ an open-source toolkit Pialign1 ((Neubig et al., 2011); (Neubig et al., 2012)) which uses Bayesian learning and inversion transduction grammars.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Neubig et al., 2011)",3.2.2 Features Obtained from the,"Then, they are associated with English words using a statistical word aligner.",
1139043600,false,finalized,3,1/24/2017 09:41:07,Pos,1.0,D15-1142,"By representing the English and Chinese sentences as eI1 = e1e2...eI and cJ1 = c1c2...cJ, respectively, where ei and cj represent single elements of the sentences, we define their alignment as aK1 , of which each element is a span ak =< s, t > and represents the alignment of the English word es with the Chinese character ct. Then, the corpus of unlabeled bilingual data can be represented as the set of sentence tuples <eI1, cJ1 , aK1 > To obtain the character-based alignment, we employ an open-source toolkit Pialign1 ((Neubig et al., 2011); (Neubig et al., 2012)) which uses Bayesian learning and inversion transduction grammars.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Neubig et al., 2012)",3.2.2 Features Obtained from the,"Then, they are associated with English words using a statistical word aligner.",
1139043601,false,finalized,3,1/24/2017 10:13:40,Neut,0.6417,D15-1142,"The GIZA++ aligner is adopted to obtain word alignments (Och and Ney, 2000) from the converted NE dictionary.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Och and Ney, 2000)","The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables.","Specifically, we apply two standard log-linear phrase-based SMT models.",
1139043602,false,finalized,3,1/24/2017 10:53:55,Neut,0.6485,D15-1142,"The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Koehn et al., 2003)","A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language.","The GIZA++ aligner is adopted to obtain word alignments (Och and Ney, 2000) from the converted NE dictionary.",
1139043603,false,finalized,3,1/24/2017 11:54:11,Neut,1.0,D15-1142,"A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Stolcke et al., 2002)","Moses (Koehn et al., 2007) is used as a decoder.","The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables.",
1139043604,false,finalized,3,1/24/2017 11:53:59,Neut,0.6677,D15-1142,"Moses (Koehn et al., 2007) is used as a decoder.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Koehn et al., 2007)","Minimum error rate training (MERT) (Och et al., 2003) is applied to tune the feature parameters on the development dataset.","A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language.",
1139043605,false,finalized,3,1/24/2017 10:28:10,Neut,1.0,D15-1142,"Minimum error rate training (MERT) (Och et al., 2003) is applied to tune the feature parameters on the development dataset.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Och et al., 2003)","Given these two phrase-based translation models, we calculate each span < i, jm, jn > in AOne for the Chinese word wn using the following formula: Str(< i, jm, jn >) = Schen(< i, jm, jn >) +Sench(< i, jm, jn >) (6) where Schen(<i,jm,jn>) = DLev(Fletei, PTchen(Fpy(cjn jm))) means that the pronunciation conversion in the Chinese-English direction is performed as follows: First, the English word ei is split into its constituent letters; Second, the sequence of Chinese characters cjn jm is converted into its pronunciation; Third, this pronunciation is input into the Chinese-English phrase-based translation model, and the corresponding translation result is obtained; And finally, the Levenshtein distance between the English letters and the translation result is returned.","Moses (Koehn et al., 2007) is used as a decoder.",
1139043606,false,finalized,3,1/24/2017 11:41:25,Neut,0.6905,D15-1142,"First, we pre-train word embeddings using the open-source toolkit Word2Vec (Mikolov et al., 2013) on the Chinese (segmented using characterlevel features only) and English sentences separately, thereby obtaining the vocabularies Vch and Ven and their corresponding embedding matrixes Lch E Rn|Vch |and Len E Rn|Ven|.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Mikolov et al., 2013)","Given a Chinese word wn with an index i in the vocabulary, it is then straightforward to retrieve the word's vector representation via simple multiplication with a binary vector d that is equal to zero at all positions except that with index i:","English-Chinese semantic gap feature To guarantee that the semantic meanings of the Chinese segmentation match those of the corresponding English sentences as closely as possible, we propose to use a feature based on the EnglishChinese semantic gap to ensure the retention of semantic meaning during the segmentation process.",
1139043607,false,finalized,3,1/24/2017 11:37:54,Neut,0.6677,D15-1142,"Moreover, the bilingual unlabeled data is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Tian et al., 2014)","There are in total 2,215,000 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous.",Table 1: Statistics of training and testing datasets,
1139043608,false,finalized,3,1/24/2017 11:53:59,Neut,1.0,D15-1142,"Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Peng et al., 2004)","Moreover, we also evaluated the performance of our sub-models by","In the following, we refer to our methods as ""SLBD"" (segmenter leveraging bilingual data).",
1139043609,false,finalized,3,1/24/2017 09:29:13,Neut,0.671,D15-1142,"Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Asahara et al., 2005)","Moreover, we also evaluated the performance of our sub-models by","In the following, we refer to our methods as ""SLBD"" (segmenter leveraging bilingual data).",
1139043610,false,finalized,3,1/24/2017 11:15:27,Neut,1.0,D15-1142,"Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Zhang and Clark, 2007)","Moreover, we also evaluated the performance of our sub-models by","In the following, we refer to our methods as ""SLBD"" (segmenter leveraging bilingual data).",
1139043611,false,finalized,3,1/24/2017 10:48:06,Neut,0.3515,D15-1142,"Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Zhao et al., 2010)","Moreover, we also evaluated the performance of our sub-models by","In the following, we refer to our methods as ""SLBD"" (segmenter leveraging bilingual data).",
1139043612,false,finalized,3,1/24/2017 11:39:37,Neut,0.3528,D15-1142,"Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&X); (Zeng et al., 2013b) (Zeng).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Sun et al., 2012)","To ensure a fair comparison, we performed the evaluation in two steps.","Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model.",
1139043613,false,finalized,3,1/24/2017 10:53:55,CoCo,0.6485,D15-1142,"Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&X); (Zeng et al., 2013b) (Zeng).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Sun and Xu, 2011)","To ensure a fair comparison, we performed the evaluation in two steps.","Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model.",
1139043614,false,finalized,3,1/24/2017 10:31:30,CoCo,0.6715,D15-1142,"Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&X); (Zeng et al., 2013b) (Zeng).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Zeng et al., 2013b)","To ensure a fair comparison, we performed the evaluation in two steps.","Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model.",
1139043615,false,finalized,3,1/24/2017 12:20:27,Neut,1.0,D15-1142,"Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Xu et al., 2008)","The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation.","The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.",
1139043616,false,finalized,3,1/24/2017 10:32:04,CoCo,0.6715,D15-1142,"Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Ma and Way, 2009)","The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation.","The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.",
1139043617,false,finalized,3,1/24/2017 11:42:01,CoCo,0.6472,D15-1142,"Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Xi et al., 2012)","The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation.","The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.",
1139043618,false,finalized,3,1/24/2017 10:20:20,Pos,0.3497,D15-1142,"Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf,"(Zeng et al., 2014)","The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation.","The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.",
1139043619,false,finalized,3,1/24/2017 11:06:35,Neut,1.0,W14-5502,"of India, 2001), mainlv in the Indian state of Goa.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,"of India, 2001)",It also has a substantial amount of linguistic minoritv population living in neighboring states of Karnataka and Kerala.,Konkani is an Indian language spoken bv approximatelv 2.5 million people (Gov.,
1139043620,false,finalized,3,1/24/2017 11:27:28,Neut,1.0,W14-5502,"Consequentlv, a significant amount of work has been done in this field (Arbabi et al., 1994; Knight and Graehl, 1998).",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,"(Arbabi et al., 1994",Machine transliteration can generallv be classified as rule-based or statistical depending on the approach.,Machine transliteration is also useful for cross-language information retrieval (CLIR).,
1139043621,false,finalized,3,1/24/2017 09:07:26,Neut,0.6815,W14-5502,"Consequentlv, a significant amount of work has been done in this field (Arbabi et al., 1994; Knight and Graehl, 1998).",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,"Knight and Graehl, 1998)",Machine transliteration can generallv be classified as rule-based or statistical depending on the approach.,Machine transliteration is also useful for cross-language information retrieval (CLIR).,
1139043622,false,finalized,3,1/24/2017 11:41:49,Neut,1.0,W14-5502,Malik et al (2008) implemented a Hindi-Urdu transliteration svstem with finite-state transducers using a universal intermediate transcription (UIT).,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,(2008),It was based on the graphemic equivalence between Perso-Arabic script and Devanagari script.,"Within Indic transliteration, there have been several attempts on rule-based approaches.",
1139043623,false,finalized,3,1/24/2017 10:29:35,Neut,1.0,W14-5502,"Similarlv, Kishorjit (2011) developed a rule-based transliteration svstem also based on direct graphemic correspondence between Meetei Mavek and Bengali script.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,(2011),"On the other hand, statistical machine transliteration svstems tvpicallv use procedures familiar from statistical machine translation, including character alignments and subsequent training on the aligned data.",It was based on the graphemic equivalence between Perso-Arabic script and Devanagari script.,
1139043624,false,finalized,3,1/24/2017 09:54:59,Neut,0.6611,W14-5502,"Jia et al (2009) also developed a noisv channel model for the English-Chinese language pair using Moses, an SMT tool.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,(2009),Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses.,"On the other hand, statistical machine transliteration svstems tvpicallv use procedures familiar from statistical machine translation, including character alignments and subsequent training on the aligned data.",
1139043625,false,finalized,3,1/24/2017 11:46:16,Neut,1.0,W14-5502,Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses.,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,(2013),"Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM).","Jia et al (2009) also developed a noisv channel model for the English-Chinese language pair using Moses, an SMT tool.",
1139043626,false,finalized,3,1/24/2017 09:48:56,Neut,1.0,W14-5502,"Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM).",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,(2009),Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration.,Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses.,
1139043627,false,finalized,3,1/24/2017 09:42:21,Neut,0.6852,W14-5502,Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration.,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,(2012),A hvbrid approach combining FSM based techniques with a statistical word language model with better performance was proposed bv Malik et al (2009).,"Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM).",
1139043628,false,finalized,3,1/24/2017 09:58:46,Pos,0.6721,W14-5502,A hvbrid approach combining FSM based techniques with a statistical word language model with better performance was proposed bv Malik et al (2009).,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,(2009),3 Initial Attempts,Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration.,
1139043629,false,finalized,3,1/24/2017 09:38:34,Pos,1.0,W14-5502,"The implementation of the framework has been done entirelv using OpenFst (Allauzen et al., 2007).",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,"(Allauzen et al., 2007)","Thrax (Tai et al., 2011) was used to define context-dependent rewrite rules and compile those rules into finite-state transducers compatible with Openfst.",4 Architecture,
1139043630,false,finalized,3,1/24/2017 10:19:40,Neut,0.6503,W14-5502,"Thrax (Tai et al., 2011) was used to define context-dependent rewrite rules and compile those rules into finite-state transducers compatible with Openfst.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,"(Tai et al., 2011)",Thrax was also used to define finite state acceptors.,"The implementation of the framework has been done entirelv using OpenFst (Allauzen et al., 2007).",
1139043631,false,finalized,3,1/24/2017 10:29:35,Neut,1.0,W14-5502,"Character alignments were performed using Phonetisaurus (Novak et al., 2011).",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,"(Novak et al., 2011)","N-gram models were created with the OpenGrm Ngram librarv (Roark et al., 2012), which also generates the models as FSTs.",We found Thrax to be particularlv robust and flexible in generating various FSTs.,
1139043632,false,finalized,3,1/24/2017 10:32:39,Neut,0.6427,W14-5502,"N-gram models were created with the OpenGrm Ngram librarv (Roark et al., 2012), which also generates the models as FSTs.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,"(Roark et al., 2012)","Below, we describe the detailed architecture for each transliteration pair.","Character alignments were performed using Phonetisaurus (Novak et al., 2011).",
1139043633,false,finalized,3,1/24/2017 09:30:33,Pos,0.6746,W14-5502,"These rules were compiled as finite-state transducers (Narasimhan et al., 2004) We have produced a list of possible suffixes and prefixes from the collected corpus.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,"(Narasimhan et al., 2004)",Let Pre denote the set of prefixes and 8uf the set of prefixes.,The rules below are listed with a corresponding sample case.,
1139043634,false,finalized,3,1/24/2017 09:12:10,Pos,0.6733,W14-5502,"We initiallv experimented with tools such as GIZA++, but found Phonetisaurus produced better alignments compared to other tools as it uses manv-to-manv alignments developed specificallv for grapheme to phoneme svstems (Jiampojamarn et al., 2007).",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,"(Jiampojamarn et al., 2007)","A sample alignment sequence from Phonetisaurus is given below: melillem I mellil'lem  m}m e}e 1}lel i}i l}l1' l}l e}e m}m gadvemtlvan I gaddientlean  g}g a}a d}dld v}i e}e m}n t}t l}l v}e a}a n}n bharatasarkva I bharotasarkea  bh}blh a}a r}r a}o t}t a}a s}s a}a r}r k}k v}e a}a bhiveli I bhieli  bh}blh ilv}i e}e l}l i}i Where } denotes individual character alignment, I between characters indicates grapheme chunks, and d}dld implies that the source grapheme ed is mapped to the target graphemic chunk edd.","Romanizing the input verv marginallv improves the character alignment process, since both input and output are then alphabetic scripts (as compared to svllabic to alphabetic alignment).",
1139043635,false,finalized,3,1/24/2017 10:20:53,Pos,0.6512,W14-5502,"This alignment lattice was then used to create a joint sequence n-gram model (Galescu and Allen, 2002) Nknrm.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf,"(Galescu and Allen, 2002)","This is then composed with the input word I, whose output projection we use.",Figure 5: 5 best paths of Lp for Kannada input 26Af aikv,
1139043636,false,finalized,3,1/24/2017 11:42:01,Neut,0.6905,W07-1431,"Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Jijkoun and de Rijke, 2005)","Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: No case of indigenously acquired rabies infection has been confirmed in the past 2 years.","A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle.",
1139043637,false,finalized,3,1/24/2017 09:02:07,Pos,0.646,W07-1431,"Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Romano et al., 2006)","Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: No case of indigenously acquired rabies infection has been confirmed in the past 2 years.","A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle.",
1139043638,false,finalized,3,1/24/2017 09:53:49,Pos,0.6741,W07-1431,"Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Hickl et al., 2006)","Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: No case of indigenously acquired rabies infection has been confirmed in the past 2 years.","A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle.",
1139043639,false,finalized,3,1/24/2017 11:36:43,Neut,1.0,W07-1431,"At the other extreme, textual inference can be approached as deduction, building on work in formal computational semantics to translate sentences into first-order logic (FOL), and then applying a theorem prover or a model builder (Akhmatova, 2005; Fowler et al., 2005).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Akhmatova, 2005","However, such approaches tend to founder on the difficulty of accurately translating natural language in FOL-tricky issues include idioms, intensionality and propositional attitudes, modalities, temporal and causal relations, certain quantifiers, and so on.","Because it drops important qualifiers in a negative context, the hypothesis does not follow; yet both the lexical content and the predicate-argument structure of the hypothesis closely match the premise.",
1139043640,false,finalized,3,1/24/2017 09:12:49,Pos,0.6741,W07-1431,"At the other extreme, textual inference can be approached as deduction, building on work in formal computational semantics to translate sentences into first-order logic (FOL), and then applying a theorem prover or a model builder (Akhmatova, 2005; Fowler et al., 2005).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"Fowler et al., 2005)","However, such approaches tend to founder on the difficulty of accurately translating natural language in FOL-tricky issues include idioms, intensionality and propositional attitudes, modalities, temporal and causal relations, certain quantifiers, and so on.","Because it drops important qualifiers in a negative context, the hypothesis does not follow; yet both the lexical content and the predicate-argument structure of the hypothesis closely match the premise.",
1139043641,false,finalized,3,1/24/2017 09:41:07,Neut,0.6478,W07-1431,"FOL-based systems that have attained high precision (Bos and Markert, 2006) have done so at the cost of very poor recall.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Bos and Markert, 2006)","In this work, we explore a different point on the spectrum, by developing a computational model of natural logic, that is, a logic whose vehicle of inference is natural language.'","However, such approaches tend to founder on the difficulty of accurately translating natural language in FOL-tricky issues include idioms, intensionality and propositional attitudes, modalities, temporal and causal relations, certain quantifiers, and so on.",
1139043642,false,finalized,3,1/24/2017 11:33:16,Neut,1.0,W07-1431,"The monotonicity calculus developed in (Sanchez Valencia, 1991) explains these polarity effects by (1) defining an entailment relation over multifarious expressions of natural language, (2) defining monotonicity properties of semantic functions, and finally (3) specifying how monotonicities combine during Fregean composition of semantic functions.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Sanchez Valencia, 1991)",The entailment relation.,"Other elements can only be contracted (not expanded) salva veritate, and thus have negative polarity: meal can be narrowed to dinner.",
1139043643,false,finalized,3,1/24/2017 10:16:28,Neut,0.3556,W07-1431,"Our natural logic system, dubbed the NatLog system, has a three-stage architecture similar to those in (Marsi and Krahmer, 2005; MacCartney et al., 2006), comprising (1) linguistic pre-preprocessing, (2) alignment, and (3) entailment classification.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Marsi and Krahmer, 2005",3.1 Linguistic pre-processing,3 The NatLog System,
1139043644,false,finalized,3,1/24/2017 11:40:32,Neut,0.6677,W07-1431,"Our natural logic system, dubbed the NatLog system, has a three-stage architecture similar to those in (Marsi and Krahmer, 2005; MacCartney et al., 2006), comprising (1) linguistic pre-preprocessing, (2) alignment, and (3) entailment classification.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"MacCartney et al., 2006)",3.1 Linguistic pre-processing,3 The NatLog System,
1139043645,false,finalized,3,1/24/2017 11:41:25,Neut,1.0,W07-1431,"We rely on the Stanford parser (Klein and Manning, 2003), a Treebank-trained statistical parser, for tokenization, part-of-speech tagging, and phrase-structure parsing.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Klein and Manning, 2003)","By far the most important analysis performed at this stage is monotonicity marking, in which we compute the effective mono","Relative to other textual inference systems, the NatLog system does comparatively little linguistic preprocessing.",
1139043646,false,finalized,3,1/24/2017 10:31:30,Neut,1.0,W07-1431,"We define a list of downward-monotone and non-monotone expressions, and for each item we specify its arity and a Tregex pattern (Levy and Andrew, 2006) which permits us to identify its occurrences.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Levy and Andrew, 2006)","We also specify, for each argument, both the monotonicity and another Tregex pattern which helps us to determine the sentence span over which the monotonicity is projected.","Unlike the categorial grammar parses assumed by Sanchez Valencia, the nesting of constituents in phrase-structure parses does not always correspond to the composition of semantic functions, which introduces a number of complications.",
1139043647,false,finalized,3,1/24/2017 09:04:17,Neut,1.0,W07-1431,"The FraCaS test suite (Cooper et al., 1996) was developed as part of a collaborative research effort in computational semantics.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Cooper et al., 1996)",It contains 346 inference problems reminiscent of a textbook on formal semantics.,4 Experiments with the FraCaS test suite,
1139043648,false,finalized,3,1/24/2017 11:51:29,CoCo,0.6909,W07-1431,"Textual inference problems from the PASCAL RTE Challenge (Dagan et al., 2005) differ from FraCaS problems in several important ways.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Dagan et al., 2005)",(See table 5 for examples.),5 Experiments with RTE data,
1139043649,false,finalized,3,1/24/2017 09:12:10,Pos,0.6611,W07-1431,"If we succeed, then we may be able to hybridize with a broad-coverage RTE system to obtain better results than either system individually-the same strategy that was adopted by (Bos and Markert, 2006) for their FOL-based system.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Bos and Markert, 2006)","For this purpose, we have chosen to use the Stanford RTE system described in (de Marneffe et al., 2006).","Rather, in applying NatLog to RTE, we hope to make reliable predictions on a subset of RTE problems, trading recall for precision.",
1139043650,false,finalized,3,1/24/2017 11:48:38,Neut,0.6867,W07-1431,"For this purpose, we have chosen to use the Stanford RTE system described in (de Marneffe et al., 2006).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(de Marneffe et al., 2006)","In applying NatLog to RTE problems, we use alignments from the Stanford system as input to our entailment model.","If we succeed, then we may be able to hybridize with a broad-coverage RTE system to obtain better results than either system individually-the same strategy that was adopted by (Bos and Markert, 2006) for their FOL-based system.",
1139043651,false,finalized,3,1/24/2017 11:41:49,CoCo,0.6876,W07-1431,"For comparison, the FOL-based system reported in (Bos and Markert, 2006) attained a similarly high precision of 76% on RTE2 problems, but was able to make a positive prediction in only about 4% of cases.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Bos and Markert, 2006)","NatLog makes positive predictions far more often-at a rate of 18% on the development set, and 24% on the test set.","Relative to the Stanford RTE system, NatLog achieves high precision on its yes predictions-about 76% on the development set, and 68% on the test set-suggesting that hybridizing may be effective.",
1139043652,false,finalized,3,1/24/2017 11:47:00,Neut,1.0,W07-1431,"While the roots of natural logic can be traced back to Aristotle's syllogisms, the modern conception of natural logic began with George Lakoff, who proposed ""a logic for natural language"" which could ""characterize all the valid inferences that can be made in natural language"" (Lakoff, 1970).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Lakoff, 1970)","The study of natural logic was formalized by Johan van Benthem, who crucially connected it with categorial grammar (van Benthem, 1986), and later was brought to fruition by Victor Sanchez Valencia, who first gave a precise definition of a calculus of mono",6 Related work,
1139043653,false,finalized,3,1/24/2017 10:41:36,Neut,0.6939,W07-1431,"The study of natural logic was formalized by Johan van Benthem, who crucially connected it with categorial grammar (van Benthem, 1986), and later was brought to fruition by Victor Sanchez Valencia, who first gave a precise definition of a calculus of mono",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(van Benthem, 1986)",199,"While the roots of natural logic can be traced back to Aristotle's syllogisms, the modern conception of natural logic began with George Lakoff, who proposed ""a logic for natural language"" which could ""characterize all the valid inferences that can be made in natural language"" (Lakoff, 1970).",
1139043654,false,finalized,3,1/24/2017 11:29:19,Neut,1.0,W07-1431,"tonicity (Sanchez Valencia, 1991).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Sanchez Valencia, 1991)","A small current of theoretical work has continued up to the present, for example (Zamansky et al., 2006).",199,
1139043655,false,finalized,3,1/24/2017 11:06:35,Neut,1.0,W07-1431,"A small current of theoretical work has continued up to the present, for example (Zamansky et al., 2006).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Zamansky et al., 2006)",There has been surprisingly little work on building computational models of natural logic.,"tonicity (Sanchez Valencia, 1991).",
1139043656,false,finalized,3,1/24/2017 11:50:33,Neut,1.0,W07-1431,"(Fyodorov et al., 2003) describes a Prolog implementation for a small fragment of English, based on a categorial grammar parser.6 In an unpublished draft, (van Eijck, 2005) describes a preliminary implementation in Haskell.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Fyodorov et al., 2003)","Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985).",There has been surprisingly little work on building computational models of natural logic.,
1139043657,false,finalized,3,1/24/2017 11:40:11,Neut,0.6905,W07-1431,"(Fyodorov et al., 2003) describes a Prolog implementation for a small fragment of English, based on a categorial grammar parser.6 In an unpublished draft, (van Eijck, 2005) describes a preliminary implementation in Haskell.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(van Eijck, 2005)","Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985).",There has been surprisingly little work on building computational models of natural logic.,
1139043658,false,finalized,3,1/24/2017 09:38:34,Neut,1.0,W07-1431,"Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Hobbs, 1985)","To our knowledge, the FraCaS results reported here represent the first such evaluation.","(Fyodorov et al., 2003) describes a Prolog implementation for a small fragment of English, based on a categorial grammar parser.6 In an unpublished draft, (van Eijck, 2005) describes a preliminary implementation in Haskell.",
1139043659,false,finalized,3,1/24/2017 09:53:49,Neut,0.6815,W07-1431,"(Sukkarieh, 2003) describes applying a deductive system to some FraCaS inferences, but does not perform a complete evaluation or report quantitative results.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf,"(Sukkarieh, 2003)",7 Conclusion,"To our knowledge, the FraCaS results reported here represent the first such evaluation.",
1139043660,false,finalized,3,1/24/2017 10:13:40,Neut,0.6417,N04-1032,based on the SVM-based algorithm proposed for English by Pradhan et al (2003).,http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,(2003),We first describe our creation of a small 1100-sentence Chinese corpus labeled according to principles from the English and (in-progress) Chinese PropBanks.,Email: jurafsky@stanford.edu.,
1139043661,false,finalized,3,1/24/2017 10:28:48,Neut,1.0,N04-1032,We then describe our port of the Collins (1999) parser to Chinese.,http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,(1999),"Finally, we apply our SVM semantic parser to a matching English corpus, and discuss the differences between English and Chinese that lead to significantly better performance on Chinese.","We then introduce the features used by our SVM classifier, and show their performance on semantic parsing for both seen and unseen verbs, given hand-corrected (Chinese TreeBank) syntactic parses.",
1139043662,false,finalized,3,1/24/2017 12:41:40,Neut,0.6867,N04-1032,"For these experiments, we therefore hand-labeled a small corpus following the Penn Chinese Propbank labeling guidelines (Xue, 2002).",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,"(Xue, 2002)","In this section, we first describe the semantic roles we used in the annotation and then introduce the data for our experiments.","Although a project to produce a Chinese PropBank is underway (Xue and Palmer 2003), this data is not expected to be available for another year.",
1139043663,false,finalized,3,1/24/2017 11:25:39,Neut,0.6649,N04-1032,"Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,(2002),"For each (non-aux/non-copula) verb in each sentence, our classifier examines each node in the syntactic parse tree for the sentence and assigns it a semantic role label.",3.1 Architecture and Classifier,
1139043664,false,finalized,3,1/24/2017 12:20:27,Neut,0.6867,N04-1032,"Our architecture is based on a Support Vector Machine classifier, following Pradhan et al (2003).",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,(2003),"Since SVMs are binary classifiers, we represent this 1-of-19 classification problem (18 roles plus NULL) by training 19 binary one-versus-all classifiers.","Most constituents are not arguments of the verb, and so the most common label is NULL.",
1139043665,false,finalized,3,1/24/2017 10:52:04,Neut,0.6485,N04-1032,"Following Pradhan et al (2003), we used tinySVM along with YamCha (Kudo and Matsumoto 2000, 2001) as the SVM training and test software.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,(2003),"The system uses a polynominal kernel with degree 2; the cost per unit violation of the margin, C=1; tolerance of the termination criterion e=0.001.","Since SVMs are binary classifiers, we represent this 1-of-19 classification problem (18 roles plus NULL) by training 19 binary one-versus-all classifiers.",
1139043666,false,finalized,3,1/24/2017 11:07:20,Neut,1.0,N04-1032,"In the original formulation for English in Gildea and Jurafsky (2002), it answers the question: Is the NP governed by IP or VP?",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,(2002),"An NP governed by an IP is likely to be a subject, while an NP governed by a VP is more likely to be an object.",feature is only applicable for NPs.,
1139043667,false,finalized,3,1/24/2017 11:50:36,CoCo,0.6688,N04-1032,"Since the governing category information is encoded in the path feature, it may be redundant; indeed this redundancy might explain why the governing category feature was used in Gildea & Jurafsky(2002) but not in Gildea and Palmer(2002).",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,(2002),"Since the ""DE"" construction caused us to modify the feature for Chinese, we conducted several experiments to test whether the governing category feature is useful or whether it is redundant with the path and position features.","""the international Olympic Conference held in Paris"" Figure 1 Example of DE construction",
1139043668,false,finalized,3,1/24/2017 11:25:20,Neut,0.6581,N04-1032,"Since the governing category information is encoded in the path feature, it may be redundant; indeed this redundancy might explain why the governing category feature was used in Gildea & Jurafsky(2002) but not in Gildea and Palmer(2002).",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,(2002),"Since the ""DE"" construction caused us to modify the feature for Chinese, we conducted several experiments to test whether the governing category feature is useful or whether it is redundant with the path and position features.","""the international Olympic Conference held in Paris"" Figure 1 Example of DE construction",
1139043669,false,finalized,3,1/24/2017 09:29:13,Pos,1.0,N04-1032,"Due to the sparsity of the head word feature, we also use the part-of-speech of the head word, following Surdeanu et al (2003).",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,(2003),"For example, ""7 26 /July 26"" may not be seen in the training, but its POS, NT(temporal noun) , is a good indicator that it is a temporal.",These statistics emphasize the key role of the lexicalized head word feature in capturing the collocation between verbs and their arguments.,
1139043670,false,finalized,3,1/24/2017 09:02:18,Neut,0.6733,N04-1032,"In this section we describe experiments on semantic parsing when given automatic parses produced by an automatic parser, the Collins (1999) parser, ported to Chinese.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,(1999),We first describe how we ported the Collins parser to Chinese and then present the results of the semantic parser with features drawn from the automatic parses.,"In practical use, of course, automatic parses will not be as accurate.",
1139043671,false,finalized,3,1/24/2017 09:53:01,Pos,0.3497,N04-1032,"The Collins parser is a state-of-the-art statistical parser that has high performance on English (Collins, 1999) and Czech(Collins et al.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,"(Collins, 1999)",1999).,4.1 The Collins parser for Chinese,
1139043672,false,finalized,3,1/24/2017 09:39:38,Neut,1.0,N04-1032,"There have been attempts in applying other algorithms in Chinese parsing (Bikel and Chiang, 2000; Chiang and Bikel 2002; Levy and Manning 2003), but there has been no report on applying the Collins parser on Chinese.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,"(Bikel and Chiang, 2000",The Collins parser is a lexicalized statistical parser based on a head-driven extended PCFG model; thus the choice of head node is crucial to the success of the parser.,1999).,
1139043673,false,finalized,3,1/24/2017 09:45:36,Pos,0.6486,N04-1032,Many of these features required creating an automatic parse; in doing so we showed that the Collins (1999) parser when ported to Chinese achieved the best reported performance on Chinese syntactic parsing.,http://www.aclweb.org/anthology/N/N04/N04-1032.pdf,(1999),"Finally, we showed that semantic parsing is significantly easier in Chinese than in English.","Second, the features that we extracted for English semantic parsing worked well when applied to Chinese.",
1139043674,false,finalized,3,1/24/2017 11:20:13,Pos,0.6571,W07-1033,"Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Kim et al., 2004)","After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000).","Such a large variety of vocabulary naturally leads to long names with productive use of general words, making the task difficult to be solved by systems with naive Markov assumption of label sequences, because such systems must perform their prediction without seeing the entire string of the entities.",
1139043675,false,finalized,3,1/24/2017 11:37:54,Neut,0.6677,W07-1033,"Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Zhou and Su, 2004)","After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000).","Such a large variety of vocabulary naturally leads to long names with productive use of general words, making the task difficult to be solved by systems with naive Markov assumption of label sequences, because such systems must perform their prediction without seeing the entire string of the entities.",
1139043676,false,finalized,3,1/24/2017 08:59:10,Pos,0.671,W07-1033,"After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000).",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Lafferty et al., 2001)","However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al.","Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.",
1139043677,false,finalized,3,1/24/2017 11:42:34,Pos,0.6472,W07-1033,"After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000).",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(McCallum et al., 2000)","However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al.","Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.",
1139043678,false,finalized,3,1/24/2017 11:35:15,Neut,1.0,W07-1033,"We had to wait until Tsai et al (2006), who combine pattern-based postprocessing with CRFs, for CRF-based systems to achieve the same level of performance as Zhou et al.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2006),"As such, a key to further improvement of the performance of bio-entity recognition has been to employ global features, which are effective to capture the features of long names appearing in the bio domain.",Another reason may be that the computational complexity of the models prevented the developers to invent effective features for the task.,
1139043679,false,finalized,3,1/24/2017 09:33:50,Pos,1.0,W07-1033,"In this paper, we use reranking architecture, which was successfully applied to the task of natural language parsing (Collins, 2000; Charniak and",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Collins, 2000",209,"As such, a key to further improvement of the performance of bio-entity recognition has been to employ global features, which are effective to capture the features of long names appearing in the bio domain.",
1139043680,false,finalized,3,1/24/2017 11:34:25,Neut,1.0,W07-1033,"Johnson, 2005), to address the problem.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"Johnson, 2005)","Reranking enables us to incorporate truly global features to the model of named entity tagging, and we aim to realize the state-of-the-art performance without depending on rule-based post-processes.","BioNLP 2007: Biological, translational, and clinical language processing, pages 209-216, Prague, June 2007. c2007 Association for Computational Linguistics",
1139043681,false,finalized,3,1/24/2017 10:16:28,Neut,0.6444,W07-1033,The former includes Relational Markov Networks by Bunescu et al (2004) and skip-edge CRFs by Sutton et al (2004).,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2004),"A major drawback of this kind of systems may be heavy computational cost of inference both for training and running the systems, because non-local dependency forces such models to use expensive approximate inference instead of dynamic-programming-based exact inference.","Such systems may be classified into two kinds, one of them uses a single classifier which is optimized incorporating non-local features, and the other consists of pipeline of more than one classifiers.",
1139043682,false,finalized,3,1/24/2017 08:46:56,Neut,1.0,W07-1033,The former includes Relational Markov Networks by Bunescu et al (2004) and skip-edge CRFs by Sutton et al (2004).,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2004),"A major drawback of this kind of systems may be heavy computational cost of inference both for training and running the systems, because non-local dependency forces such models to use expensive approximate inference instead of dynamic-programming-based exact inference.","Such systems may be classified into two kinds, one of them uses a single classifier which is optimized incorporating non-local features, and the other consists of pipeline of more than one classifiers.",
1139043683,false,finalized,3,1/24/2017 09:42:21,Neut,0.6852,W07-1033,"The latter, pipelined systems include a recent study by Krishnan et al (2006), as well as our reranking system.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2006),"Their method is a two stage model of CRFs, where the second CRF uses the global information of the output of the first CRF.","A major drawback of this kind of systems may be heavy computational cost of inference both for training and running the systems, because non-local dependency forces such models to use expensive approximate inference instead of dynamic-programming-based exact inference.",
1139043684,false,finalized,3,1/24/2017 11:43:45,Neut,1.0,W07-1033,"The training data provided by the shared task consisted of 2000 abstracts of biomedical articles taken from the GENIA corpus version 3 (Ohta et al., 2002), which consists of the MEDLINE abstracts with publication years from 1990 to 1999.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Ohta et al., 2002)",The articles are annotated with named-entity BIO tags as an example shown in Table 1.,"This section overviews the task of biomedical named entity recognition as presented in JNLPBA shared task held at COLING 2004, and the systems that were successfully applied to the task.",
1139043685,false,finalized,3,1/24/2017 12:20:27,Neut,1.0,W07-1033,"Kim et al (Kim et al., 2004) compare the 8 systems participated in the shared task.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Kim et al., 2004)","The systems use various classification models including CRFs, hidden Markov models (HMMs), support vector machines (SVMs), and MEMMs, with various features and external resources.",The difference of publication years between the training and test sets reflects the organizer's intention to see the entity recognizers' portability with regard to the differences of the articles' publication years.,
1139043686,false,finalized,3,1/24/2017 11:27:35,Neut,1.0,W07-1033,"Though it is impossible to observe clear correlation between the performance and classification models or resources used, an important characteristic of the best system by Zhou et al (2004) seems to be extensive use of rule-based post processing they apply to the output of their classifier.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2004),"After the shared task, several researchers tackled the problem using the CRFs and their extensions.","The systems use various classification models including CRFs, hidden Markov models (HMMs), support vector machines (SVMs), and MEMMs, with various features and external resources.",
1139043687,false,finalized,3,1/24/2017 08:59:10,Pos,0.671,W07-1033,"Okanohara et al (2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2006),Friedrich et al (2006) used CRFs with features from the external gazetteer.,"After the shared task, several researchers tackled the problem using the CRFs and their extensions.",
1139043688,false,finalized,3,1/24/2017 12:00:42,Neut,1.0,W07-1033,"Okanohara et al (2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Sarawagi and Cohen, 2004)",Friedrich et al (2006) used CRFs with features from the external gazetteer.,"After the shared task, several researchers tackled the problem using the CRFs and their extensions.",
1139043689,false,finalized,3,1/24/2017 11:54:35,Neut,1.0,W07-1033,Friedrich et al (2006) used CRFs with features from the external gazetteer.,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2006),"Current state-of-the-art for the shared-task is achieved by Tsai et al (2006), whose improvement depends on careful design of features including the normalization of numeric expressions, and use of post-processing by automatically extracted patterns.","Okanohara et al (2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state.",
1139043690,false,finalized,3,1/24/2017 10:32:57,Neut,0.6496,W07-1033,"Current state-of-the-art for the shared-task is achieved by Tsai et al (2006), whose improvement depends on careful design of features including the normalization of numeric expressions, and use of post-processing by automatically extracted patterns.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2006),210,Friedrich et al (2006) used CRFs with features from the external gazetteer.,
1139043691,false,finalized,3,1/24/2017 09:53:01,Pos,0.689,W07-1033,"As our n-best tagger, we use a first order MEMM model (McCallum et al., 2000).",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(McCallum et al., 2000)","Though CRFs (Lafferty et al., 2001) can be regarded as improved version of MEMMs, we have chosen MEMMs because MEMMs are usually much faster to train compared to CRFs, which enables extensive feature selection.",3 N-best MEMM tagger,
1139043692,false,finalized,3,1/24/2017 11:42:01,Neut,0.6624,W07-1033,"Though CRFs (Lafferty et al., 2001) can be regarded as improved version of MEMMs, we have chosen MEMMs because MEMMs are usually much faster to train compared to CRFs, which enables extensive feature selection.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Lafferty et al., 2001)","Training a CRF tagger with features selected using an MEMM may result in yet another performance boost, but in this paper we concentrate on the MEMM as our n-best tagger, and consider CRFs as one of our future extensions.","As our n-best tagger, we use a first order MEMM model (McCallum et al., 2000).",
1139043693,false,finalized,3,1/24/2017 09:22:53,CoCo,0.3522,W07-1033,"Though existing studies suggest that changing the tag set of the original corpus, such as splitting of O tags, can contribute to the performances of named entity recognizers (Peshkin and Pfefer, 2003), our system uses the original tagset of the training data, except that the 'BOS' label is added to represent the state before the beginning of sentences.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Peshkin and Pfefer, 2003)",Probability of state transition to the i-th label of a sentence is calculated by the following formula:,Table 1 shows the state transition table of our MEMM model.,
1139043694,false,finalized,3,1/24/2017 11:45:36,Neut,1.0,W07-1033,"where li is the next BIO tag, li1 is the previous BIO tag, S is the target sentence, and fj and lj are feature functions and parameters of a log-linear model (Berger et al., 1996).",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Berger et al., 1996)","As a first order MEMM, the probability of a label li is dependent on the previous label li1, and when we calculate the normalization constant in the right hand side (i.e.",Table 2: (Recall/Precision/F-score) of forward and backward tagging.,
1139043695,false,finalized,3,1/24/2017 11:43:08,Neut,0.6503,W07-1033,"We could have partially alleviated this effect by employing head-word triggers as done in Zhou et al (2004), but we decided to use backward tagging because the results of a number of preliminary experiments, including the ones shown in Table 2 above, seemed to be showing that the backward tagging is preferable in this task setting.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2004),3.2 Feature set,"In biomedical named-entity tagging, right boundaries are usually easier to detect, and it may be the reason of the superiority of the backward tagging.",
1139043696,false,finalized,3,1/24/2017 09:45:36,Pos,0.6486,W07-1033,"The training data of the reranker is created by the n-best tagger, and every set of 17 sections from the training set is used to train the n-best tagger for the remaining section (The same technique is used by previous studies to avoid the n-best tagger's 'unrealistically good' performance on the training set (Collins, 2000)).",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Collins, 2000)","Among the n-best sequences output by the MEMM tagger, the sequence with the highest F-score is used as the 'correct' sequence for training the reranker.","We divided the data into 20 contiguous and equally-sized sections, and used the first 18 sections for training, and the last 2 sections for testing while development (henceforth the training and development sets, respectively).",
1139043697,false,finalized,3,1/24/2017 09:12:25,Pos,0.6583,W07-1033,"In both models, Gaussian prior distributions are used to avoid overfitting (Chen and Rosenfeld, 1999), and the standard deviations of the Gaussian distributions are optimized to maximize the performance on the development set.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Chen and Rosenfeld, 1999)",We also used a thresholding technique which discards features with low frequency.,The two log-linear models for the MEMM tagger and reranker are estimated using a limited-memory BFGS algorithm implemented in an open-source software Amis3.,
1139043698,false,finalized,3,1/24/2017 11:39:52,Neut,1.0,W07-1033,"of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006), features of shallow parsers had a large contribution to the performance.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Kim et al., 2004","The information of the previous labels was also quite effective, which indicates that label unigram models (i.e.",Table 4: Performance of the reranker.,
1139043699,false,finalized,3,1/24/2017 11:15:03,Neut,1.0,W07-1033,"of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006), features of shallow parsers had a large contribution to the performance.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"Okanohara et al., 2006","The information of the previous labels was also quite effective, which indicates that label unigram models (i.e.",Table 4: Performance of the reranker.,
1139043700,false,finalized,3,1/24/2017 09:02:07,Pos,0.646,W07-1033,"of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006), features of shallow parsers had a large contribution to the performance.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"Tzong-Han Tsai et al., 2006)","The information of the previous labels was also quite effective, which indicates that label unigram models (i.e.",Table 4: Performance of the reranker.,
1139043701,false,finalized,3,1/24/2017 09:07:26,Neut,0.6741,W07-1033,"For example, Krauthammer et al (2004) report the inter-annotater agreement rate of 77.6% for the three way bio-entity classification task.)",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2004),and the performance can be said to be at the same level as the best systems.,"As naturally expected, our system outperformed the systems that cannot accommodate truly global features (Note that one point of F-score improvement is valuable in this task, because inter-annotator agreement rate of human experts in bio-entity recognition is likely to be about 80%.",
1139043702,false,finalized,3,1/24/2017 10:52:04,Neut,0.6617,W07-1033,"F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2006),Table 7: Performance comparison on the test set.,Table 6: Comparison of the F-scores of rerankers trained and evaluated with various 0-best taggers.,
1139043703,false,finalized,3,1/24/2017 09:44:07,CoCo,0.6478,W07-1033,"F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2004),Table 7: Performance comparison on the test set.,Table 6: Comparison of the F-scores of rerankers trained and evaluated with various 0-best taggers.,
1139043704,false,finalized,3,1/24/2017 10:29:35,CoCo,0.6715,W07-1033,"F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2006),Table 7: Performance comparison on the test set.,Table 6: Comparison of the F-scores of rerankers trained and evaluated with various 0-best taggers.,
1139043705,false,finalized,3,1/24/2017 08:41:35,CoCo,1.0,W07-1033,"F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,(2006),Table 7: Performance comparison on the test set.,Table 6: Comparison of the F-scores of rerankers trained and evaluated with various 0-best taggers.,
1139043706,false,finalized,3,1/24/2017 11:16:08,Neut,0.6541,W07-1033,"Our future plans include further sophistication of features, such as the use of external gazetteers which is reported to improve the F-score by 1.0 and 2.7 points in (Zhou and Su, 2004) and (Friedrich et al., 2006), respectively.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Zhou and Su, 2004)","We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase.","Our system showed clear improvement over many of the machine-learning-based systems reported to date, and also proved comparable to the existing state-ofthe-art systems that use rule-based post-processing.",
1139043707,false,finalized,3,1/24/2017 08:44:30,Pos,0.6444,W07-1033,"Our future plans include further sophistication of features, such as the use of external gazetteers which is reported to improve the F-score by 1.0 and 2.7 points in (Zhou and Su, 2004) and (Friedrich et al., 2006), respectively.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(Friedrich et al., 2006)","We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase.","Our system showed clear improvement over many of the machine-learning-based systems reported to date, and also proved comparable to the existing state-ofthe-art systems that use rule-based post-processing.",
1139043708,false,finalized,3,1/24/2017 11:43:08,Pos,0.689,W07-1033,"We also plan to apply self-training of n-best tagger which successfully boosted the performance of one of the best existing English syntactic parser (McClosky et al., 2006).",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf,"(McClosky et al., 2006)",,"We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase.",
1139043709,false,finalized,3,1/24/2017 09:04:17,Neut,0.6829,W03-1807,"The Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) built in Sheffield was used to evaluate our approach.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,"(Gaizauskas et al., 2001","In our evaluation, this approach extracted a total of 4,195 MWE candidates, of which, after manual checking, 3,792 were accepted as valid MWEs, producing a precision of 90.39% and an estimated recall of 39.38%.",We use an English semantic tagger (USAS) developed at Lancaster University to identify multiword units which depict single semantic concepts.,
1139043710,false,finalized,3,1/24/2017 11:26:49,Neut,0.6677,W03-1807,"The Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) built in Sheffield was used to evaluate our approach.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,"Clough et al., 2002)","In our evaluation, this approach extracted a total of 4,195 MWE candidates, of which, after manual checking, 3,792 were accepted as valid MWEs, producing a precision of 90.39% and an estimated recall of 39.38%.",We use an English semantic tagger (USAS) developed at Lancaster University to identify multiword units which depict single semantic concepts.,
1139043711,false,finalized,3,1/24/2017 11:31:28,Neut,1.0,W03-1807,"However, efficient extraction of MWEs still remains an unsolved issue, to the extent that Sag et al (2001b) call it ""a pain in the neck of NLP"".",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,(2001b),"In this paper, we present our work in which we approach the issue of MWE extraction by using a semantic field annotator.",A number of approaches have been suggested and tested to address this problem.,
1139043712,false,finalized,3,1/24/2017 11:44:41,Neut,0.6677,W03-1807,"We have drawn from the Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) a collection of British newspaper reports on court stories to evaluate our approach.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,"(Gaizauskas et al., 2001","Our experiment shows that it is efficient in identifying MWEs, in particular MWEs of low frequencies.",multiword expressions.,
1139043713,false,finalized,3,1/24/2017 10:18:27,Neut,0.6503,W03-1807,"We have drawn from the Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) a collection of British newspaper reports on court stories to evaluate our approach.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,"Clough et al., 2002)","Our experiment shows that it is efficient in identifying MWEs, in particular MWEs of low frequencies.",multiword expressions.,
1139043714,false,finalized,3,1/24/2017 10:18:53,Neut,1.0,W03-1807,"2001a, 2001b; Biber et al.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,"a, 2001b",2003).,1997; Michiels and Dufour 1998; Merkel and Andersson 2000; Piao and McEnery 2001; Sag et al.,
1139043715,false,finalized,3,1/24/2017 08:42:48,Neut,1.0,W03-1807,Such approaches include Dagan and Church's (1994) Termight Tool.,http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,(1994),"In this tool, they first collect candidate nominal terms with a POS syntactic pattern filter, then use concordances to identify frequently co-occurring multiword units.","In practice, most statistical approaches use linguistic filters to collect candidate MWEs.",
1139043716,false,finalized,3,1/24/2017 09:12:49,Neut,0.6741,W03-1807,"In his Xtract system, Smadja (1993) first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance, strength and spread, and then examined concordances of the bi-grams to find longer frequent multiword units.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,(1993),"Similarly, Merkel and Andersson (2000) compared frequency-based and entropy based algorithms, each of which was combined with a language filter.","In this tool, they first collect candidate nominal terms with a POS syntactic pattern filter, then use concordances to identify frequently co-occurring multiword units.",
1139043717,false,finalized,3,1/24/2017 11:52:47,Neut,0.6677,W03-1807,"Similarly, Merkel and Andersson (2000) compared frequency-based and entropy based algorithms, each of which was combined with a language filter.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,(2000),They reported that the entropy-based algorithm produced better results.,"In his Xtract system, Smadja (1993) first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance, strength and spread, and then examined concordances of the bi-grams to find longer frequent multiword units.",
1139043718,false,finalized,3,1/24/2017 11:51:57,Neut,1.0,W03-1807,"For example, Wu (1997) used an English-Chinese bilingual parser based on stochastic transduction grammars to identify terms, including multiword expressions.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,(1997),"In their DEFI Project, Michiels and Dufour (1998) used dictionaries to identify English and French multiword expressions and their translations in the other language.",Lexical resources and parsers are used to obtain better coverage of the lexicon in MWE extraction.,
1139043719,false,finalized,3,1/24/2017 11:29:07,Neut,1.0,W03-1807,"In their DEFI Project, Michiels and Dufour (1998) used dictionaries to identify English and French multiword expressions and their translations in the other language.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,(1998),Wehrli (1998) employed a generative grammar framework to identify compounds and idioms in their ITS-2 MT English-French system.,"For example, Wu (1997) used an English-Chinese bilingual parser based on stochastic transduction grammars to identify terms, including multiword expressions.",
1139043720,false,finalized,3,1/24/2017 11:40:11,Neut,1.0,W03-1807,Wehrli (1998) employed a generative grammar framework to identify compounds and idioms in their ITS-2 MT English-French system.,http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,(1998),Sag et al (2001b) introduced Head-driven Phrase Structure Grammar for analyzing MWEs.,"In their DEFI Project, Michiels and Dufour (1998) used dictionaries to identify English and French multiword expressions and their translations in the other language.",
1139043721,false,finalized,3,1/24/2017 11:33:16,Neut,1.0,W03-1807,Sag et al (2001b) introduced Head-driven Phrase Structure Grammar for analyzing MWEs.,http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,(2001b),"Like pure statistical approaches, purely knowledgebased symbolic approaches also face problems.",Wehrli (1998) employed a generative grammar framework to identify compounds and idioms in their ITS-2 MT English-French system.,
1139043722,false,finalized,3,1/24/2017 12:12:57,Neut,1.0,W03-1807,"As Sag et al (2001b) suggest, it is important to find the right balance between symbolic and statistical approaches.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,(2001b),"In this paper, we propose a new approach to MWEs extraction using semantic field information.",They are language dependent and not flexible enough to cope with complex structures of MWEs.,
1139043723,false,finalized,3,1/24/2017 11:16:08,Neut,0.6919,W03-1807,"Based on POS annotation provided by the CLAWS tagger (Garside and Smith, 1997), USAS assigns a set of semantic tags to each item in running text and then attempts to disambiguate the tags in order to choose the most likely candidate in each context.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,"(Garside and Smith, 1997)",Items can be single words or multiword expressions.,The USAS system has been in development at Lancaster University since 19901.,
1139043724,false,finalized,3,1/24/2017 10:28:29,Neut,0.6496,W03-1807,"The initial tagset was loosely based on Tom McArthur's Longman Lexicon of Contemporary English (McArthur, 1981) as this appeared to offer the most appropriate thesaurus type classification of word senses for this kind of analysis.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,"(McArthur, 1981)",The tagset has since been considerably revised in the light of practical tagging problems met in the course of the research.,The groups include not only synonyms and antonyms but also hypernyms and hyponyms.,
1139043725,false,finalized,3,1/24/2017 11:43:11,Neut,1.0,W03-1807,"We will list these only briefly here, since they are described in more detail elsewhere in a sentence, the following heuristics are ap(Garside and Rayson, 1997).",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,"(Garside and Rayson, 1997)",plied in sequence:,"As in the case of grammatical tagging, the task of semantic tagging subdivides broadly into two phases: Phase I (Tag assignment): attaching a set of potential semantic tags to each lexical unit and Phase II (Tag disambiguation): selecting the contextually appropriate semantic tag from the set provided by Phase I. USAS makes use of seven major techniques or sources of information in phase II.",
1139043726,false,finalized,3,1/24/2017 10:28:29,Neut,1.0,W03-1807,"It has been claimed (by Gale et al, 1992) on the basis of corpus analysis that to a very large extent a word keeps the same meaning throughout a text.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,"(by Gale et al, 1992)",6.,Text-based disambiguation.,
1139043727,false,finalized,3,1/24/2017 09:10:09,Neut,0.6815,W03-1807,"The Meter Corpus chosen as the test data is a collection of court reports from the British Press Association (PA) and some leading British newspapers (Gaizauskas 2001; Clough et al., 2002).",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,"Clough et al., 2002)","In our experiment, we used the newspaper part of the corpus containing 774 articles with more than 250,000 words.","Finally, we manually checked the results.",
1139043728,false,finalized,3,1/24/2017 10:30:50,Neut,1.0,W03-1807,"For example, Smadja (1993) suggests a basic characteristic of collocations and multiword units is recurrent, domain-dependent and cohesive lexical clusters.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,(1993),Sag et el.,We noticed various possible definitions have been suggested for MWE/MWU.,
1139043729,false,finalized,3,1/24/2017 11:29:07,Neut,1.0,W03-1807,"(2001b) suggest that MWEs can roughly be defined as ""idiosyncratic interpretations that cross word boundaries (or spaces)"".",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,(2001b),"Biber et al (2003) describe MWEs as lexical bundles, which they go on to define as combinations of words that can be repeated frequently and tend to be used frequently by many different speakers/writers within a register.",Sag et el.,
1139043730,false,finalized,3,1/24/2017 11:44:32,Neut,1.0,W03-1807,"Biber et al (2003) describe MWEs as lexical bundles, which they go on to define as combinations of words that can be repeated frequently and tend to be used frequently by many different speakers/writers within a register.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf,(2003),"Although it is not difficult to interpret these deifications in theory, things became much more complicated when we undertook our practical checking of the MWE candidates.","(2001b) suggest that MWEs can roughly be defined as ""idiosyncratic interpretations that cross word boundaries (or spaces)"".",

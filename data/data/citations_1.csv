marker,article,previous,current,next,link
"(Fellbaum, 1998)",W14-0118,1 Introduction,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).","Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Rada et al., 1989)",W14-0118,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).","Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Leacock and Chodorow, 1998)",W14-0118,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).","Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Wu and Palmer, 1994)",W14-0118,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).","Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Resnik, 1995)",W14-0118,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).","Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Lin, 1998)",W14-0118,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).","Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Jiang and Conrath, 1997)",W14-0118,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).","Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Pedersen et al., 2004)",W14-0118,"Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998).","Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were implemented in the WordNet::Similarity package (Pedersen et al., 2004).",WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(1965),W14-0118,WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,The toolkit was used to evaluate the different measures against a gold standard of English words created by Rubenstein and Goodenough (1965) and Miller and Charles (1991).,The evaluation results tell us something about the capacity of WordNet to mimic human judgements of similarity but also about the different methods in relation to each other.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(1991),W14-0118,WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations.,The toolkit was used to evaluate the different measures against a gold standard of English words created by Rubenstein and Goodenough (1965) and Miller and Charles (1991).,The evaluation results tell us something about the capacity of WordNet to mimic human judgements of similarity but also about the different methods in relation to each other.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Vossen, Soria and Monachini, 2013)",W14-0118,The evaluation results tell us something about the capacity of WordNet to mimic human judgements of similarity but also about the different methods in relation to each other.,"Unfortunately, WordNet::Similarity only works for the Princeton WordNet released in its proprietary format and not wordnets in other languages in other formats, such as Wordnet-LMF (Vossen, Soria and Monachini, 2013).","Furthermore, no gold standard exists for Dutch, the language that we study.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Vossen et al., 2013)",W14-0118,Section 3 explains how we created the Dutch gold standard and section 4 the WordnetTools implementation of the similarity functions.,"In section 5, we report the results using the Dutch wordnet Cornetto 2.1 (Vossen et al., 2013).",2 Related work,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(1998),W14-0118,"part-whole or causal relations, are most likely not similar but strongly related.","This difference is dubbed the 'tennisphenomenon' in Fellbaum (1998) : where tennis ball, player, racket and game are closely related but all very different things.","Since WordNet dominantly consists of synonymy and hyponymy relations, it more naturally reflects similarity than relatedness.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Lesk, 1986)",W14-0118,"Since the first release of WordNet, researchers have tried to use it to simulate similarity.","Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.",Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011).,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Patwardhan and Pedersen, 2006)",W14-0118,"Since the first release of WordNet, researchers have tried to use it to simulate similarity.","Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.",Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011).,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Patwardhan and Pedersen, 2006)",W14-0118,"Since the first release of WordNet, researchers have tried to use it to simulate similarity.","Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.",Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011).,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(2011),W14-0118,"Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy.",Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011).,"The aim of their paper is to show that it might be possible to use the scores from the English gold standards in other languages, hence making it unnecessary to create gold standards with human-assigned judgements in every single language.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(2011),W14-0118,"The difference between these correlations was relatively small, which is why they claim that it is possible to use the original scores from the English gold standard in other languages.","Besides Joubarne and Inkpen (2011), other studies have made an effort to translate the original datasets by Rubenstein & Goodenough and by Miller & Charles.","Hassan and Mihalcea (2009) translated these datasets into Spanish, Arabic, and Romanian.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(2009),W14-0118,"Besides Joubarne and Inkpen (2011), other studies have made an effort to translate the original datasets by Rubenstein & Goodenough and by Miller & Charles.","Hassan and Mihalcea (2009) translated these datasets into Spanish, Arabic, and Romanian.","For Spanish, native speakers, who were highly proficient in English, were asked to translate the datasets.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(2005),W14-0118,"Because of the fact that the Pearson correlation with the original datasets was 0.86, only one translator translated the datasets into Arabic and Romanian.","Finally, Gurevych (2005) translated the datasets into German.","However, no instructions, as to how it was done, were provided.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(2009),W14-0118,"Whenever Rubenstein & Goodenough used the word cord, Miller & Charles uses the word chord.","Inspired by Hassan and Mihalcea (2009), the following general procedure is followed in the translation of the 49 words: 2",1.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Miller et al., 1993)",W14-0118,"In addition, the relative frequencies of the English word and its translation were checked.","In order to calculate relative frequencies of the English words, the English sense-tagged corpus SemCor (Miller et al., 1993) was used.","For Dutch, such a resource was not available.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Vossen et al., 2012)",W14-0118,"For Dutch, such a resource was not available.","We are aware of the fact that the Dutch sense-tagged corpus DutchSemCor (Vossen et al., 2012) exists.","However, an effort was made to provide an equal number of examples for each meaning in this corpus.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Oostdijk et al., 2008)",W14-0118,"Although this is very useful for WSD-experiments, this makes this corpus less useful for Information Content calculations.","Therefore the frequencies of the lemmas in the Dutch corpus called SoNaR (Oostdijk et al., 2008) were used.",It was checked whether or not the English word and its Dutch counterpart were located in the same class of relative frequency.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Finkelstein et al., 2002)",W14-0118,These instructions were explained to the participants by an example of each value that could be assigned to a word pair and a general description.,"The WordSimilarity-353 Test Collection (Finkelstein et al., 2002) was used to obtain example word pairs for each value that could be assigned to a word pair.",This dataset contains two sets of English word pairs with similarity scores assigned by humans.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(2010),W14-0118,"We start by comparing the Dutch to the English gold standards, followed by an evaluation of the comparison between the Dutch gold standards and the similarity measures.","Finally, we try to replicate the English experiment by Pedersen (2010) using English Wordnet-LMF and WordnetTools.",5,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(2010),W14-0118,can we reproduce the results of WordNet::Similarity with the original WordNet database with WordnetTools with the WordnetLMF version of the English WordNet.,"In order to do this, we compare the correlations that Pedersen (2010) reports when calculating the correlations between the original gold standards and the scores from the six similarity measures using WordNet::Similarity to the same procedure but using the WordNetTools to compute the similarity scores.","We used the following settings for WordNetTools:7 -lmf-file Path to WordNet in LMF format -pos no pos-filter was used -relations has hypernym, has hyperonym, -input path to English gold standards -pairs ""words"" -method all.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(2010),W14-0118,SM McPed McWT diff RgPed RgWT diff path 0.68 0.72 -0.04 0.69 0.78 -0.09 lch 0.71 0.72 -0.01 0.70 0.78 -0.08 wup 0.74 0.74 0.00 0.69 0.78 -0.09 res 0.74 0.75 -0.01 0.69 0.76 -0.07 jcn 0.72 0.65 0.07 0.51 0.56 -0.05 lin 0.73 0.67 0.06 0.58 0.60 -0.02,Table 3: Comparison of the results by Pedersen (2010) and the replication of these results using Wordnet-LMF and the WordnetToolkit,"7The depth parameter is set to 19, For more information, we refer to section 6.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(2010),W14-0118,"7The depth parameter is set to 19, For more information, we refer to section 6.","The results show that for both gold standards, we approach the correlations that are reported by Pedersen (2010), but that there are probably still differences in the implementation of the measures that lead to different output values.",6 Discussion,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(2009),W14-0118,"Firstly, the correlations between the English and Dutch gold standards are very high.","Given the fact that this was also the case for the Spanish and English intuitions, as discussed by Hassan and Mihalcea (2009), it might be the case the people with different mother tongues have a shared sense of similarity of meaning.",It should be noted that all speakers from the different languages share a similar Western background.,http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(2013),W14-0118,"Finally, the differences between the scores from the WordNet::Similarity package and the WordNetTools show that we did not reproduce the results exactly.","This in itself is not surprising, given the fact that Fokkens et al (2013) showed that even replicating the results that Pedersen (2010) reports can be challenging.","They showed that even if the main properties are kept stable, such as software and versions of software, variations in minor properties can lead to completely different outcomes.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
(2010),W14-0118,"Finally, the differences between the scores from the WordNet::Similarity package and the WordNetTools show that we did not reproduce the results exactly.","This in itself is not surprising, given the fact that Fokkens et al (2013) showed that even replicating the results that Pedersen (2010) reports can be challenging.","They showed that even if the main properties are kept stable, such as software and versions of software, variations in minor properties can lead to completely different outcomes.",http://www.aclweb.org/anthology/W/W14/W14-0118.pdf
"(Fellbaum, 1998)",S13-2019,Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.,"One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).",Our phrasal semantic relatedness approach is inspired from those methods.,http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Leacock and Chodorow, 1998",S13-2019,Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.,"One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).",Our phrasal semantic relatedness approach is inspired from those methods.,http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"Wu and Palmer, 1994)",S13-2019,Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.,"One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).",Our phrasal semantic relatedness approach is inspired from those methods.,http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Hirst and St-Onge, 1998)",S13-2019,Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.,"One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).",Our phrasal semantic relatedness approach is inspired from those methods.,http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Tsatsaronis et al., 2010)",S13-2019,Knowledge-based approaches to semantic relatedness use the features of the knowledge base to measure the relatedness.,"One of most frequently used semantic network is the Princeton's WordNet (Fellbaum, 1998) which groups words into synonyms sets (called synsets) and includes 26 semantic relations between those synsets, including: hypernymy, hyponymy, meronymy, entailment ... To measure relatedness, most of those approaches rely on the structure of the semantic network, such as the semantic link path, depth (Leacock and Chodorow, 1998; Wu and Palmer, 1994), direction (Hirst and St-Onge, 1998), or type (Tsatsaronis et al., 2010).",Our phrasal semantic relatedness approach is inspired from those methods.,http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Harabagiu et al., 1999)",S13-2019,Figure 1: Example of the semantic network around the word car.,"ponym/hypernym relations but also all 26 available semantic relations found in WordNet in addition to relations extracted from each of the eXtended WordNet (Harabagiu et al., 1999) synset's logical form.","To implement our idea, we created a weighted and directed semantic network based on the relations of WordNet and eXtended WordNet.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Dijkstra, 1959)",S13-2019,1The weight can be seen as the cost of traversing an edge; hence a lower weight is assigned to a highly contributory relation.,"2The shortest path is based on an implementation of Dijkstras graph search algorithm (Dijkstra, 1959)",109,http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Harris, 1954)",S13-2019,2.2 Distributional Similarity Model,"Distributional similarity models rely on the distributional hypothesis (Harris, 1954) to represent a word by its context in order to compare word semantics.","There are various approach for the selection, representation, and comparison of contextual data.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
(2008),S13-2019,"Most use the vector space model to represent the context as dimensions in a vector space, where the feature are frequency of co-occurrence of the context words, and the comparison is usually the cosine similarity.","To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).","In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
(2008),S13-2019,"Most use the vector space model to represent the context as dimensions in a vector space, where the feature are frequency of co-occurrence of the context words, and the comparison is usually the cosine similarity.","To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).","In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
(2011),S13-2019,"Most use the vector space model to represent the context as dimensions in a vector space, where the feature are frequency of co-occurrence of the context words, and the comparison is usually the cosine similarity.","To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).","In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008).",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
(2008),S13-2019,"To go beyond lexical semantics and to represent phrases, a compositional model is created, some use the addition or multiplication of vectors such as Mitchell and Lapata (2008), or the use of tensor product to account for word order as in the work of Widdows (2008), or a more complex model as the work of Grefenstette and Sadrzadeh (2011).","In our model, we are inspired by those various work, and more specifically by the work of Mitchell and Lapata (2008).","The compositional model is based on phrase words vectors addition, where each vector is composed of the collocation pointwise mutual information of the word up to a window of 3 words left and right of the main word.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Brants and Franz, 2006)",S13-2019,"The compositional model is based on phrase words vectors addition, where each vector is composed of the collocation pointwise mutual information of the word up to a window of 3 words left and right of the main word.","The corpus used to collect the features and their frequencies is the Web 1TB corpus (Brants and Franz, 2006).","For the Interview to Formal Meeting example, the vector of the word interview is first created from the corpus of the top 1000 words collocating interview between the window of 1 to 3 words with their frequencies.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Witten et al., 1999)",S13-2019,"The task provided an English dataset of 15,628 word-phrases, 60% annotated for training and 40% for testing, with the goal of classifying each word-phrase as either positive or negative.","To transform the semantic relatedness measure to a semantic similarity classification one, we first calculated the semantic relatedness of each word-phrase in the training set, and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999), in order to learn a set of rules that can differentiate between a positive semantic similarity and a negative one.","The classifier resulted in rules for the semantic network model based relatedness that could be summarized as follows: If the semantic relatedness of the word-phrase is over 61% then the similarity is positive, otherwise it is negative.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Cohen and Singer, 1999)",S13-2019,"The task provided an English dataset of 15,628 word-phrases, 60% annotated for training and 40% for testing, with the goal of classifying each word-phrase as either positive or negative.","To transform the semantic relatedness measure to a semantic similarity classification one, we first calculated the semantic relatedness of each word-phrase in the training set, and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999), in order to learn a set of rules that can differentiate between a positive semantic similarity and a negative one.","The classifier resulted in rules for the semantic network model based relatedness that could be summarized as follows: If the semantic relatedness of the word-phrase is over 61% then the similarity is positive, otherwise it is negative.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Brants and Franz, 2006)",S13-2019,"For example, the phrase big picture is used literally in the sentence Click here for a bigger picture and figuratively in To solve this problem, you have to look at the bigger picture.","Our approach for this task is a supervised approached based on two main components: first, the availability of the phrases most frequent collocating expressions in a large corpus, and more specifically the top 1000 phrases by frequency in Web 1TB corpus (Brants and Franz, 2006).","For example, for the phrase big picture, we collect the top 1000 phrases that come before and after the phrase in a corpus, those includes look at the, see the, understand the .....",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Witten et al., 1999)",S13-2019,"The data set contains a total of 1114 training instances, and 518 test instances.","We use the training data and computed the three features (Frequent Collocation (FC), Semantic Relatedness word Before (SRB), and Semantic Relatedness word After (SRA), and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999) to learn a set of rule that differentiate between a figurative and literal phrase use.","This method resulted in a set of rules that can be summarized as follows: if FC is equal to 0 and SRB < 75% then it is used literally in this context, else if FC is equal to 0 and SRA < 75% then it is is also used literally, otherwise it is used figuratively.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Cohen and Singer, 1999)",S13-2019,"The data set contains a total of 1114 training instances, and 518 test instances.","We use the training data and computed the three features (Frequent Collocation (FC), Semantic Relatedness word Before (SRB), and Semantic Relatedness word After (SRA), and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999) to learn a set of rule that differentiate between a figurative and literal phrase use.","This method resulted in a set of rules that can be summarized as follows: if FC is equal to 0 and SRB < 75% then it is used literally in this context, else if FC is equal to 0 and SRA < 75% then it is is also used literally, otherwise it is used figuratively.",http://www.aclweb.org/anthology/S/S13/S13-2019.pdf
"(Nagata et al, 2001, ",P08-1113,There have been two approaches to finding such parenthetical translations.,"One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005).","Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"Kwok et al, 2005)",P08-1113,There have been two approaches to finding such parenthetical translations.,"One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005).","Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(Cao et al, 2007)",P08-1113,"One is to assume that the English term e1e2...en is given and use a search engine to retrieve text snippets containing e1e2...en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005).","Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4).",We followed the second approach since it does not require a predefined list of English terms and is amendable for extraction at large scale.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(Kwok et al, 2005)",P08-1113,"In both cases, one can obtain a list of candidate pairs, where the translation of the in-parenthesis terms is a suffix of the pre-parenthesis text.","The lengths and frequency counts of the suffixes have been used to determine what is the translation of the in-parenthesis term (Kwok et al, 2005).","For example, Table 1 lists a set of Chinese segments (with word-to-word translation underneath) that precede the English term Lower Egypt.",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(MacArthur, 1967)",P08-1113,The example in row f is ruled out because '/' is not found in the pre-parenthesis text.,"Examples with translations in Function of the initalic parenthesis text a ARTi3l9t1.4~3.0ZN to provide citation (MacArthur, 1967) The range of its values is within","1.4~3.0 (MacArthur, 1967) b cJILS/ (VN901 flight information 15:20-22:30) Vietnam Airlines Beijing/Ho Chi Minh (VN901 15:20-22:30) c ' Af*#(255-8FT) product Id.",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(MacArthur, 1967)",P08-1113,"Examples with translations in Function of the initalic parenthesis text a ARTi3l9t1.4~3.0ZN to provide citation (MacArthur, 1967) The range of its values is within","1.4~3.0 (MacArthur, 1967) b cJILS/ (VN901 flight information 15:20-22:30) Vietnam Airlines Beijing/Ho Chi Minh (VN901 15:20-22:30) c ' Af*#(255-8FT) product Id.","sale of pool table (255-8FT) d // _tV1T_- // void main ( void ) function declaration // main program // void main (void ) e *,*,?",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(Sag et al, 2003)",P08-1113,"Since parenthetical translations are mostly translation of terms, it makes sense to further constrain the left boundary of the Chinese side to be a term boundary.","Determining what should be counted as a term is a difficult task and there are not yet well-accepted solutions (Sag et al, 2003).",We compiled an approximate term vocabulary by taking the top 5 million most frequent Chinese queries as according to a fully anonymized collection of search engine query logs.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(Brown et al, 1993",P08-1113,4 Word Alignment,"Word alignment is a well-studied topic in Machine Translation with many algorithms having been proposed (Brown et al, 1993; Och and Ney 2003).","We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(Melamed, 2000)",P08-1113,"Word alignment is a well-studied topic in Machine Translation with many algorithms having been proposed (Brown et al, 1993; Och and Ney 2003).","We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000).",The algorithm assumes that there is a score associated with each pair of words in a bi-text.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
(2004),P08-1113,The algorithm terminates when there are no more links to make.,Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest precision scores.,"A disadvantage of Competitive Linking, however, is that the alignments are restricted word-to-word alignments, which implies that multi-word expressions can only be partially linked at best.",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(Gale and Church, 1991)",P08-1113,4.2 Link scoring,"We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel.","2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005).",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(Dunning, 1993)",P08-1113,"We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel.","2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005).",The T2 statistics for a pair of words e; and fj is computed as,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(Taskar et al, 2005)",P08-1113,"We used T2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as X2 (Zhang, S. Vogel.","2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005).",The T2 statistics for a pair of words e; and fj is computed as,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(Cao et al, 2007",P08-1113,"For example, the word happens to have a similar 2 score with Shapiro as the word  (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus.","Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007).",They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"Jiang et al, 2007",P08-1113,"For example, the word happens to have a similar 2 score with Shapiro as the word  (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus.","Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007).",They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"Wu and Chang, 2007)",P08-1113,"For example, the word happens to have a similar 2 score with Shapiro as the word  (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus.","Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007).",They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
(2007),P08-1113,5.2 Evaluation with term translation requests,"To evaluate the coverage of output produced by their method, Cao et al (2007) extracted English queries from the query log of a Chinese search engine.",They assume that the reason why users typed the English queries in a Chinese search box is mostly to find out their Chinese translations.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(Cao et al., 2007)",P08-1113,We use * to mark incorrect translations.,"When compared with the sample queries in (Cao et al., 2007), the queries in our sample seem to contain more phrasal words and technical terminology.","It is interesting to see that even though parenthetical translations tend to be out-of-vocabulary words, as we have remarked in the introduction, the sheer size of the web means that occasionally translations of common words such as 'use' are sometimes included as well.",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(Koehn et al, 2003",P08-1113,The extracted translations may serve as training data for statistical machine translation systems.,"To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003).",We then added the extracted translation pairs as additional parallel training corpus.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"Brants et al, 2007)",P08-1113,The extracted translations may serve as training data for statistical machine translation systems.,"To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003).",We then added the extracted translation pairs as additional parallel training corpus.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(NIST, 2003)",P08-1113,The extracted translations may serve as training data for statistical machine translation systems.,"To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003).",We then added the extracted translation pairs as additional parallel training corpus.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
(2001),P08-1113,6 Related Work,Nagata et al (2001) made the first proposal to mine translations from the web.,"Their work was concentrated on terminologies, and assumed the English terms were given as input.",http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
(2007),P08-1113,"Their work was concentrated on terminologies, and assumed the English terms were given as input.","Wu and Chang (2007), Kwok et al (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration.",It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
(2005),P08-1113,"Their work was concentrated on terminologies, and assumed the English terms were given as input.","Wu and Chang (2007), Kwok et al (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration.",It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
(2007),P08-1113,It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by.,"Cao et al (2007), like us, used a 300GB collection of web documents as input.",They used supervised learning to build models that deal with phonetic transliterations and semantic translations separately.,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"(Cao et al., 2007)",P08-1113,Our work relies on unsupervised learning and does not make a distinction between translations and transliterations.,"Furthermore, we are able to extract two orders of magnitude more translations from than (Cao et al., 2007).",7 Conclusion,http://www.aclweb.org/anthology/P/P08/P08-1113.pdf
"Traum and Hinkelmau, 1992]",W93-0235,The cmly difference between speech acts and rhetorical relations is that the latter are explicitly concerned with the linkage of separate segments of language.,"[Traum and Hinkelmau, 1992] presents a multistratal theory of Conversation Acts,",This material is based upon work supported in part by the NSF under research grant no.,http://www.aclweb.org/anthology/W/W93/W93-0235.pdf
"Bratman, 1990]",W93-0235,Intentions are commitments towards a course of action.,"[Bratman, 1990] discusses three roles that intention plays in deliberative behavior: serving as a motivation for planning, a ""filter of admissibility"" on plans and further intentions, and a controller of conduct, motivating execution monitoring and repair and replanning when necessary.","Rhetorical relations are thus actions in the world distinguished by conditions on their occurrence and effects, which will generally be changes to the conversational state and the beliefs of the conversants.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf
"Austin, 1962]",W93-0235,"As with other actions, relations can be performed intentionally or incidentally.","Of intentional actions, it is also possible to draw the distinction made in speech act theory between illocutionary acts, those in which part of the intended effect includes an awareness on the part of the hearer of this intention, and perlocutionary acts, in which it is only the effect that matters and not recognition of the intention [Austin, 1962].","For non-illocutionary acts, the intention of the speaker is not relevant - these actions can be produced as side-effects of the speaker's intention, so that a determination of the intention is not necessary to determining whether the act was performed.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf
"Mann and Thompson, 1987]",W93-0235,"For non-illocutionary acts, the intention of the speaker is not relevant - these actions can be produced as side-effects of the speaker's intention, so that a determination of the intention is not necessary to determining whether the act was performed.","For example, the evidence relation of [Mann and Thompson, 1987] may hold between two text spans even if the speaker did not intend such a relation, all that is required is that the hearer's belief in the nucleus is increased though the understanding of the satellite.","For an illocutionary act, on the other hand, the recognition of communicative intention is crucial to understanding.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf
"Cohen and Levesque, 1990",W93-0235,It is on the following point that the main criticism of bounded sets of speech acts or rhetorical relations (e.g.,"[Cohen and Levesque, 1990; Grosz and Sidner, 1986; Lochbaum; hughes and McCoy]) is centered: intuitively, all that is needed for successful communication is that the hearer understand the speaker's end intentions, not that the act types themselves be recognized.","This intuition, along with the lack of general agreement on the precise set of acts or relations lead some to reject the utility of relations altogether and concentrate only On intentions.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf
"Grosz and Sidner, 1986",W93-0235,It is on the following point that the main criticism of bounded sets of speech acts or rhetorical relations (e.g.,"[Cohen and Levesque, 1990; Grosz and Sidner, 1986; Lochbaum; hughes and McCoy]) is centered: intuitively, all that is needed for successful communication is that the hearer understand the speaker's end intentions, not that the act types themselves be recognized.","This intuition, along with the lack of general agreement on the precise set of acts or relations lead some to reject the utility of relations altogether and concentrate only On intentions.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf
"Allen and Schubert, 1991]",W93-0235,Rhetorical Relations in the TRAINS System,"In the TRAINS Conversation System implementation [Allen and Schubert, 1991], we take a fairly pragmatic approach towards rhetorical relations.",Those relations that are conventionally signalled by surface features (e.g.,http://www.aclweb.org/anthology/W/W93/W93-0235.pdf
"Heeman, 1993]",W93-0235,Those relations that are conventionally signalled by surface features (e.g.,"by clue words such as ""so"", ""no"", ""okay"", purpose clauses) are hypothesized by the Speech Act Interpreter [Heeman, 1993] and used by the Dialogue Manager [Traum, 1993] to guide further interpretation.","In the case of more implicit relationships we often do not identify the precise relation, merely operating on the speech act level forms.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf
"Traum, 1993]",W93-0235,Those relations that are conventionally signalled by surface features (e.g.,"by clue words such as ""so"", ""no"", ""okay"", purpose clauses) are hypothesized by the Speech Act Interpreter [Heeman, 1993] and used by the Dialogue Manager [Traum, 1993] to guide further interpretation.","In the case of more implicit relationships we often do not identify the precise relation, merely operating on the speech act level forms.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf
"Ferguson and Allen, 1993]",W93-0235,This is particularly true for subject matter relations.,"For example, a. purpose clause is useful for the domain plan recognizer [Ferguson and Allen, 1993] in incorporating new content into an existing (partial) plan, but in the absence of such a cue, the recognizer will still try to connect the new content to previous content.","It would then be possible to deduce the relations that this item holds with previous items, but we currently see no need to do this.",http://www.aclweb.org/anthology/W/W93/W93-0235.pdf
"(Attali and Burstein, 2006",W12-2011,1 Introduction and Motivation,"Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).","One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"Yannakoudakis et al., 2011)",W12-2011,1 Introduction and Motivation,"Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).","One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Rozovskaya and Roth, 2011",W12-2011,"Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).","One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).","Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"Tetreault and Chodorow, 2008)",W12-2011,"Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).","One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).","Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Dale and Kilgarriff, 2011)",W12-2011,"Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011).","One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).","Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011).",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Dickinson et al., 2011)",W12-2011,"One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011).","Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011).","In our work, we extend the task to predicting the learner's level based on the errors, focusing on Hebrew.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Fulcher, 1997)",W12-2011,"Placing learners into levels is generally done by a human, based on a written exam (e.g.","(Fulcher, 1997)).","To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Hawkins and Filipovic, 2010",W12-2011,"To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels.","There is only little work investigating this correspondence formally (see (Hawkins and Filipovic, 2010; Alexopoulou et al., 2010) for discussion) and only on error-annotated English learner corpora.","For this reason, we follow a data-driven approach to learn the correspondence between errors and levels, based on exercises from written placement exams.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"Alexopoulou et al., 2010)",W12-2011,"To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels.","There is only little work investigating this correspondence formally (see (Hawkins and Filipovic, 2010; Alexopoulou et al., 2010) for discussion) and only on error-annotated English learner corpora.","For this reason, we follow a data-driven approach to learn the correspondence between errors and levels, based on exercises from written placement exams.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Goldberg and Elhadad, 2011",W12-2011,"The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95-104, Montreal, Canada, June 3-8, 2012. c2012 Association for Computational Linguistics","learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions.","For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"Yona and Wintner, 2008",W12-2011,"The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95-104, Montreal, Canada, June 3-8, 2012. c2012 Association for Computational Linguistics","learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions.","For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"Itai and Wintner, 2008)",W12-2011,"The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95-104, Montreal, Canada, June 3-8, 2012. c2012 Association for Computational Linguistics","learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions.","For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Dickinson, 2011)",W12-2011,e.g.,"(Dickinson, 2011)).","An error could feature, for instance, a letter inserted in an irregular verb stem, or between two nouns; any of these properties may be relevant to describing the error (cf.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Diaz-Negrillo and Fernandez-Dominguez, 2006",W12-2011,"how errors are described in different taxonomies, e.g.","(Diaz-Negrillo and Fernandez-Dominguez, 2006; Boyd, 2010)).","Specific error types are unlikely to recur, making sparsity even more of a concern.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"Boyd, 2010)",W12-2011,"how errors are described in different taxonomies, e.g.","(Diaz-Negrillo and Fernandez-Dominguez, 2006; Boyd, 2010)).","Specific error types are unlikely to recur, making sparsity even more of a concern.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Fulcher, 1997)",W12-2011,"The system is intended to semi-automatically place incoming students into the appropriate Hebrew course, i.e., level.","As with many exams, the main purpose is to ""reduce the number of students who attend an oral interview"" (Fulcher, 1997).",2 The Data,http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"an et al., 2001)",W12-2011,(1) barC beph dibrw hybrit ieral la tmid beph in-the-language hybrit the-Hebrew,"1We follow the transliteration scheme of the Hebrew Treebank (Sima'an et al., 2001).",'They did not always speak in the Hebrew language in the land of Israel.',http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Yannakoudakis et al., 2011)",W12-2011,"While features can be based on individual phenomena of any type, we base our extracted features largely upon learner errors.","Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al., 2011).","To detect errors, we align the learner sentence with a gold standard and extract the features.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"e et al., 2010)",W12-2011,This method is reminiscent of alignment approaches in paraphrasing (e.g.,"(Crigonyt`e et al., 2010)), but note that our problem is more restricted in that we have the same number of words, and in most cases identical words.","We use different distance and similarity metrics, to ensure robustness across different kinds of errors.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Hawkins and Buttery, 2010)",W12-2011,The second approach counteracts this confusion by selecting the most prototypical level for an individual phenomenon (cf.,"criterial features in (Hawkins and Buttery, 2010)), giving less noise to phase 2.","We may lose important non-best level information, but as we show in sec.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Hawkins and Buttery, 2010)",W12-2011,We aggregate the information from phase 1 classification to classify overall learner levels.,"We assume that the set of all individual phenomena and their quantities (e.g., proportion of phenomena classified as 200-level in phase 1) characterize a learner's level (Hawkins and Buttery, 2010).",The feature types are given in table 4.,http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Daelemans et al., 2010",W12-2011,6.1 Details of the experiments,"We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999), a memory-based learner (MBL), for both phases.","We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001); allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"Daelemans et al., 1999)",W12-2011,6.1 Details of the experiments,"We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999), a memory-based learner (MBL), for both phases.","We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001); allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Banko and Brill, 2001)",W12-2011,"We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999), a memory-based learner (MBL), for both phases.","We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001); allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space.","We mostly use the default settings of TiMBL-the IB1 learning algorithm and overlap comparison metric between instances-and experiment with different values of k. For prediction of phenomenon level (phase 1) and learner level (phase 2), the system is trained on data from placement exams previously collected in a Hebrew language program, as described in sec.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Yannakoudakis et al., 2011)",W12-2011,e.g.,"(Yannakoudakis et al., 2011))-we can determine their relative importance and usefulness.",We run phase 2 (k = 1) using different combinations of phase 1 classifiers (1-best) as input.,http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Fulcher, 1997)",W12-2011,e.g.,"(Fulcher, 1997)), while at the same time incorporating more linguistic processing for more complex input.","For example, with question formation exercises, no closed set of correct answers exists, and one must use parse tree distance to delineate features.",http://www.aclweb.org/anthology/W/W12/W12-2011.pdf
"(Brown et al., 1990)",N04-1023,1 Introduction,"The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.","Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Och and Ney, 2002)",N04-1023,"The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.","Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.","In this paper, we introduce two novel machine learning algorithms specialized for the MT task.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Och, 2003)",N04-1023,"The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.","Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.","In this paper, we introduce two novel machine learning algorithms specialized for the MT task.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Brown et al., 1990)",N04-1023,1.1 Generative Models for MT,"The seminal IBM models (Brown et al., 1990) were the first to introduce generative models to the MT task.",The IBM models applied the sequence learning paradigm well-known from Hidden Markov Models in speech recognition to the problem of MT.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
(1998),N04-1023,Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality.,Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments.,"Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Och and Weber, 1998",N04-1023,"Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders.","In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation.","However, phrase level alignment cannot handle long distance reordering effectively.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"Och et al., 1999)",N04-1023,"Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders.","In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation.","However, phrase level alignment cannot handle long distance reordering effectively.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
(1997),N04-1023,Parse trees have also been used in alignment models.,Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form.,"(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Wu, 1997)",N04-1023,Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form.,"(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank.",Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
(2001),N04-1023,"(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank.",Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment.,Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
(2003),N04-1023,Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment.,Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages.,The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
(2002),N04-1023,1.2 Discriminative Models for MT,"Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model estimated using a maximum entropy model.",A small number of feature functions defined on the source and target sentence were used to rerank the translations generated by a baseline MT system.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
(2003),N04-1023,"While the total number of feature functions was small, each feature function was a complex statistical model by itself, as for example, the alignment template feature functions used in this approach.",Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU.,The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Och and Ney, 2002)",N04-1023,The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation.,"This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).","SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
(2003),N04-1023,"This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).","SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.",More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
(2003),N04-1023,"This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).","SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.",More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
(2003),N04-1023,More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output.,"By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%.",2 Ranking and Reranking,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Papineni et al., 2001)",N04-1023,More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output.,"By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%.",2 Ranking and Reranking,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Collins, 2000)",N04-1023,"In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing.","Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).",The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Collins and Duffy, 2002)",N04-1023,"In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing.","Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).",The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Shen and Joshi, 2003)",N04-1023,"In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing.","Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).",The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Shen and Joshi, 2004)",N04-1023,The reranking problem is reduced to a classification problem by using pairwise samples.,"In (Shen and Joshi, 2004), we have introduced a new perceptron-like ordinal regression algorithm for parse reranking.","In that algorithm, pairwise samples are used for training and margins are defined as the distance between parses of different ranks.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Crammer and Singer, 2001",N04-1023,Two large margin approaches have been used.,"One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003).","However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"Harrington, 2003)",N04-1023,Two large margin approaches have been used.,"One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003).","However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Herbrich et al., 2000)",N04-1023,"However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks.","The other approach is to reduce the ranking problem to a classification problem by using the method of pairwise samples (Herbrich et al., 2000).",The underlying assumption is that the samples of consecutive ranks are separable.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Collins, 2000)",N04-1023,"Although we might invent metrics to define the quality of a translation, standard reranking algorithms cannot be directly applied to MT.","In parse reranking, each training sentence has a ranked list of 27 candidates on average (Collins, 2000), but for machine translation, the number of candidate translations in the -best list is much higher.","(SMT Team, 2003) show that to get a reasonable improvement in the BLEU score at least 1000 candidates need to be considered in the -best list.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(SMT Team, 2003)",N04-1023,"In parse reranking, each training sentence has a ranked list of 27 candidates on average (Collins, 2000), but for machine translation, the number of candidate translations in the -best list is much higher.","(SMT Team, 2003) show that to get a reasonable improvement in the BLEU score at least 1000 candidates need to be considered in the -best list.","In addition, the parallel hyperplanes separating and actually are unable to distinguish good translations from bad translations, since they are not trained to distinguish any translations in .",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Vapnik, 1998)",N04-1023,3.6 Large Margin Classifiers,"There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).",The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Schapire et al., 1997)",N04-1023,3.6 Large Margin Classifiers,"There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).",The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Zhang, 2000)",N04-1023,3.6 Large Margin Classifiers,"There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).",The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Krauth and Mezard, 1987)",N04-1023,3.6 Large Margin Classifiers,"There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).",The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Shen and Joshi, 2003)",N04-1023,"However, SVMs are extremely slow in training since they need to solve a quadratic programming search.","For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003).","Taking this into account, we use perceptron-like algorithms, since the perceptron algorithm is fast in training which allow us to do experiments on real-world data.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Herbrich et al., 2000)",N04-1023,"However, the size of generated training samples will be very large.","For samples, the total number of pairwise samples in (Herbrich et al., 2000) is roughly .","In the next section, we will introduce two perceptron-like algorithms that utilize pairwise samples while keeping the complexity of data space unchanged.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Shen and Joshi, 2004)",N04-1023,classification algorithm.,"We also experimented with an ordinal regression algorithm proposed in (Shen and Joshi, 2004).","For the sake of completeness, we will briefly describe the algorithm here.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Shen and Joshi, 2004)",N04-1023,4.2 Ordinal Regression,"The second algorithm that we will use for MT reranking is the -insensitive ordinal regression with uneven margin, which was proposed in (Shen and Joshi, 2004), as shown in Algorithm 2.","In Algorithm 2, the function is used to control the level of insensitivity, and the function is used to control the learning margin between pairs of translations with different ranks as described in Section 3.5.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(SMT Team, 2003)",N04-1023,We provide experimental results on the NIST 2003 Chinese-English large data track evaluation.,"We use the data set used in (SMT Team, 2003).","The training data consists of about 170M English words, on which the baseline translation system is trained.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(SMT Team, 2003)",N04-1023,do 4: compute and for all ; 5: for ( ) do 6: if and and then,"Table 1: BLEU scores reported in (SMT Team, 2003).",Every single feature was combined with the 6 baseline features for the training and test.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Och, 2003)",N04-1023,Every single feature was combined with the 6 baseline features for the training and test.,"The minimum error training (Och, 2003) was used on the development data for parameter estimation.",Feature BLEU% Baseline 31.6 POS Language Model 31.7 Supertag Language Model 31.7 Wrong NN Position 31.7 Word Popularity 31.8 Aligned Template Models 31.9 Count of Missing Word 31.9 Template Right Continuity 32.0 IBM Model 1 32.5,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(SMT Team, 2003)",N04-1023,The test set is used to assess the quality of the reranking output.,"In (SMT Team, 2003), 450 features were generated.","Six features from (Och, 2003) were used as baseline features.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Och, 2003)",N04-1023,"In (SMT Team, 2003), 450 features were generated.","Six features from (Och, 2003) were used as baseline features.",Each of the 450 features was evaluated independently by combining it with 6 baseline features and assessing on the test data with the minimum error training.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(SMT Team, 2003)",N04-1023,Table 1 shows some of the best performing features.,"In (SMT Team, 2003), aggressive search was used to combine features.","After combining about a dozen features, the BLEU score did not improve any more, and the score was 32.9%.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Och, 2003)",N04-1023,"By combining the four features, Model 1, matched parentheses, matched quotation marks and POS language model, the system achieved a BLEU score of 32.6%.","In our experiments, we will use 4 different kinds of feature combinations: Baseline: The 6 baseline features used in (Och, 2003), such as cost of word penalty, cost of aligned template penalty.",Best Feature: Baseline + IBM Model 1 + matched parentheses + matched quotation marks + POS language model.,http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Shen and Joshi, 2004)",N04-1023,"In addition, generalization performance in the Top Twenty set is better than the Large Set due to the smaller set of ""better"" features, cf.","(Shen and Joshi, 2004).","If the number of the non-discriminative features is large enough, the data set becomes unsplittable.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Li et al., 2002)",N04-1023,"If the number of the non-discriminative features is large enough, the data set becomes unsplittable.","We have tried using the trick as in (Li et al., 2002) to make data separable artificially, but the performance could not be improved with such features.","We achieve similar results with Algorithm 2, the ordinal regression with uneven margin.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(SMT Team, 2003)",N04-1023,This algorithm does not converge on the Large Set in 10000 iterations.,"We compare our perceptron-like algorithms with the minimum error training used in (SMT Team, 2003) as shown in Table 2.","The splitting algorithm achieves slightly better results on the Baseline and the Best Feature set, while the minimum error training and the regression algorithm tie for first place on feature combinations.",http://www.aclweb.org/anthology/N/N04/N04-1023.pdf
"(Cunninham et al., 1997)",A97-2017,"But the pressure towards theoretical diversity means that there is no point attempting to gain agreement, in the short term, on what set of component technologies should be developed or on the informational content or syntax of representations that these components should require or produce.","Our response has been to design and implement a software environment called GATE (Cunninham et al., 1997), which we will demonstrate at ANLP.",GATE attempts to meet the following objectives:,http://www.aclweb.org/anthology/A/A97/A97-2017.pdf
"(Grishman, 1996)",A97-2017,"All communication between the components of an LE system goes through GDM, which insulates these components from direct contact with each other and provides them with a uniform API for manipulating the data they produce and consume.","The basic concepts of the data model underlying the GDM are those of the TIPSTER architecture, which is specified (Grishman, 1996).","All the real work of analysing texts in a GATEbased LE system is done by CREOLE modules or objects (we use the terms module and object rather loosely to mean interfaces to resources which may be predominantly algorithmic or predominantly data, or a mixture of both).",http://www.aclweb.org/anthology/A/A97/A97-2017.pdf
"(Gaizauskas et al., 1995)",A97-2017,(Modules running as external executables might also be recompiled between runs.),"To illustrate the process of converting pre-existing LE systems into GATE-compatible CREOLE sets we use as an example the creation of VIE (Vanilla Information Extraction system) from LaSIE (LargeScale Information Extraction system) (Gaizauskas et al., 1995), Sheffield's entry in the MUC-6 system evaluations.",LaSIE module interfaces were not standardised when originally produced and its CREOLEization gives a good indication of the ease of integrating other LE tools into GATE.,http://www.aclweb.org/anthology/A/A97/A97-2017.pdf
"(Vygotsky, 1978)",Q14-1040,The test difficulty needs to match the intended target group as the test should be challenging for the learner but not lead to frustration.,"According to Vygotsky's zone of proximal development (Vygotsky, 1978), the range of suitable material is very small.","Thus, creating a test that fits this narrow target zone is a tedious and timeconsuming task.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Spolsky, 1969)",Q14-1040,This results in a subjective difficulty estimation that often lacks the consistency required for comparing learners over different tests.,"The underlying principle of most language proficiency tests is the concept of reduced redundancy testing (Spolsky, 1969).","It is based on the idea that ""natural language is redundant"" and that more advanced learners can be distinguished from beginners by their ability to deal with reduced redundancy.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Eckes and Grotjahn, 2006)",Q14-1040,"For language testing, redundancy can be reduced by eliminating words from a text and asking the learner to fill in the gap, also known as the cloze test.","The C-test is a variant of the cloze test which contains more gaps but provides part of the solution as a hint and has been found to be a good estimate for language proficiency (Eckes and Grotjahn, 2006).",We present an approach for determining the difficulty of C-tests that overcomes the mentioned drawbacks of subjective evaluation by teachers.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(2002),Q14-1040,"For each gap, the smaller half of the word is provided and the missing part has to be completed by the learner.","Since its introduction, the C-test has been researched from many angles and has been adapted for over 20 languages (see Grotjahn et al (2002) for an overview).",2.1 C-Tests vs Cloze Tests,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Raatz and Klein-Braley, 2002)",Q14-1040,"In relaxed scoring, teachers accept all tolerable candidates for a gap and not only the intended solution as in exact scoring.","Unfortunately, this scoring method turned out to be quite subjective and time-consuming as it is not possible to anticipate all tolerable solutions (Raatz and Klein-Braley, 2002).",The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Sakaguchi et al., 2013",Q14-1040,The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked.,"Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves.","However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"Zesch and Melamud, 2014)",Q14-1040,The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked.,"Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves.","However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1984),Q14-1040,"However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results.","In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.","Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Babaii and Ansary, 2001",Q14-1040,"In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.","Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).","For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"Klein-Braley, 1997",Q14-1040,"In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.","Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).","For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"Jafarpur, 1995)",Q14-1040,"In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative.","Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995).","For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1984),Q14-1040,"In addition, the prefix hint allows for a narrower deletion pattern (every second gap) providing more empirical evidence for the students' abilities on less text.","As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition.","However, Jakschik et al (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(2010),Q14-1040,"As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition.","However, Jakschik et al (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps.","This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Mostow and Jang, 2012",Q14-1040,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"Agarwal and Mannem, 2011",Q14-1040,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"Mitkov et al., 2006)",Q14-1040,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Skory and Eskenazi, 2010",Q14-1040,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"Heilman et al., 2007",Q14-1040,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"Brown et al., 2005)",Q14-1040,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Perez-Beltrachini et al., 2012)",Q14-1040,Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective.,"The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012).",The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Eckes and Grotjahn, 2006",Q14-1040,"The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: ""Which skills does the C-test measure?""","While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.",The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"Sigott, 1995",Q14-1040,"The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: ""Which skills does the C-test measure?""","While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.",The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"Klein-Braley, 1985)",Q14-1040,"The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: ""Which skills does the C-test measure?""","While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.",The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Babaii and Ansary, 2001)",Q14-1040,"The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: ""Which skills does the C-test measure?""","While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.",The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Chapelle, 1994",Q14-1040,"The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: ""Which skills does the C-test measure?""","While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.",The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"Singleton and Little, 1991)",Q14-1040,"The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: ""Which skills does the C-test measure?""","While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language.",The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1984),Q14-1040,The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level.,Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators - average sentence length and type-token ratio - obtaining good results for her target group.,Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(2011),Q14-1040,Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators - average sentence length and type-token ratio - obtaining good results for her target group.,Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level.,He created a tailored C-test that only contains selected gaps in order to better discriminate between the students.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1993),Q14-1040,Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators - average sentence length and type-token ratio - obtaining good results for her target group.,Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level.,He created a tailored C-test that only contains selected gaps in order to better discriminate between the students.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1989),Q14-1040,Previous work on gap difficulty is based on correlation analyses.,"Brown (1989) identifies the word class, the local word frequency, and readability measures as factors correlating with cloze gap difficulty.",Sigott (1995) exam,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1995),Q14-1040,"Brown (1989) identifies the word class, the local word frequency, and readability measures as factors correlating with cloze gap difficulty.",Sigott (1995) exam,"2It should be noted, that their definition of ""vocabulary"" is very wide.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1996),Q14-1040,"ines word frequency, word class, and constituent type of the gap for the C-test and finds high correlation only for the word frequency.","Klein-Braley (1996) identifies additional error patterns related to production problems (right word stem in wrong form) and early closure, i.e.",the solution works locally but not in the larger context.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(2002),Q14-1040,of vs. off or then vs. than and we cannot decide whether it is a spelling error or a wrong word choice.,"As the generous time limit allows the students to revise their solutions for typos, we consider them as normal errors in line with Raatz and Klein-Braley (2002).",4 C-Test Difficulty Model,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Babaii and Ansary, 2001)",Q14-1040,We find that the difficulty of C-tests is determined by a combination of many factors.,"In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001).","Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Sigott, 2006",Q14-1040,"In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001).","Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving.","Therefore, we developed a model for C-test difficulty that incorporates features from both processing levels (see Figure 4).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"Grotjahn and Stemmer, 2002)",Q14-1040,"In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001).","Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving.","Therefore, we developed a model for C-test difficulty that incorporates features from both processing levels (see Figure 4).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1989),Q14-1040,We therefore calculate the frequency of the solution and also its length as more frequent words tend to be shorter in English.,"In previous work, Brown (1989) calculates the frequency of the target word on the basis of the current test text.","This is clearly a biased estimate of the frequency, but it is still identified as a good indicator for cloze gap difficulty.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1995),Q14-1040,"This is clearly a biased estimate of the frequency, but it is still identified as a good indicator for cloze gap difficulty.","Sigott (1995) calculates the frequency of the solution word using counts from the SUSANNE corpus.7 For our calculations, we use the larger Web1T corpus (Brants and Franz, 2006) and extract normalized probabilities instead of absolute frequencies for better comparison.","Furthermore, a gap is easier to solve, if the solution occurs in a very typical context, e.g.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Brants and Franz, 2006)",Q14-1040,"This is clearly a biased estimate of the frequency, but it is still identified as a good indicator for cloze gap difficulty.","Sigott (1995) calculates the frequency of the solution word using counts from the SUSANNE corpus.7 For our calculations, we use the larger Web1T corpus (Brants and Franz, 2006) and extract normalized probabilities instead of absolute frequencies for better comparison.","Furthermore, a gap is easier to solve, if the solution occurs in a very typical context, e.g.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Gurevych et al., 2012)",Q14-1040,"the word well has a high frequency, but it occurs only rarely in its sense fountain.","In order to account for polysemy, we count the number of represented word senses for the solution in the lexical-semantic resource UBY (Gurevych et al., 2012).",The two senses of well also differ in their word class.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1989),Q14-1040,The word class has been studied as a difficulty indicator by several researchers but with mixed results.,"Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners.",Sigott (1995) could not confirm any effect of the word class on C-test difficulty.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1996),Q14-1040,The word class has been studied as a difficulty indicator by several researchers but with mixed results.,"Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners.",Sigott (1995) could not confirm any effect of the word class on C-test difficulty.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1995),Q14-1040,"Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners.",Sigott (1995) could not confirm any effect of the word class on C-test difficulty.,"6In all examples, we only highlight a single gap to illustrate a certain phenomenon.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Beinborn et al., 2014)",Q14-1040,"As additional feature, we calculate the probability of the POS sequence of the micro context.","Cognateness Frequency is not the only indicator for word familiarity and can sometimes even be misleading (Beinborn et al., 2014).","Many solution words are cognates, i.e.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Beinborn et al., 2013)",Q14-1040,"for skeletons we find *skellets, *skelleton(s), *skelets, *skelletts, *skeletton(s), *skeltons, *skeletes, and *skelette(s).11 In order to account for this phenomenon, we estimate the cognateness of words by gathering data from four different lists.","We retrieve cognates from UBY using string similarity and from a cognate production algorithm (Beinborn et al., 2013).","In addition, we consult the COCA list of academic words12 and a list of words with latin roots.13 Inflection Many errors are caused by wrong morphological inflection as in this example: And in har times like these, ... [harder] The base form hard (72) is provided more often than the correct comparative harder (48), although it is too short.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1989),Q14-1040,not as a gap) because it facilitates the correct production for the student.,This feature is comparable to the semantic cache used by Brown (1989).,Phonetic complexity Wrong answer variants for C-test gaps are often rooted in phonetic problems.,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Pauls and Klein, 2011)",Q14-1040,"In addition, a spelling error is more likely to occur, in words with rare grapheme-phoneme mapping as in Wednesday.","We build a character-based language model that indicates the probability of a character sequence using BerkeleyLM (Pauls and Klein, 2011).","In addition, we build a phonetic model using phonetisaurus, a statistical alignment algorithm that maps characters onto phonemes.14 Both models are trained only on words from the Basic English list in order to reflect the knowledge of a language learner.15 Based on this scarce data, the phonetic model only learns the most frequent character-tophoneme mappings and assigns higher phonetic scores to less general letter sequences.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1992),Q14-1040,It would be interesting to repeat the test with the constraint that false answers have a negative influence on the overall score in order to find out whether the students are aware of the length violation.,Bresnihan and Ray (1992) show that students perform,522,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(2010),Q14-1040,"This macro-level dimension assesses the dependency of the current gap on previous gaps: can it be solved, even if the previous gap has not been solved?","In previous work, Harsch and Hartig (2010) examine dependencies between individual gaps using a Rasch testlet model and find that some gaps strongly depend on each other, while others can be solved independently.","At the same time, fertility is set to fall as women leave childbirth la and la .",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1989),Q14-1040,We calculate the following readability features for the whole paragraph and for the sentence containing the gap.,Average word and sentence length are the underlying basis of traditional readability measures such as Flesch-Kincaid and Fry which correlate with cloze test difficulty according to Brown (1989).,"We calculate both, but do not find much variety as the paragraphs in our data are all of comparable length (64-99 words, 3-7 sentences, 4.85 characters per word).",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1984),Q14-1040,"The type-token ratio, the verb variation, and the pronoun ratio are used as indicators for lexical diversity and referentiality.",Klein-Braley (1984) already determined the type-token ratio as useful cue for paragraph difficulty prediction.,"We also use syntactic readability features such as the number of entity mentions, the number of certain POS types (e.g.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(KleinBraley, 1984",Q14-1040,"Having introduced all four dimensions of C-test difficulty, we now report on the results of the actual difficulty prediction.","Difficulty prediction of C-tests has up to now only been performed on the paragraph level (KleinBraley, 1984; Traxel and Dresemann, 2010).","In this article, we go beyond paragraphs and predict the difficulty of gaps.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"Traxel and Dresemann, 2010)",Q14-1040,"Having introduced all four dimensions of C-test difficulty, we now report on the results of the actual difficulty prediction.","Difficulty prediction of C-tests has up to now only been performed on the paragraph level (KleinBraley, 1984; Traxel and Dresemann, 2010).","In this article, we go beyond paragraphs and predict the difficulty of gaps.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1995),Q14-1040,"Thus, it is possible to outperform human performance with automatic methods and provide a very helpful tool.",Classification Regression P R Fl Pearson's r RMSE Majority Baseline .19 .43 .26 .00 .25 Sigott (1995) .23 .40 .28 .34 .24 Our Approach .46 .48 .46 .64 .20 Human Median .56 .53 .54 -,Table 3: Results for leave-one-out crossvalidation on the training set for regression and classification prediction (both trained on support vector machines).,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(de Castilho and Gurevych, 2014)",Q14-1040,Our difficulty prediction approach is based on the model described in the previous section.,"We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych, 2014).","We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Hall et al., 2009)",Q14-1040,"We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych, 2014).","We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18",6.1 Classification vs Regression,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Daxenberger et al., 2014)",Q14-1040,"We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych, 2014).","We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18",6.1 Classification vs Regression,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1995),Q14-1040,We perform leave-one-out testing on the training set in order to determine the best approach.,"We compare our model against the human performance and two baselines: A naive one that predicts the majority class for classification and the mean value for regression and one that only uses the features proposed by Sigott (1995) (solution probability, word class of solution, and constituent type of gap).","In Table 3, we report weighted precision, recall and Fl-measure over all classes for classification and Pearson correlation and root mean squared error for regression.",http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
(1995),Q14-1040,"Starting from a sample size of about 70 instances, the learning curve proceeds as",# LOOCV Train Train-Test LOOCV All Mean Baseline 1 .00 .00 .00 Sigott (1995) 7 .34 .38 .36 Full Model 87 .64 .32 .60 Selected Features 21 .68 .44 .57,Table 8: Results on the train and the test set,http://www.aclweb.org/anthology/Q/Q14/Q14-1040.pdf
"(Frank et al., 1999",N09-1070,"The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection.","Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005).","However, we may not have a big background collection that matches the test domain for a reliable IDF estimate.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"Hulth, 2003",N09-1070,"The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection.","Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005).","However, we may not have a big background collection that matches the test domain for a reliable IDF estimate.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"Kerner et al., 2005)",N09-1070,"The idea is to identify words that appear frequently in a document, but do not occur frequently in the entire document collection.","Much work has shown that TFIDF is very effective in extracting keywords for scientific journals, e.g., (Frank et al., 1999; Hulth, 2003; Kerner et al., 2005).","However, we may not have a big background collection that matches the test domain for a reliable IDF estimate.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Matsuo and Ishizuka, 2004)",N09-1070,"However, we may not have a big background collection that matches the test domain for a reliable IDF estimate.","(Matsuo and Ishizuka, 2004) proposed a co-occurrence distribution based method using a clustering strategy for extracting keywords for a single document without relying on a large corpus, and reported promising results.",Web information has also been used as an additional knowledge source for keyword extraction.,http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Turney, 2002)",N09-1070,Web information has also been used as an additional knowledge source for keyword extraction.,"(Turney, 2002) selected a set of keywords first and then determined whether to add another keyword hypothesis based on its PMI (point-wise mutual information) score to the current selected keywords.",The preselected keywords can be generated using basic extraction algorithms such as TFIDF.,http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Inkpen and Desilets, 2004)",N09-1070,It is important to ensure the quality of the first selection for the subsequent addition of keywords.,"Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).","Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Frank et al., 1999",N09-1070,"Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).","Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).","In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"Turney, 2000",N09-1070,"Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).","Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).","In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"Kerner et al., 2005",N09-1070,"Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).","Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).","In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"Turney, 2002",N09-1070,"Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).","Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).","In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"Turney, 2003)",N09-1070,"Other researchers also used PMI scores between each pair of candidate keywords to select the top k% of words that have the highest average PMI scores as the final keywords (Inkpen and Desilets, 2004).","Keyword extraction has also been treated as a classification task and solved using supervised machine learning approaches (Frank et al., 1999; Turney, 2000; Kerner et al., 2005; Turney, 2002; Turney, 2003).","In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Turney, 2000)",N09-1070,"In these approaches, the learning algorithm needs to learn to classify candidate words in the documents into positive or negative examples using a set of features.","Useful features for this approach include TFIDF and its variations, position of a phrase, POS information, and relative length of a phrase (Turney, 2000).",Some of these features may not work well for meeting transcripts.,http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Liu et al., 2008)",N09-1070,"However, for the less well structured meeting domain (lack of title and paragraph), these kinds of features may not be indicative.","A supervised approach to keyword extraction was used in (Liu et al., 2008).","Even though the data set in that study is not very big, it seems that a supervised learning approach can achieve reasonable performance for this task.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Brin and Page, 1998)",N09-1070,"Even though the data set in that study is not very big, it seems that a supervised learning approach can achieve reasonable performance for this task.","Another line of research for keyword extraction has adopted graph-based methods similar to Google's PageRank algorithm (Brin and Page, 1998).","In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Wan et al., 2007)",N09-1070,"Another line of research for keyword extraction has adopted graph-based methods similar to Google's PageRank algorithm (Brin and Page, 1998).","In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences.",We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study.,http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Plas et al., 2004)",N09-1070,Not many studies have been performed on speech transcripts for keyword extraction.,"The most relevant work to our study is (Plas et al., 2004), where the task is keyword extraction in the multiparty meeting corpus.",They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).,http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Munteanu et al., 2007",N09-1070,They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).,"There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).","(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"Bulyko et al., 2007",N09-1070,They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).,"There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).","(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"Wu et al., 2007",N09-1070,They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).,"There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).","(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"Desilets et al., 2002",N09-1070,They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).,"There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).","(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"Rogina, 2002)",N09-1070,They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF).,"There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).","(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Wu et al., 2007)",N09-1070,"There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002).","(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.","In (Rogina, 2002), keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Rogina, 2002)",N09-1070,"(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data.","In (Rogina, 2002), keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures.",There are many differences between written text and speech - meetings in particular.,http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Janin et al., 2003)",N09-1070,3 Data,"We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings.","All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Shriberg et al., 2004)",N09-1070,"We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings.","All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).","The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Murray et al., 2005)",N09-1070,"We used the meetings from the ICSI meeting data (Janin et al., 2003), which are recordings of naturally occurring meetings.","All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).","The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Zhu et al., 2005)",N09-1070,"All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al., 2004), topics, and extractive summaries (Murray et al., 2005).","The ASR output for this corpus is obtained from a state-of-the-art SRI conversational telephone speech system (Zhu et al., 2005), with a word error rate of about 38.2% on the entire corpus.","We align the human transcripts and ASR output, then map the human annotated DA boundaries and topic boundaries to the ASR words, such that we have human annotation of these information for the ASR output.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Murray et al., 2005)",N09-1070,"The average length of the topics (measured using the number of dialog acts) among all the meetings is 172.5, with a high standard deviation of 236.8.","We used six meetings as our development set (the same six meetings as the test set in (Murray et al., 2005)) to optimize our keyword extraction methods, and the remaining 21 meetings for final testing in Section 5.",One example of the annotated keywords for a topic segment is:,http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Galley et al., 2003",N09-1070,"Note that these meetings are research discussions, and that the annotators may not be very familiar with","1We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005).",the topics discussed and often had trouble deciding the important sentences or keywords.,http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"Murray et al., 2005)",N09-1070,"Note that these meetings are research discussions, and that the annotators may not be very familiar with","1We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005).",the topics discussed and often had trouble deciding the important sentences or keywords.,http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Brants, 2000)",N09-1070,"Our hypothesis is that verb, noun and adjective words are more likely to be keywords, so we restrict our selection to words with these POS tags only.","We used the TnT POS tagger (Brants, 2000) trained from the Switchboard data to tag the meeting transcripts.","(C) Integrating word clustering One weakness of the baseline TFIDF is that it counts the frequency for a particular word, without considering any words that are similar to it in terms of semantic meaning.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Stolcke, 2002)",N09-1070,Thus we want to assign higher weights to the words in this cluster.,"We used the SRILM toolkit (Stolcke, 2002) for automatic word clustering over the entire document collection.",It minimizes the perplexity of the induced class-based n-gram language model compared to the original word-based model.,http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Radev et al., 2001)",N09-1070,The sentence score is calculated based on its cosine similarity to the entire meeting.,"This score is often used in extractive summarization to select summary sentences (Radev et al., 2001).","The cosine similarity between two vectors, D1 and D2, is defined as:",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Wan et al., 2007)",N09-1070,4.2 Graph-based Methods,"For the graph-based approach, we adopt the iterative reinforcement approach from (Wan et al., 2007) in the hope of leveraging sentence information for keyword extraction.",This algorithm is based on the assumption that important sentences/words are connected to other important sentences/words.,http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Wan et al., 2007)",N09-1070,S-W and W-S Graphs: The weight for an edge between a sentence and a word is the TF of the word in the sentence multiplied by the word's IDF value.,"These weights are initially added only to the S-W graph, as in (Wan et al., 2007); then that graph is normalized and transposed to create the W-S graph.",S-S Graph: The sentence node uses a vector,http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Liu et al., 2008)",N09-1070,"Therefore, when the system only generates five keywords, the upper bound of the recall rate may not be 100%.","In (Liu et al., 2008), a lenient metric is used which accounts for some inflection of words.","Since that is highly correlated with the results using exact word match, we report results based on strict matching in the following experiments.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Nenkova and Passonneau, 2004)",N09-1070,"Since that is highly correlated with the results using exact word match, we report results based on strict matching in the following experiments.","The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation.","Instead of comparing the system output with each individual human annotation, the method creates a ""pyramid"" using all the human annotated keywords, and then compares system output to this pyramid.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Liu et al., 2008)",N09-1070,Table 1 shows the results using human transcripts for different methods on the 21 test meetings (139 topic segments in total).,"For comparison, we also show results using the supervised approach as in (Liu et al., 2008), which is the average of the 21fold cross validation.","We only show the maximum F-measure with respect to individual annotations, since the average scores show similar trend.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Wan et al., 2007)",N09-1070,The global information in the S-S graph (connecting a sentence to other sentences in the document) is propagated to the word nodes.,"Unlike the study in (Wan et al., 2007), this information does not yield any gain.","We did find that the graph approach performed better in the development set, but it seems that it does not generalize to this test set.",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"(Liu et al., 2008)",N09-1070,Again note that these results used wordbased selection.,"(Liu et al., 2008) investigated adding bigram key phrases, which we expect to be independent of these unigram-based approaches and adding bigram phrases will yield further performance gain for the unsupervised approach.","Finally, we analyzed if the system's keyword extraction performance is correlated with human annotation disagreement using the unsupervised approach (TFIDF+POS+Sent weight).",http://www.aclweb.org/anthology/N/N09/N09-1070.pdf
"Marton et al., 2009",N12-4007,"""phrases"" -- and doing so overcoming previous working memory and representation limitations.","We focus on distributional paraphrasing (Pasca and Dienes 2005; Marton et al., 2009; Marton, to appear 2012).","We will also cover pivot paraphrasing (Bannard and Callison-Burch, 2005).",http://www.aclweb.org/anthology/N/N12/N12-4007.pdf
"(Bannard and Callison-Burch, 2005)",N12-4007,"We focus on distributional paraphrasing (Pasca and Dienes 2005; Marton et al., 2009; Marton, to appear 2012).","We will also cover pivot paraphrasing (Bannard and Callison-Burch, 2005).","We will discuss several weaknesses of distributional paraphrasing, and where the stateof-the-art is.",http://www.aclweb.org/anthology/N/N12/N12-4007.pdf
"Hovy, 2010",N12-4007,What else can be done to ameliorate this problem?,"(Mohammad et al., EMNLP 2008; Hovy, 2010; Marton et al., WMT 2011).","Another potential weakness is the difficulty in detecting and generating longer-thanword (phrasal) paraphrases, because pre-calculating a collocation matrix for phrases becomes prohibitive in the matrix size with longer phrases, even with sparse representation.",http://www.aclweb.org/anthology/N/N12/N12-4007.pdf
"(Manber and Myers, 1993",N12-4007,"We will present an alternative to pre-calculation: on-demand paraphrasing, as described in Marton (to appear 2012).","There, searching the monolingual text resource is done ondemand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007).","This enables constructing large vector representation, since there is no longer a need to compute a whole matrix.",http://www.aclweb.org/anthology/N/N12/N12-4007.pdf
"Gusfield, 1997",N12-4007,"We will present an alternative to pre-calculation: on-demand paraphrasing, as described in Marton (to appear 2012).","There, searching the monolingual text resource is done ondemand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007).","This enables constructing large vector representation, since there is no longer a need to compute a whole matrix.",http://www.aclweb.org/anthology/N/N12/N12-4007.pdf
"Lopez, 2007)",N12-4007,"We will present an alternative to pre-calculation: on-demand paraphrasing, as described in Marton (to appear 2012).","There, searching the monolingual text resource is done ondemand with a suffix array or prefix tree with suffix links (Manber and Myers, 1993; Gusfield, 1997; Lopez, 2007).","This enables constructing large vector representation, since there is no longer a need to compute a whole matrix.",http://www.aclweb.org/anthology/N/N12/N12-4007.pdf
"(Marino et al., 2006)",W08-0315,1 Introduction,"Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system (Marino et al., 2006).","In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006), Callison-Burch et al (2007).",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf
(2006),W08-0315,"Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system (Marino et al., 2006).","In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006), Callison-Burch et al (2007).",We present a summary of the TALP-UPC Ngrambased SMT system used for this shared task.,http://www.aclweb.org/anthology/W/W08/W08-0315.pdf
(2007),W08-0315,"Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system (Marino et al., 2006).","In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006), Callison-Burch et al (2007).",We present a summary of the TALP-UPC Ngrambased SMT system used for this shared task.,http://www.aclweb.org/anthology/W/W08/W08-0315.pdf
"(Brown et al., 1990)",W08-0315,2 Ngram-based SMT System,"Our translation system implements a log-linear model in which a foreign language sentence fJ1 = f1, f2, ..., fJ is translated into another language eI1 = f1, f2, ..., eI by searching for the translation hypothesis oI1 maximizing a log-linear combination of several feature models (Brown et al., 1990):","M oI1 = argmax { m=1 E amhm(ef, fJ1 ) eI l 1",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf
(2006),W08-0315,"It actually constitutes an Ngram-based LM of bilingual units (called tuples), which approximates the joint probability between the languages under consideration.",The procedure of tuples extraction from awordto-word alignment according to certain constraints is explained in detail in et al (2006).,The Ngram-based approach differs fr,http://www.aclweb.org/anthology/W/W08/W08-0315.pdf
"(Och and Ney, 2004)",W08-0315,"The TALP-UPC 2008 translation system, besides the bilingual translation model, which consists of a 4-gram LM of tuples with Kneser-Ney discounting (estimated with SRI Language Modeling Toolkit1), implements a log-linear combination of five additional feature models:"," a target language model (a 4-gram model of words, estimated with Kneser-Ney smoothing);  a POS target language model (a 4-gram model of tags with Good-Turing discounting (TPOS));  a word bonus model, which is used to compensate the system's preference for short output sentences;  a source-to-target lexicon model and a target-tosource lexicon model, these models use word-toword IBM Model 1 probabilities (Och and Ney, 2004) to estimate the lexical weights for each tuple in the translation table.",Decisions on the particular LM configuration and smoothing technique were taken on the minimalperplexity and maximal-BLEU bases.,http://www.aclweb.org/anthology/W/W08/W08-0315.pdf
(2006),W08-0315,In our system we use an extended monotone reordering model based on automatically learned reordering rules.,A detailed description can be found in Crego and Marino (2006).,1http://www.speech.sri.com/projects/srilm/ 2http://gps-tsc.upc.es/veu/soft/soft/marie/,http://www.aclweb.org/anthology/W/W08/W08-0315.pdf
"(Crego and Marino, 2006)",W08-0315,"where t1, ..., tn is a sequence of POS tags (relating a sequence of source words), and i1, ..., in indicates which order of the source words generate monotonically the target words.","Patterns are extracted in training from the crossed links found in the word alignment, in other words, found in translation tuples (as no word within a tuple can be linked to a word out of it (Crego and Marino, 2006)).","Having all the instances of rewrite patterns, a score for each pattern on the basis of relative frequency is calculated as shown below:",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf
"(Carreras et al., 2004)",W08-0315,POS information for the source and the target languages was considered for both translation tasks that we have participated.,"The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English.","The number of classes for English is 44, while Spanish is considered as a more inflectional language, and the tag set contains 376 different tags.",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf
"(Brants, 2000)",W08-0315,POS information for the source and the target languages was considered for both translation tasks that we have participated.,"The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English.","The number of classes for English is 44, while Spanish is considered as a more inflectional language, and the tag set contains 376 different tags.",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf
"(Och and Ney, 2000)",W08-0315,Word Alignment.,"The word alignment is automatically computed by using GIZA++4(Och and Ney, 2000) in both directions, which are symmetrized by using the union operation.","Instead of aligning words themselves, stems are used for aligning.",http://www.aclweb.org/anthology/W/W08/W08-0315.pdf
"(de Gispert, 2006)",W08-0315,"In particular, the pronouns attached to the verb were separated and contractions as del or al were splitted into de el or a el.","As a post-processing, in the En2Es direction we used a POS target LM as a feature (instead of the target language model based on classes) that allowed to recover the segmentations (de Gispert, 2006).",4.3 Experiments and Results,http://www.aclweb.org/anthology/W/W08/W08-0315.pdf
"(Lita et al., 2003)",W04-3237,Automatic capitalization can be seen as a sequence tagging problem: each lower-case word receives a tag that describes its capitalization form.,"Similar to the work in (Lita et al., 2003), we tag each word in a sentence with one of the tags:", LOC lowercase  CAP capitalized  MXC mixed case; no further guess is made as to the capitalization of such words.,http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Lita et al., 2003)",W04-3237,"2As with everything in natural language, it is not hard to find exceptions to this ""rule"".","Due to its popularity, both our work and that of (Lita et al., 2003) uses the 1-gram capitalizer as a baseline.","The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Kim and Woodland, 2004)",W04-3237,"Due to its popularity, both our work and that of (Lita et al., 2003) uses the 1-gram capitalizer as a baseline.","The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well.",2.2 Previous Work,http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Lita et al., 2003)",W04-3237,2.2 Previous Work,"We share the approach to capitalization as sequence tagging with that of (Lita et al., 2003).","In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Kim and Woodland, 2004)",W04-3237,"In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques.","The same idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from speech recognition output.","A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Brill, 1994)",W04-3237,"The same idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from speech recognition output.","A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors.","Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996).",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Ratnaparkhi, 1996)",W04-3237,"A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors.","Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996).","The MEMM approach models the tag sequence T conditionally on the word sequence W, which has a few substantial advantages over the 1-gram tagging approach:",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Lafferty et al., 2001)",W04-3237," discriminative training of probability model P(T|W) using conditional maximum likelihood is well correlated with tagging accuracy  ability to use a rich set of word-level features in a parsimonious way: sub-word features such as prefixes and suffixes, as well as future words3 are easily incorporated in the probability model  no concept of ""out-of-vocabulary"" word: subword features are very useful in dealing with words not seen in the training data  ability to integrate rich contextual features into the model","More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al., 2001) which slightly outperforms MEMMs on a standard partof-speech tagging task.","In a similar vein, the work",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Collins, 2002)",W04-3237,"3Relative to the current word, whose tag is assigned a probability value by the MEMM.","of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs.","The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Chen and Rosenfeld, 2000)",W04-3237,"of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs.","The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).","We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Goodman, 2004)",W04-3237,"The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).","We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).","Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Brill, 1994)",W04-3237,"Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario.","A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible.",3 MEMM for Sequence Labeling,http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Ratnaparkhi, 1996)",W04-3237,"Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario.","A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible.",3 MEMM for Sequence Labeling,http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Ratnaparkhi, 1996)",W04-3237,model is built.,"The approach we took is the one in (Ratnaparkhi, 1996), which uses xi(W, T i1 1 ) = {wi, wi1, wi+1, ti1, ti2}.","We note that the probability model is causal in the sequencing of tags (the probability assignment for ti only depends on previous tags ti1, ti2) which allows for efficient algorithms that search for the most likely tag sequence T(W) = arg maxT P(T |W) as well as ensures a properly normalized conditional probability model P(T|W).",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Berger et al., 1996)",W04-3237,"The probability P(ti|xi(W,T i1 1 )) is modeled using a maximum entropy model.","The next section briefly describes the training procedure; for details the reader is referred to (Berger et al., 1996).",3.1 Maximum Entropy State Transition Model,http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Chen and Rosenfeld, 2000)",W04-3237,3.1.2 Parameter Estimation,"The model parameters A are estimated such that the model assigns maximum log-likelihood to the training data subject to a Gaussian prior centered at 0, A  N(0, diag(u2i )), that ensures smoothing (Chen and Rosenfeld, 2000):","L(A) = E p(x, y)log pA(y|x) (1) x,y n P(T|W) = i=1 E Z(x, A) = y i 2' + const(A)i",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Chen and Rosenfeld, 2000)",W04-3237,"L(A) = E p(x, y)log pA(y|x) (1) x,y n P(T|W) = i=1 E Z(x, A) = y i 2' + const(A)i","As shown in (Chen and Rosenfeld, 2000) - and rederived in Appendix A for the non-zero mean case - the update equations are:","(t+1) i= (t) i+ i, where i satisfies: i p(x, y)fi(x, y)  = 2 i p(x)p(y|x)fi(x, y)exp(if#(x, y))",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Chen and Rosenfeld, 2000)",W04-3237,A simple way to accomplish this is to use MAP adaptation using a prior distribution on the model parameters.,"A Gaussian prior for the model parameters A has been previously used in (Chen and Rosenfeld, 2000) for smoothing MaxEnt models.","The prior has 0 mean and diagonal covariance: A  N(0, diag(2 i)).",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Pietra et al., 1995)",W04-3237,"F  i=1 E x,y i + (2) 2i E x,y F  i=1","ing (Pietra et al., 1995) between the background exponential model B - assumed fixed - and an exponential model A built using the Fbackground U Fadapt feature set.","It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0 - following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models - then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed5.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Chen and Rosenfeld, 2000)",W04-3237,"ing (Pietra et al., 1995) between the background exponential model B - assumed fixed - and an exponential model A built using the Fbackground U Fadapt feature set.","It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0 - following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models - then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed5.","However, we wish to point out that the equivalence holds only if the feature set for the new model A is Fbackground U Fadapt.",http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
"(Paul and Baker, 1992)",W04-3237,5 Experiments,"The baseline 1-gram and the background MEMM capitalizer were trained on various amounts of WSJ (Paul and Baker, 1992) data from 1987 - files WS87_{001-126}.",The in-domain test data used was file WS94_000 (8.7kwds).,http://www.aclweb.org/anthology/W/W04/W04-3237.pdf
(2001),P13-1112,"The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing.","Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.","In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(1998),P13-1112,"The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing.","Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.","In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(2001),P13-1112,"The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing.","Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.","In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(1998),P13-1112,"The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing.","Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items.","In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(2009),P13-1112,"A similar argument can be made about some parts of gender, tense, and aspect systems.","Besides, Wong and Dras (2009) show that there are no significant differences, between mother tongues, in the misuse of certain syntactic features such as subject-verb agreement that have different tendencies depending on their mother tongues.","Considering these, one could not be so sure which argument is correct.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Chodorow et al., 2010)",P13-1112,"One of the major contributions of this work is to reveal and visualize a language family tree preserved in non-native texts, by examining the hypothesis.","This becomes important in native language identification) which is useful for improving grammatical error correction systems (Chodorow et al., 2010) or for providing more targeted feedback to language learners.",As we will see in Sect.,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Enright and Kondrak, 2011",P13-1112,"6, this paper reveals several crucial findings that contribute to improving native language identification.","In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.",The rest of this paper is structured as follows.,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"Gray and Atkinson, 2003",P13-1112,"6, this paper reveals several crucial findings that contribute to improving native language identification.","In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.",The rest of this paper is structured as follows.,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"Barbancon et al., 2007",P13-1112,"6, this paper reveals several crucial findings that contribute to improving native language identification.","In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.",The rest of this paper is structured as follows.,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"Batagelj et al., 1992",P13-1112,"6, this paper reveals several crucial findings that contribute to improving native language identification.","In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.",The rest of this paper is structured as follows.,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"Nakhleh et al., 2005)",P13-1112,"6, this paper reveals several crucial findings that contribute to improving native language identification.","In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics.",The rest of this paper is structured as follows.,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Beekes, 2011",P13-1112,2 Approach,"To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother tongue is one of the Indo-European languages (Beekes, 2011; Ramat and Ramat, 2006).","If the reconstructed tree is sufficiently similar to the original Indo-European family tree, it will support the hypothesis.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"Ramat and Ramat, 2006)",P13-1112,2 Approach,"To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother tongue is one of the Indo-European languages (Beekes, 2011; Ramat and Ramat, 2006).","If the reconstructed tree is sufficiently similar to the original Indo-European family tree, it will support the hypothesis.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Han and Kamber, 2006)",P13-1112,"If not, it suggests that some features other than mother tongue interference are more influential.","The approach we use for reconstructing a language family tree is to apply agglomerative hierarchical clustering (Han and Kamber, 2006) to English texts written by non-native speakers.",Researchers have already performed related work on reconstructing language family trees.,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(1937),P13-1112,Researchers have already performed related work on reconstructing language family trees.,"For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.","More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(1959),P13-1112,Researchers have already performed related work on reconstructing language family trees.,"For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.","More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(1992),P13-1112,"For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.","More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering.","Among them, the 'Recently, native language identification has drawn the attention of NLP researchers.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(1999),P13-1112,"For instance, Kroeber and Chrietien (1937) and Ellegard (1959) proposed statistical methods for measuring the similarity metric between languages.","More recently, Batagelj et al (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering.","Among them, the 'Recently, native language identification has drawn the attention of NLP researchers.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(1999),P13-1112,"For instance, a shared task on native language identification took place at an NAACL-HLT 2013 workshop.",most related method is that of Kita (1999).,"In his method, a variety of languages are modeled by their spelling systems (i.e., character-based n-gram language models).",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(1985),P13-1112,"Then, agglomerative hierarchical clustering is applied to the language models to reconstruct a language family tree.",The similarity used for clustering is based on a divergence-like distance between two language models that was originally proposed by Juang and Rabiner (1985).,This method is purely data-driven and does not require human expert knowledge for the selection of linguistic features.,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(1992),P13-1112,This significant difference prevents us from directly applying techniques in the literature to our task.,"For instance, Batagelj et al (1992) use basic vocabularies such as belly in English and ventre in French to measure similarity between languages.","Obviously, this does not work on our task; belly is belly in English writing whoever writes it.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Kneser and Ney, 1995)",P13-1112,"Now, the language model Mi can be built from Di.","We set n = 3 (i.e., trigram language model) following Kita's work and use Kneser-Ney (KN) smoothing (Kneser and Ney, 1995) to estimate its conditional probabilities.","With Mi and Di, we can naturally apply Kita's method to our task.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Granger et al., 2009)",P13-1112,4 Experiments,"We selected the ICLE corpus v.2 (Granger et al., 2009) as the target language data.",It consists of English essays written by a wide variety of nonnative speakers of English.,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Sugiura et al., 2007)",P13-1112,"Also, the symbols ' and ' were unified into '4.","For reference, we also used native English (British and American university students' essays in the LOCNESS corpus5) and two sets of Japanese English (ICLE and the NICE corpus (Sugiura et al., 2007)).",Table 1 shows the statistics on the corpus data.,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Nagata et al., 2011)",P13-1112,Existing POS taggers might not perform well on non-native English texts because they are normally developed to analyze native English texts.,"Considering this, we tested CRFTagger6 on non-native English texts containing various grammatical errors before the experiments (Nagata et al., 2011).",It turned out that CRFTagger achieved an accuracy of 0.932 (compared to 0.970 on native texts).,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(1997),P13-1112,Fig.1 shows the experimental results.,The tree at the top is the Indo-European family tree drawn based on the figure shown in Crystal (1997).,"It shows that the 11 languages are divided into three groups: Italic, Germanic, and Slavic branches.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Swan and Smith, 2001)",P13-1112,4.,"This corresponds to the fact that noun-noun compounds are less common in the Italic languages than in English and that instead, the of-phrase (NN of NN) is preferred (Swan and Smith, 2001).",For,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(1996),P13-1112,Fig.,"5 shows that the average length roughly distinguishes the Italic Englishes from the other nonnative Englishes; French-English is the shortest, which is explained by the discussion above, while Dutch- and German-Englishes are longest, which may correspond to the fact that they have a preference for noun-noun compounds as Snyder (1996) argues.","For instance, German allows the concatenated form as in Orangensaft (equivalently orangejuice).",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(2009),P13-1112,6 Implications for Work in Related Domains,"Researchers including Wong and Dras (2009), Wong et al (2011; 2012), and Koppel et al (2005) work on native language identification and show that machine learning-based methods are effective.","Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(2005),P13-1112,6 Implications for Work in Related Domains,"Researchers including Wong and Dras (2009), Wong et al (2011; 2012), and Koppel et al (2005) work on native language identification and show that machine learning-based methods are effective.","Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(2009),P13-1112,"Researchers including Wong and Dras (2009), Wong et al (2011; 2012), and Koppel et al (2005) work on native language identification and show that machine learning-based methods are effective.","Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.","Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identification because translation is not typically performed by non-native speakers but rather native speakers of the target language11.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Koppel and Ordan, 2011",P13-1112,"Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.","Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identification because translation is not typically performed by non-native speakers but rather native speakers of the target language11.",The experimental results show that n-grams containing articles are predictive for identifying native languages.,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"van Halteren, 2008)",P13-1112,"Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation.","Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identification because translation is not typically performed by non-native speakers but rather native speakers of the target language11.",The experimental results show that n-grams containing articles are predictive for identifying native languages.,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Koehn, 2011)",P13-1112,Fig.,"7 shows that the observed values in the FrenchEnglish data very closely fit the theoretical proba""For comparison, we conducted a pilot study where we reconstructed a language family tree from English texts in European Parliament Proceedings Parallel Corpus (Europarl) (Koehn, 2011).",It turned out that the reconstructed tree was different from the canonical tree (available at http: //web.hyogo-u.ac.jp/nagata/acl/).,http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Enright and Kondrak, 2011",P13-1112,"In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.","Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.","Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"Gray and Atkinson, 2003",P13-1112,"In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.","Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.","Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"Barbancon et al., 2007",P13-1112,"In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.","Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.","Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"Batagelj et al., 1992",P13-1112,"In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.","Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.","Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"Nakhleh et al., 2005)",P13-1112,"In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees.","Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.","Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"(Kita, 1999",P13-1112,"Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.","Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.","These methods reconstruct language family trees based on linguistic features that exist within words including lexical, phonological, and morphological features.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
"Rama and Singh, 2009)",P13-1112,"Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbancon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction.","Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose.","These methods reconstruct language family trees based on linguistic features that exist within words including lexical, phonological, and morphological features.",http://www.aclweb.org/anthology/P/P13/P13-1112.pdf
(1993),W03-1717,"In the rest of this paper, we will present a learning procedure that learns those relations by processing a large corpus with a chart-filter, a treefilter and an LLR filter.","The approach is in the spirit of Smadja (1993) on retrieving collocations from text corpora, but is more integrated with parsing.",We will show in the evaluation section how much the learned knowledge can help improve sentence analysis.,http://www.aclweb.org/anthology/W/W03/W03-1717.pdf
"(Dunning, 1993)",W03-1717,"Secondly, those verbs tend not to occur in the modifier-head relation with a following noun and we gain very little in terms of disambiguation by storing those pairs in the knowledge base.","To prune away those pairs, we used the log-likelihood-ratio algorithm (Dunning, 1993) to compute the degree of association between the verb and the noun in each pair.","Pairs where there is high ""mutual information"" between the verb and noun would receive higher scores while pairs where the verb can co-occur with many different nouns would receive lower scores.",http://www.aclweb.org/anthology/W/W03/W03-1717.pdf
"me Traitement Automatique des Langues Naturelles, Marseille, 2014",W14-6700,,"21me Traitement Automatique des Langues Naturelles, Marseille, 2014",Actes de l'atelier  Reseaux Lexicaux et Traitement des Langues Naturelles.,http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"(Barrat, 2008, ",W14-6700,"Parallelement  l'evolution des ressources lexicales, on a pu observer une explosion de travaux portant sur les graphes (graphes complexes, phenomene 'petit monde', etc.).","Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.","En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"Barabsi, 2003)",W14-6700,"Parallelement  l'evolution des ressources lexicales, on a pu observer une explosion de travaux portant sur les graphes (graphes complexes, phenomene 'petit monde', etc.).","Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.","En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"(Bieman, 2012",W14-6700,"Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.","En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"Mihalcea et Radev, 2011",W14-6700,"Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.","En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"Widdows, 2004",W14-6700,"Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.","En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"Sowa, 1991)",W14-6700,"Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.","En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"(Schvaneveldt, 1989, ",W14-6700,"Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.","En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"Nelson et al., 1998)",W14-6700,"Ces derniers semblent se preter  merveille  la modelisation de nombreux domaines (Barrat, 2008, Barabsi, 2003) y compris la langue.","En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"(Vitevitch, 2008)",W14-6700,"En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.",Les graphes sont essentiellement une forme de representation mathematique et visuelle des relations entre des objets/entites.,http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"(Ferrer i Cancho & Sole, 2001)",W14-6700,"En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.",Les graphes sont essentiellement une forme de representation mathematique et visuelle des relations entre des objets/entites.,http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"(Dion, 2012)",W14-6700,"En effet, il y a eu de nombreux travaux montrant leur pertinence pour capter le sens des mots et celui des phrases (Bieman, 2012; Mihalcea et Radev, 2011; Widdows, 2004; Sowa, 1991) ou pour modeliser divers aspects du  monde  lexical : structures associatives (Schvaneveldt, 1989, Nelson et al., 1998), structure du dictionnaire (Gaume et al.","2008), densitd lexicale, distance moyenne entre les mots (Vitevitch, 2008), accessibilitd (Ferrer i Cancho & Sole, 2001), aspects dynamiques des graphes (Dion, 2012), etc.",Les graphes sont essentiellement une forme de representation mathematique et visuelle des relations entre des objets/entites.,http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"(Widdows, 2004, ",W14-6700,4 Conclusion,"Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.","Il n'est pas facile de savoir ce qui a pu causer ce 'silence', car il contraste 6norm6ment avec le succes d'un autre 6v6nement, consacr6  des problemes tres voisins : CogALex (http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/index.html).",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"Vitevitch, 2008)",W14-6700,4 Conclusion,"Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.","Il n'est pas facile de savoir ce qui a pu causer ce 'silence', car il contraste 6norm6ment avec le succes d'un autre 6v6nement, consacr6  des problemes tres voisins : CogALex (http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/index.html).",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"(Sahlgren, 2008)",W14-6700,4 Conclusion,"Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.","Il n'est pas facile de savoir ce qui a pu causer ce 'silence', car il contraste 6norm6ment avec le succes d'un autre 6v6nement, consacr6  des problemes tres voisins : CogALex (http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/index.html).",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"(Baroni et Lenci, 2010)",W14-6700,4 Conclusion,"Vu le dynamisme du domaine od de 'nouvelles' th6ories comme les methodes vectorielles (Widdows, 2004, Vitevitch, 2008), la semantique distributionnelle (Sahlgren, 2008), et la memoire distributionelle (Baroni et Lenci, 2010) etc., ont vu le jour et se sont g6n6ralis6es, et vu le vivier du monde francophone travaillant sur les ressources lexicales nous 6tions tres surpris du faible nombre de soumissions.","Il n'est pas facile de savoir ce qui a pu causer ce 'silence', car il contraste 6norm6ment avec le succes d'un autre 6v6nement, consacr6  des problemes tres voisins : CogALex (http://pageperso.lif.univ-mrs.fr/~michael.zock/CogALex-IV/cogalex-webpage/index.html).",http://www.aclweb.org/anthology/W/W14/W14-6700.pdf
"(Markou and Singh, 2003a",N06-1017,"If a system has seen only positive examples, how does it recognize a negative example?","This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data.","Outlier detection approaches typically derive some model of ""normal"" objects from the training set and use a distance measure and a threshold to detect abnormal items.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"Markou and Singh, 2003b",N06-1017,"If a system has seen only positive examples, how does it recognize a negative example?","This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data.","Outlier detection approaches typically derive some model of ""normal"" objects from the training set and use a distance measure and a threshold to detect abnormal items.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"Marsland, 2003)",N06-1017,"If a system has seen only positive examples, how does it recognize a negative example?","This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data.","Outlier detection approaches typically derive some model of ""normal"" objects from the training set and use a distance measure and a threshold to detect abnormal items.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Schutze, 1998)",N06-1017,against which new occurrences are compared will consist of sense-annotated text.,"Unknown sense detection is related to word sense disambiguation (WSD) and to word sense discrimination (Schutze, 1998), but differs from both.","In WSD all senses are assumed known, and the task is to select one of them, while in unknown sense detection the task is to decide whether a given occurrence matches any of the known senses or none of them, and all training instances, regardless of the sense to which they belong, are modeled as one group of known data.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Erk and Pado, 2006)",N06-1017,"In cases where a sense is missing from the inventory, WSD will wrongly assign one of the existing senses.","Figure 1 shows an example, a sentence from the Hound of the Baskervilles, analyzed by the SHALMANESER (Erk and Pado, 2006) shallow semantic parser.","The analysis is based on FrameNet (Baker et al., 1998), a resource that lists senses and semantic roles for English expressions.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Baker et al., 1998)",N06-1017,"Figure 1 shows an example, a sentence from the Hound of the Baskervilles, analyzed by the SHALMANESER (Erk and Pado, 2006) shallow semantic parser.","The analysis is based on FrameNet (Baker et al., 1998), a resource that lists senses and semantic roles for English expressions.","FrameNet is lacking a sense of ""expectation"" or ""being mentally prepared"" for the verb prepare, so prepared has been assigned the sense COOKING CREATION, a possible but improbable analysis2.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Tax and Duin, 2000)",N06-1017,"Our study will be evaluated on FrameNet because of our main aim of improving shallow semantic parsing, but the method we propose is applicable to any sense inventory that has annotated data; in particular, it is also applicable to WordNet.","In this paper we model unknown sense detection as outlier detection, using a simple Nearest Neighbor-based method (Tax and Duin, 2000) that compares the local probability density at each test item with that of its nearest training item.","To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Widdows, 2003",N06-1017,"To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.","There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.",Plan of the paper.,http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"Curran, 2005",N06-1017,"To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.","There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.",Plan of the paper.,http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"Burchardt et al., 2005)",N06-1017,"To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.","There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.",Plan of the paper.,http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Fillmore, 1982)",N06-1017,2 FrameNet,"Frame Semantics (Fillmore, 1982) models the meanings of a word or expression by reference to frames which describe the background and situational knowledge necessary for understanding what the predicate is ""about"".",Each frame provides its specific set of semantic roles.,http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Baker et al., 1998)",N06-1017,Each frame provides its specific set of semantic roles.,"The Berkeley FrameNet project (Baker et al., 1998) is building a semantic lexicon for English describing the frames and linking them to the words and expressions that can evoke them.","These can be verbs as well as nouns, adjectives, prepositions, adverbs, and multiword expressions.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
(2005),N06-1017,"This is problematic for the use of FrameNet analyses as a basis for inferences over text, as e.g.",in Tatu and Moldovan (2005).,"For example, the verb prepare from Figure 1 is associated with the frames",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Lin, 1993)",N06-1017,"After removal of instances that were annotated with more than one sense, we obtain 26,496 annotated sentences for the 1,031 ambiguous lemmas.","They were parsed with Minipar (Lin, 1993); named entities were computed using Heart of Gold (Callmeier et al., 2004).",4 Experiment 1: WSD confidence scores,http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Callmeier et al., 2004)",N06-1017,"After removal of instances that were annotated with more than one sense, we obtain 26,496 annotated sentences for the 1,031 ambiguous lemmas.","They were parsed with Minipar (Lin, 1993); named entities were computed using Heart of Gold (Callmeier et al., 2004).",4 Experiment 1: WSD confidence scores,http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Erk, 2005)",N06-1017,Modeling.,"We test whether the WSD system built into SHALMANESER (Erk, 2005) can distinguish known sense items from unknown sense items reliably by its confidence scores.","The system extracts a rich feature set, which forms the basis of all three experiments in this paper:",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
(2002),N06-1017,"for verb targets, the target voice.",The feature set is based on Florian et al (2002) but contains additional syntax-related features.,"Each word-related feature is represented as four features for word, lemma, part of speech, and named entity.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Markou and Singh, 2003a",N06-1017,"In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not.","Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).","Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"Markou and Singh, 2003b",N06-1017,"In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not.","Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).","Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"Marsland, 2003)",N06-1017,"In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not.","Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).","Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Hickinbotham and Austin, 2000)",N06-1017,"Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).","Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).","One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Tax and Duin, 1998",N06-1017,"Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).","Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).","One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"Scholkopf et al., 2000)",N06-1017,"Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).","Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).","One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Yeung and Chow, 2002",N06-1017,"Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).","Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).","One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"Dasgupta and Forrest, 1999)",N06-1017,"Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of ""normality"" to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003).","Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Scholkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999).","One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
(2000),N06-1017,"One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set.","Rather than estimating the complete density function, Tax and Duin (2000) approximate local density at the test object by comparing distances between nearest neighbors.","Given a test object x, the approach considers the training object t nearest to x and compares the distance dxt between x and t to the distance dtt, between t and its own nearest training data neighbor t'.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Mount and Arya, 2005)",N06-1017,"We model unknown sense detection as an outlier detection task, using Tax and Duin's outlier detection approach that we have outlined in the previous section.","Nearest neighbors (by Euclidean distance) were computed using the ANN tool (Mount and Arya, 2005).",We compute one outlier detection model per lemma.,http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Markou and Singh, 2003a",N06-1017,One obvious possibility is the extension of Tax and Duin's method to more than one nearest training neighbor for a more accurate estimate of local density.,"Furthermore, more sophisticated feature vectors can be employed to generalize over context words, and other outlier detection approaches (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003) can be tested on this task.","Our immediate goal is to use unknown sense detection in combination with WSD, to filter out items that the WSD system cannot handle due to missing senses.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"Markou and Singh, 2003b",N06-1017,One obvious possibility is the extension of Tax and Duin's method to more than one nearest training neighbor for a more accurate estimate of local density.,"Furthermore, more sophisticated feature vectors can be employed to generalize over context words, and other outlier detection approaches (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003) can be tested on this task.","Our immediate goal is to use unknown sense detection in combination with WSD, to filter out items that the WSD system cannot handle due to missing senses.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"Marsland, 2003)",N06-1017,One obvious possibility is the extension of Tax and Duin's method to more than one nearest training neighbor for a more accurate estimate of local density.,"Furthermore, more sophisticated feature vectors can be employed to generalize over context words, and other outlier detection approaches (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003) can be tested on this task.","Our immediate goal is to use unknown sense detection in combination with WSD, to filter out items that the WSD system cannot handle due to missing senses.",http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Widdows, 2003",N06-1017,"Once items have been identified as unknown, they are available for further processing: If possible one would like to assign some measure of sense information even to these items.","Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.",,http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"Curran, 2005",N06-1017,"Once items have been identified as unknown, they are available for further processing: If possible one would like to assign some measure of sense information even to these items.","Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.",,http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"Burchardt et al., 2005)",N06-1017,"Once items have been identified as unknown, they are available for further processing: If possible one would like to assign some measure of sense information even to these items.","Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.",,http://www.aclweb.org/anthology/N/N06/N06-1017.pdf
"(Decadt et al., 2004)",P05-3014,"In recent SENSEVAL-3 evaluations, the most successful approaches for all words word sense disambiguation relied on information drawn from annotated corpora.","The system developed by (Decadt et al., 2004) uses two cascaded memory-based classifiers, combined with the use of a genetic algorithm for joint parameter optimization and feature selection.","A separate ""word expert"" is learned for each ambiguous word, using a concatenated corpus of English sense",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf
"(Yuret, 2004)",P05-3014,The performance of this system on the SENSEVAL-3 English all words data set was evaluated at 65.2%.,"Another top ranked system is the one developed by (Yuret, 2004), which combines two Naive Bayes statistical models, one based on surrounding collocations and another one based on a bag of words around the target word.","The statistical models are built based on SemCor and WordNet, for an overall disambiguation accuracy of 64.1%.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf
"(Mihalcea and Faruque, 2004)",P05-3014,"The statistical models are built based on SemCor and WordNet, for an overall disambiguation accuracy of 64.1%.","A different version of our own SENSELEARNER system (Mihalcea and Faruque, 2004), using three of the semantic models described in this paper, combined with semantic generalizations based on syntactic dependencies, achieved a performance of 64.6%.",3 SenseLearner,http://www.aclweb.org/anthology/P/P05/P05-3014.pdf
"(Miller et al., 1993)",P05-3014,"SENSELEARNER is attempting to learn general semantic models for various word categories, starting with a relatively small sense-annotated corpus.","We base our experiments on SemCor (Miller et al., 1993), a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers.",The input to the disambiguation algorithm consists of raw text.,http://www.aclweb.org/anthology/P/P05/P05-3014.pdf
"(Miller, 1995)",P05-3014,The output is a text with word meaning annotations for all open-class words.,"The algorithm starts with a preprocessing stage, where the text is tokenized and annotated with part-ofspeech tags; collocations are identified using a sliding window approach, where a collocation is defined as a sequence of words that forms a compound concept defined in WordNet (Miller, 1995); named entities are also identified at this stager.","Next, a semantic model is learned for all predefined word categories, which are defined as groups of words that share some common syntactic or semantic properties.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf
"(Mihalcea and Faruque, 2004)",P05-3014,The words that are not covered by these models (typically about 10-15% of the words in the test corpus) are assigned with the most frequent sense in WordNet.,"An alternative solution to this second step was suggested in (Mihalcea and Faruque, 2004), using semantic generalizations learned from dependencies identified between nodes in a conceptual network.","Their approach however, although slightly more accurate, conflicted with our goal of creating an efficient WSD system, and therefore we opted for the simpler backoff method that employs WordNet sense frequencies.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf
"(Decadt et al., 2004)",P05-3014,Different semantic models can be defined and trained for the disambiguation of different word categories.,"Although more general than models that are built individually for each word in a test corpus (Decadt et al., 2004), the applicability of the semantic models built as part of SENSELEARNER is still limited to those words previously seen in the training corpus, and therefore their overall coverage is not 100%.","Starting with an annotated corpus consisting of all annotated files in SemCor, a separate training data set is built for each model.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf
"(Daelemans et al., 2001)",P05-3014,"Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model.","For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002), (Mihalcea, 2002).","Following the learning stage, each vector in the test data set is labeled with a predicted word and sense.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf
"(Hoste et al., 2002)",P05-3014,"Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model.","For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002), (Mihalcea, 2002).","Following the learning stage, each vector in the test data set is labeled with a predicted word and sense.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf
"(Mihalcea, 2002)",P05-3014,"Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model.","For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002), (Mihalcea, 2002).","Following the learning stage, each vector in the test data set is labeled with a predicted word and sense.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf
"(Mihalcea and Moldovan, 2002)",P05-3014,"A baseline, computed using the most frequent sense in WordNet, is also indicated.","The best results reported on these data sets are 69.0% on SENSEVAL-2 data (Mihalcea and Moldovan, 2002),",55,http://www.aclweb.org/anthology/P/P05/P05-3014.pdf
"(Decadt et al., 2004)",P05-3014,binations of contextual (model*1/2) and collocational (model*Coll) models are also included.,"and 65.2% on SENSEVAL-3 data (Decadt et al., 2004).","Note however that both these systems rely on significantly larger training data sets, and thus the results are not directly comparable.",http://www.aclweb.org/anthology/P/P05/P05-3014.pdf
"(Kamp and Reyle, 1993)",W04-0208,"In RST (Mann and Thompson 1988), (Marcu 2000), these relations are ultimately between semantic elements corresponding to discourse units that can be simple sentences or clauses as well as entire discourses.","In SDRT (Asher and Lascarides 2003), these relations are between representations of propositional content, called Discourse Representation Structures (Kamp and Reyle, 1993).","Despite a considerable amount of very productive research, annotating such discourse relations has proved problematic.",http://www.aclweb.org/anthology/W/W04/W04-0208.pdf
(1994),W04-0208,"In this respect, they differ from TDMs, which do not commit to specific rhetorical relations.",Spejewski (1994) developed a tree-based model of the temporal structure of a sequence of sentences.,"Her approach is based on relations of temporal coordination and subordination, and is thus a major motivation for our own approach.",http://www.aclweb.org/anthology/W/W04/W04-0208.pdf
"(Pereira and Shabes, 1992)",P08-1100,Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima.,"Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective.","Since then, there has been a large body of work addressing the flaws of the EM-based approach.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"(Carroll and Charniak, 1992)",P08-1100,Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima.,"Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective.","Since then, there has been a large body of work addressing the flaws of the EM-based approach.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"(Clark, 2001",P08-1100,"Since then, there has been a large body of work addressing the flaws of the EM-based approach.","Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).",Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"Klein and Manning, 2004)",P08-1100,"Since then, there has been a large body of work addressing the flaws of the EM-based approach.","Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).",Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
(2005),P08-1100,"Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).",Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,"Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
(2006),P08-1100,"Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004).",Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,"Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"(Goldwater and Griffiths, 2007",P08-1100,Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,"Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).","Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"Johnson, 2007",P08-1100,Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,"Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).","Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"Kurihara and Sato, 2006)",P08-1100,Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure.,"Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006).","Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"(Klein and Manning, 2004)",P08-1100,"Our key idea for understanding this mis-match is to ""cheat"" and initialize EM with the true relationship and then study the ways in which EM repurposes our desired syntactic structures to increase likelihood.","We present a metamodel of the changes that EM makes and show how this tool can shed some light on the undesired biases of the HMM, the PCFG, and the dependency model with valence (Klein and Manning, 2004).",Identifiability error can be incurred when two distinct parameter settings yield the same probability distribution over sentences.,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"(Klein and Manning, 2004)",P08-1100,"We represent y as a multiset of binary rewrites of the form (y -* y1 y2), where y is a nonterminal and y1, y2 can be either nonterminals or terminals.","In the dependency model with valence (DMV) (Klein and Manning, 2004), the input x = (x1, ... , xm) is a sequence of POS tags and the output y specifies the directed links of a projective dependency tree.","The generative model is as follows: for each head xi, we generate an independent sequence of arguments to the left and to the right from a direction-dependent distribution over tags.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
(2004),P08-1100,"At each point, we stop with a probability parametrized by the direction and whether any arguments have already been generated in that direction.",See Klein and Manning (2004) for a formal description.,"In all our experiments, we used the Wall Street Journal (WSJ) portion of the Penn Treebank.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"(Klein and Manning, 2004)",P08-1100,We binarized the PCFG trees and created gold dependency trees according to the Collins head rules.,"We trained 45-state HMMs on all 49208 sentences, 11-state PCFGs on WSJ-10 (7424 sentences) and DMVs on WSJ-20 (25523 sentences) (Klein and Manning, 2004).",We ran EM for 100 iterations with the parameters initialized uniformly (always plus a small amount of random noise).,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"(Merialdo, 1994",P08-1100,4 Approximation error,"We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmax ]E log p(x, y), which acts as a surrogate for p. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly).",We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models.,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"Smith and Eisner, 2005",P08-1100,4 Approximation error,"We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmax ]E log p(x, y), which acts as a surrogate for p. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly).",We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models.,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"Haghighi and Klein, 2006)",P08-1100,4 Approximation error,"We start by analyzing approximation error, the discrepancy between p and p1 (the model found by optimizing likelihood), a point which has been discussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 Bgen = argmax ]E log p(x, y), which acts as a surrogate for p. As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly).",We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models.,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
(2005),P08-1100,"While seemingly benign, this symmetry actually presents a serious challenge in measuring discrepancy (Section 5.1).",Grenager et al (2005) augments an HMM to allow emission from a generic stopword distribution at any position with probability q.,"Their model would definitely not be identifiable if q were a free parameter, since we can set q to 0 and just mix in the stopword distribution with each of the other emission distributions to obtain a different parameter setting yielding the same overall distribution.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"(Kuhn, 1955)",P08-1100,"We can form a K xK matrix M, where each entry MZj is the distance between the parameters involving label i of B and label j of B0.","D(B  ||B0) can then be computed by finding a maximum weighted bipartite matching on M using the O(K3) Hungarian algorithm (Kuhn, 1955).","For models such as the HMM and PCFG, computing D is NP-hard, since the summation in d (1) contains both first-order terms which depend on one label (e.g., emission parameters) and higher-order terms which depend on more than one label (e.g., transitions or rewrites).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
(1992),P08-1100,"Figures 4(f,g) show how both likelihood and accuracy, which both start quite low, improve substantially over time for the HMM on artificial data.",Carroll and Charniak (1992) report that EM fared poorly with local optima.,"We do not claim that there are no local optima, but only that the likelihood surface that EM is optimizing can become smoother with more examples.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
(2006),P08-1100,"With more examples, there is less noise in the aggregate statistics, so it might be easier for EM to pick out the salient patterns.",Srebro et al (2006) made a similar observation in the context of learning Gaussian mixtures.,"They characterized three regimes: one where EM was successful in recovering the true clusters (given lots of data), another where EM failed but the global optimum was successful, and the last where both failed (without much data).",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"(Ron et al., 1998",P08-1100,There is also a rich body of theoretical work on learning latent-variable models.,"Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).","But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"Clark and Thollard, 2005",P08-1100,There is also a rich body of theoretical work on learning latent-variable models.,"Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).","But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"Adriaans, 1999)",P08-1100,There is also a rich body of theoretical work on learning latent-variable models.,"Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).","But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"(Dasgupta, 1999",P08-1100,There is also a rich body of theoretical work on learning latent-variable models.,"Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).","But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"Feldman et al., 2005)",P08-1100,There is also a rich body of theoretical work on learning latent-variable models.,"Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).","But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.",http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
(2007),P08-1100,"Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999), others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005).","But with the exception of Dasgupta and Schulman (2007), there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.",8 Conclusion,http://www.aclweb.org/anthology/P/P08/P08-1100.pdf
"(Lyman and Varian, 2003)",W10-3506,1 Introduction,"The volume of information available to users on the World Wide Web is growing at an exponential rate (Lyman and Varian, 2003).","Current keyword-matching information retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears (Metzler and Croft, 2006).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Metzler and Croft, 2006)",W10-3506,"The volume of information available to users on the World Wide Web is growing at an exponential rate (Lyman and Varian, 2003).","Current keyword-matching information retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears (Metzler and Croft, 2006).","In recent years, much research attention has therefore been given to semantic techniques of information retrieval.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Tran et al., 2008)",W10-3506,"In recent years, much research attention has therefore been given to semantic techniques of information retrieval.","Such systems allow for sophisticated semantic search, however, require the use of a more difficult-to-understand querysyntax (Tran et al., 2008).","Furthermore, these methods require specially encoded (and thus costly) ontologies to describe the particular domain knowledge in which the system operates, and the specific interrelations of concepts within that domain.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Witten and Milne, 2008)",W10-3506,"New measures for computing similarity between individual concepts (inter-concept similarity, such as ""France"" and ""Great Britain""), as well as between documents (inter-document similarity) are proposed and tested.","It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) methods respectively.","Our methods outperform WLM in computing inter-concept similarity, and match ESA for inter-document similarity.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Gabrilovich and Markovitch, 2007)",W10-3506,"New measures for computing similarity between individual concepts (inter-concept similarity, such as ""France"" and ""Great Britain""), as well as between documents (inter-document similarity) are proposed and tested.","It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) methods respectively.","Our methods outperform WLM in computing inter-concept similarity, and match ESA for inter-document similarity.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Collins and Loftus, 1975)",W10-3506,2 Related Work and Overview,"Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).","Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Preece, 1982)",W10-3506,2 Related Work and Overview,"Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).","Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Crestani, 1997)",W10-3506,2 Related Work and Overview,"Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).","Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Witten and Milne, 2008)",W10-3506,"Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).","Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.","WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Gabrilovich and Markovitch, 2007)",W10-3506,"Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).","Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.","WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Budanitsky and Hirst, 2001)",W10-3506,"Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975), it has been applied computationally to IR with various levels of success (Preece, 1982), with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997).","Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001), arguably due to Wikipedia's larger conceptual coverage.","WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
(2009),W10-3506,Text is categorised as vectors in this concept space and similarity is computed as the cosine similarity of their ESA vectors.,"The most similar work to ours is Yeh (2009) in which the authors derive a graph structure from the inter-article links in Wikipedia pages, and then perform random walks over the graph to compute relatedness.","In Wikipedia, users create links between articles which are seen to be related to some degree.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Berger et al., 2004)",W10-3506,"In Wikipedia, users create links between articles which are seen to be related to some degree.","Since links relate one article to its neighbours, and by extension to their neighbours, we extract and process this hyperlink structure (using SA) as an Associative Network (AN) (Berger et al., 2004) of concepts and links relating them to one another.","The SA algorithm can briefly be described as an iterative process of propagating real-valued energy from one or more source nodes, via weighted links over an associative network (each such a propagation is called a pulse).",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Witten and Milne, 2008)",W10-3506,47,"Links into pages are used, since this leads to better results (Witten and Milne, 2008).","The Wikipedia graph structure is represented in an adjacency list structure, i.e.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Crestani, 1997)",W10-3506,4 Adapting Spreading Activation for Wikipedia's Hyperlink Structure,"Each pulse in the Spreading Activation (SA) process consists of three stages: 1) pre-adjustment, 2) spreading, and 3) post-adjustment (Crestani, 1997).","During pre- and post-adjustment, some form of activation decay is optionally applied to the active nodes.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Crestani, 1997)",W10-3506,(1) vjN(vi),"This pure model of SA has several significant problems, the most notable being that activation can saturate the entire network unless certain constraints are imposed, namely limiting how far activation can spread from the initially activated nodes (distance constraint), and limiting the effect of very highly-connected nodes (fanout constraint) (Crestani, 1997).",In the following three sections we discuss how these constraints were implemented in our model for SA.,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Salton and McGill, 1983)",W10-3506,"For instance, we consider a path connecting two nodes via a general article such as USA (connected to 322,000 articles) not nearly as indicative of a semantic relationship, as a path connecting them via a very specific concept, such as Hair Pin (only connected to 20 articles).","Inverse Link-Frequency (ILF) is inspired by the term-frequency inverse document-frequency (tf-idf) heuristic (Salton and McGill, 1983) in which a term's weight is reduced as it is contained in more documents in the corpus.","It is based on the idea that the more a term appears in documents across the corpus, the less it can discriminate any one of those documents.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
[0],W10-3506,ILF reaches a maximum of log |G |when |N(vi) |= 1 (see Equation 2).,"We therefore divide by log |G |to normalise its range to [0,1].",Threshold constraint,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
[1],W10-3506,ILF reaches a maximum of log |G |when |N(vi) |= 1 (see Equation 2).,"We therefore divide by log |G |to normalise its range to [0,1].",Threshold constraint,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Witten and Milne, 2008)",W10-3506,The second approach is called the Agglomerative Approach since we agglomerate all activations into one score resembling relatedness.,"After spreading has terminated, relatedness is computed as the amount of overlap between the individual nodes' activation vectors, using either the cosine similarity (AA-cos), or an adapted version of the information theory based WLM (Witten and Milne, 2008) measure.",Assume the same set of initial nodes vi and vj.,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Witten and Milne, 2008)",W10-3506,(4) ||Ai |Aj||,"For our adaptation of the Wikipedia Link-based Measure (WLM) approach to spreading activation, we define the WLM Agglomerative Approach (henceforth called AA-wlm2) as 2AA-wlm is our adaptation of WLM (Witten and Milne, 2008) for SA, not to be confused with their method, which we simply call WLM.","simAA,cos(Ai, Aj) A Ai  Aj",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
(2007),W10-3506,Experimental Method,"In order to compare our method with results reported by Gabrilovich and Markovitch (2007) and Witten and Milne (2008), we followed the same approach by randomly selecting Algorithm 1 Pseudo code to spread activation depth-first from node vi up to level Lp,max, using global decay d, and threshold T, given an adjacency list graph structure G and a weighting scheme W such that 0 < wig E W < 1.","Require: G, Lp,-ax, d, T function SPREAD UNIDIR(vi, A, P) if (vi, ai) E/ A or ai < T then > Threshold return end if Add vi to P > To avoid cycles for vj E N(vi) do > Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ < Lp,-ax then aj =aj+ai*wij*d Replace (vj, aj) E A with (vj, aj ) SPREAD UNIDIR(vj, A, P) end if end for return end function",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
(2008),W10-3506,Experimental Method,"In order to compare our method with results reported by Gabrilovich and Markovitch (2007) and Witten and Milne (2008), we followed the same approach by randomly selecting Algorithm 1 Pseudo code to spread activation depth-first from node vi up to level Lp,max, using global decay d, and threshold T, given an adjacency list graph structure G and a weighting scheme W such that 0 < wig E W < 1.","Require: G, Lp,-ax, d, T function SPREAD UNIDIR(vi, A, P) if (vi, ai) E/ A or ai < T then > Threshold return end if Add vi to P > To avoid cycles for vj E N(vi) do > Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ < Lp,-ax then aj =aj+ai*wij*d Replace (vj, aj) E A with (vj, aj ) SPREAD UNIDIR(vj, A, P) end if end for return end function",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Gabrilovich, 2002)",W10-3506,"Require: G, Lp,-ax, d, T function SPREAD UNIDIR(vi, A, P) if (vi, ai) E/ A or ai < T then > Threshold return end if Add vi to P > To avoid cycles for vj E N(vi) do > Process neighbours if (vj, aj) E/ A then aj = 0 end if if vj E/ P and JPJ < Lp,-ax then aj =aj+ai*wij*d Replace (vj, aj) E A with (vj, aj ) SPREAD UNIDIR(vj, A, P) end if end for return end function","50 word-pairs from the WordSimilarity-353 dataset (Gabrilovich, 2002) and correlating our method's scores with the human-assigned scores.","To reduce the possibility of overestimating the performance of our technique on a sample set that happens to be favourable to our technique, we furthermore implemented a technique of repeated holdout (Witten and Frank, 2005): Given a sample test set of N pairs of words with human-assigned ratings of relatedness, randomly divide this set into k parts of roughly equal size3.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Witten and Frank, 2005)",W10-3506,"50 word-pairs from the WordSimilarity-353 dataset (Gabrilovich, 2002) and correlating our method's scores with the human-assigned scores.","To reduce the possibility of overestimating the performance of our technique on a sample set that happens to be favourable to our technique, we furthermore implemented a technique of repeated holdout (Witten and Frank, 2005): Given a sample test set of N pairs of words with human-assigned ratings of relatedness, randomly divide this set into k parts of roughly equal size3.",Hold out one part of the data and iteratively evaluate the performance of the algorithm on the remaining k1 parts until all k parts have been held out once.,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Witten and Milne, 2008)",W10-3506,"Secondly, the AA-cos spreading strategy significantly outperforms the AA-wlm strategy over this sample set (pmax,wlm = 0.60 vs pmax,cos = 0.70).","These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008) (p = 0.69) and ESA (Gabrilovich and Markovitch, 2007) (p = 0.75).","Maximum path length Lp,max is related to how far one node can spread its activation in the network.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Gabrilovich and Markovitch, 2007)",W10-3506,"Secondly, the AA-cos spreading strategy significantly outperforms the AA-wlm strategy over this sample set (pmax,wlm = 0.60 vs pmax,cos = 0.70).","These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008) (p = 0.69) and ESA (Gabrilovich and Markovitch, 2007) (p = 0.75).","Maximum path length Lp,max is related to how far one node can spread its activation in the network.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Csomai and Mihalcea, 2008)",W10-3506,"To compute document similarity, we first extract key representative Wikipedia concepts from a document to produce document concept vectors4.","This process is known as wikification (Csomai and Mihalcea, 2008), and we used an implementation of Milne and Witten (2008).","This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
(2008),W10-3506,"To compute document similarity, we first extract key representative Wikipedia concepts from a document to produce document concept vectors4.","This process is known as wikification (Csomai and Mihalcea, 2008), and we used an implementation of Milne and Witten (2008).","This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
(2005),W10-3506,"This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document.","We next present two algorithms, MAXSIM and WIKISPREAD, for computing document similarity, and test these over the Lee (2005) document similarity dataset, a set of 50 documents between 51 and 126 words each, with the averaged gold standard similarity ratings produced by 83 test subjects (see (Lee et al., 2005)).",The first metric we propose is called MAXSIM (see Algorithm 2) and is based on the idea of measuring document similarity by pairing up each Wikipedia concept in one document's concept vector with its most similar concept in the other document.,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Lee et al., 2005)",W10-3506,"This produces document concept vectors of the form Vi = {(id1, w1), (id2, w2), ...} with idi some Wikipedia article identifier and wi a weight denoting how strongly the concept relates to the current document.","We next present two algorithms, MAXSIM and WIKISPREAD, for computing document similarity, and test these over the Lee (2005) document similarity dataset, a set of 50 documents between 51 and 126 words each, with the averaged gold standard similarity ratings produced by 83 test subjects (see (Lee et al., 2005)).",The first metric we propose is called MAXSIM (see Algorithm 2) and is based on the idea of measuring document similarity by pairing up each Wikipedia concept in one document's concept vector with its most similar concept in the other document.,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
(2007),W10-3506,Table 4: Summary of final document similarity correlations over the Lee & Pincombe document similarity dataset.,ESA score from Gabrilovich and Markovitch (2007).,Pearson p Cosine VSM (with tf-idf) only 0.56 MaxSim method 0.68 WikiSpread method 0.62 ESA 0.72 Combined (Cosine + MaxSim) 0.72,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Salton and McGill, 1983)",W10-3506,We therefore hypothesise that combining the two approaches will lead to more robust document similarity performance.,"Therefore, the final document similarity metric we evaluate (COMBINED) is a linear combination of the best-performing Wikipedia-based methods described above, and the well-known Vector Space Model (VSM) with cosine similarity and tf-idf (Salton and McGill, 1983).",Results,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
(2005),W10-3506,Results,"The results obtained on the Lee (2005) document similarity dataset using the three document similarity metrics (MAXSIM, WIKISPREAD, and COMBINED) are summarised in Table 4.","Of the two Wikipedia-only methods, the MaxSim method achieves the best correlation score of p = 0.68.",http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
(2005),W10-3506,This suggests that selective knowledge-based augmentation of simple VSM methods can lead to more accurate document similarity performance.,Figure 1: Parameter sweep over A showing contributions from cosine (A) and Wikipedia-based MAXSIM method (1  A) to the final performance over the Lee (2005) dataset.,9 Conclusion,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Gabrilovich and Markovitch, 2007)",W10-3506,"Finally, we show that using our best Wikipedia-based method to augment the cosine VSM method using tf-idf, leads to the best results.","The final result of p = 0.72 is equal to that reported for ESA (Gabrilovich and Markovitch, 2007), while requiring less than 10% of the Wikipedia database required for ESA.",Table 4 summarises the document-similarity results.,http://www.aclweb.org/anthology/W/W10/W10-3506.pdf
"(Brown et al., 1993)",P05-1058,1 Introduction,"Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).","In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu, 1997",P05-1058,"Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).","In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"Och and Ney, 2003",P05-1058,"Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).","In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"Cherry and Lin, 2003)",P05-1058,"Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).","In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Smadja et al., 1996",P05-1058,"Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).","In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"Ahrenberg et al., 1998",P05-1058,"Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).","In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"Tufis and Barbu, 2002)",P05-1058,"Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993).","In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links.","In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Ker and Chang, 1997)",P05-1058,"In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training.","When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997).","However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu and Wang, 2004)",P05-1058,"When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997).","However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available.","In this paper, we address the problem of word alignment in a specific domain, in which only a small-scale corpus is available.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Iyer et al., 1997)",P05-1058,We implement this by using alignment model adaptation.,"Although the adaptation technology is widely used for other tasks such as language modeling (Iyer et al., 1997), only a few studies, to the best of our knowledge, directly address word alignment adaptation.",Wu and Wang (2004) adapted the alignment results obtained with the out-of-domain corpus to the results obtained with the in-domain corpus.,http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
(2004),P05-1058,"Although the adaptation technology is widely used for other tasks such as language modeling (Iyer et al., 1997), only a few studies, to the best of our knowledge, directly address word alignment adaptation.",Wu and Wang (2004) adapted the alignment results obtained with the out-of-domain corpus to the results obtained with the in-domain corpus.,"This method first trained two models and two translation dictionaries with the in-domain corpus and the out-of-domain corpus, respectively.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Brown et al., 1993)",P05-1058,2 Statistical Word Alignment,"According to the IBM models (Brown et al., 1993), the statistical word alignment model can be generally represented as in Equation (1).","f e , ) = | a '  |e)  p(a' ,  |) f e (1) , a ( f p ( a p",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Al-Onaizan et al., 1999)",P05-1058,"f e , ) = | a '  |e)  p(a' ,  |) f e (1) , a ( f p ( a p","In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999), which is shown in Equation (2).","This simplified version does not take word classes into account as described in (Brown et al., 1993).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Brown et al., 1993)",P05-1058,"In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999), which is shown in Equation (2).","This simplified version does not take word classes into account as described in (Brown et al., 1993).","p(a,  |e) =  Pr( ,  |) f ( , )   e   l m , , 1 j = j : 0 a j =  n(i  |ei ) i i =1 i=1 m m e) m 0 l l |  ,  e)Pr(=  p(a, | f ( , )     m 0 = 0   p   20 0 p1  ! )",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Brown et al., 1993)",P05-1058,"p(a,  |e) =  Pr( ,  |) f ( , )   e   l m , , 1 j = j : 0 a j =  n(i  |ei ) i i =1 i=1 m m e) m 0 l l |  ,  e)Pr(=  p(a, | f ( , )     m 0 = 0   p   20 0 p1  ! )","d j a ( | j t (fj   |e j a (3) 1 A cept is defined as the set of target words connected to a source word (Brown et al., 1993). )",") ( |   t f e j a j i 1, 0 aj= = l m  )  n(i  |e = 1 = i j 1 m j j= 1, 0 a j = m 0 ( ([ ( )] ( j h a d j c =   )) j 1 aj m ([ ( )] ( ( )))) j h a d j p j =   j > 1   + 0 =   p   m 0  2  0 0 p1 } i = max{i ': i' >",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Och and Ney, 2000)",P05-1058,"Thus, some multi-word units in the domain-specific corpus cannot be correctly aligned.","In order to deal with this problem, we perform word alignment in two directions (source to target, and target to source) as described in (Och and Ney, 2000).",The GIZA++ toolkit2 is used to perform statistical word alignment.,http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"more frequently in a specific domain than in the general domain, it can usually be considered as a domain-specific word et al., 2001)",P05-1058,Equation (8) indicates that if a word occurs,"more frequently in a specific domain than in the general domain, it can usually be considered as a domain-specific word et al., 2001).",For,http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Och and Ney, 2000)",P05-1058,"By taking the intersection of the two word alignment results, we build a new alignment set.","The alignment links in this intersection set are extended by iteratively adding word alignment links into it as described in (Och and Ney, 2000).","Based on the extended alignment links, we build a translation dictionary.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Dunning, 1993)",P05-1058,"Based on the extended alignment links, we build a translation dictionary.","In order to filter the noise caused by the error alignment links, we only retain those translation pairs whose log-likelihood ratio scores (Dunning, 1993) are above a threshold.",Based on the alignment results on the d one-to-many alignments.,http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu and Wang, 2004)",P05-1058,6 Evaluation,"The first method is descri ed in (Wu and Wang, 2004).","We call it ""Result Adaptation (ResAdapt)"".",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Melamed, 1997)",P05-1058,The detailed algorithm is shown in Figure 2.,"For each sentence pair, there are two different word alignment results, from which the final alignment links are selected according to their translation probabilities in the dictionary D. The selection order is similar to that in the competitive linking algorithm (Melamed, 1997).",The difference is that we allow many-to-one an We compare our method with four other methods.,http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu and Wang, 2004)",P05-1058,6.2 Evaluation Metrics,"We use the same evaluation metrics as described in (Wu and Wang, 2004).","If we use to represent SG the set of alignment links identified by the proposed methods and to denote the reference SC alignment set, the methods to calculate the precision, recall, f-measure, and alignment error rate (AER) are shown in Equation (13), (14), (15), and (16).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu and Wang, 2004)",P05-1058,Results,"In order to further compare our method with the method described in (Wu and Wang, 2004).","We do another experiment using almost the same-scale in-domain training corpus as described in (Wu and Wang, 2004).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu and Wang, 2004)",P05-1058,"In order to further compare our method with the method described in (Wu and Wang, 2004).","We do another experiment using almost the same-scale in-domain training corpus as described in (Wu and Wang, 2004).","From the in-domain training corpus, we randomly select about 500 sentence pairs to build the smaller training set.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu and Wang, 2004)",P05-1058,472,"while the method ""ResAdapt"" described in (Wu and Wang, 2004) only achieves an error rate reduction of 8.59%.","Compared with the method ""ResAdapt"", our method achieves an error rate reduction of 10.15%.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu and Wang, 2004)",P05-1058,"Compared with the method ""ResAdapt"", our method achieves an error rate reduction of 10.15%.","This result is different from that in (Wu and Wang, 2004), where their method achieved an error rate reduction of 21.96% as compared with the method ""Gen+Spec"".","The main reason is that the in-domain training corpus and testing corpus in this paper are different from those in (Wu and Wang, 2004).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu and Wang, 2004)",P05-1058,"This result is different from that in (Wu and Wang, 2004), where their method achieved an error rate reduction of 21.96% as compared with the method ""Gen+Spec"".","The main reason is that the in-domain training corpus and testing corpus in this paper are different from those in (Wu and Wang, 2004).","The training data and the testing data described in (Wu and Wang, 2004) are from a single manual.",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu and Wang, 2004)",P05-1058,"The main reason is that the in-domain training corpus and testing corpus in this paper are different from those in (Wu and Wang, 2004).","The training data and the testing data described in (Wu and Wang, 2004) are from a single manual.",The data in our corpus are from several manuals describing how to use the diagnostic ultrasound systems.,http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
(2000),P05-1058,The data in our corpus are from several manuals describing how to use the diagnostic ultrasound systems.,"In addition to the above evaluations, we also evaluate our model adaptation method using the ""refined"" combination in Och and Ney (2000) instead of the translation dictionary.","Using the ""refined"" method to select the alignments produced by our model adaptation method (AER: 0.2371) still yields better result than directly combining out-of-domain and in-domain corpora as training data of the ""refined"" method (AER: 0.2290).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu and Wang, 2004)",P05-1058,Our method achieves a relative error rate reduction of 17.43% as compared with the method directly combining the out-of-domain corpus and the in-domain corpus as training data.,"It also achieves a relative error rate reduction of 6.56% as compared with the previous work in (Wu and Wang, 2004).","In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004), our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004).",http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu and Wang, 2004)",P05-1058,"It also achieves a relative error rate reduction of 6.56% as compared with the previous work in (Wu and Wang, 2004).","In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004), our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004).",We also use in-domain corpora and out-of-domain corpora of different sizes to perform adaptation experiments.,http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
"(Wu and Wang, 2004)",P05-1058,"It also achieves a relative error rate reduction of 6.56% as compared with the previous work in (Wu and Wang, 2004).","In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004), our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004).",We also use in-domain corpora and out-of-domain corpora of different sizes to perform adaptation experiments.,http://www.aclweb.org/anthology/P/P05/P05-1058.pdf
(1995),W12-1635,Little work has been reported on measures of the relationship between dialogue complexity and the semantic structure of a DS application's database.,"Zadrozny (1995) proposes Q-Complexity, which roughly corresponds to vocabulary size, and is essentially the number of questions that can be asked about a database.","Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf
(2000),W12-1635,"Zadrozny (1995) proposes Q-Complexity, which roughly corresponds to vocabulary size, and is essentially the number of questions that can be asked about a database.","Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space.",Gorin et al (2000) distinguish between semantic and linguistic complexity of calls to a spoken DS.,http://www.aclweb.org/anthology/W/W12/W12-1635.pdf
(2000),W12-1635,"Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space.",Gorin et al (2000) distinguish between semantic and linguistic complexity of calls to a spoken DS.,"Semantic complexity is measured by inheritance relations between call types, the number of type labels per call, and how often calls are routed to human agents.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf
(2003),W12-1635,"Linguistic complexity is measured by utterance length, vocabulary size and perplexity.","Popescu et al (2003) identify a class of ""semantically tractable"" natural language questions that can be mapped to an SQL query to return the question's unique correct answer.",Ambiguous questions with multiple correct answers are not considered semantically tractable.,http://www.aclweb.org/anthology/W/W12/W12-1635.pdf
(2008),W12-1635,Ambiguous questions with multiple correct answers are not considered semantically tractable.,"Polifroni and Walker (2008) address how to present informative options to users who are exploring a database, for example, to choose a restaurant.","When a query returns many options, their system summarizes the return using attribute value pairs shared by many of the members.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf
[0],W12-1635,"A larger query return size indicates a more ambiguous attribute, one less able to distinguish among instances in I.","To produce specificity values in the range [0, 1], w(j) should decrease as j increases, but not penalize any query that returns a single instance, that is, w(1) = 1.","The faster w decreases, the more it penalizes an ambiguous attribute.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf
[1],W12-1635,"A larger query return size indicates a more ambiguous attribute, one less able to distinguish among instances in I.","To produce specificity values in the range [0, 1], w(j) should decrease as j increases, but not penalize any query that returns a single instance, that is, w(1) = 1.","The faster w decreases, the more it penalizes an ambiguous attribute.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf
"(Polifroni and Walker, 2008)",W12-1635,"When the system cannot uniquely identify a requested book, it begins a disambiguation subdialogue, an example of which is shown in Figure 1.","To avoid addressing information presentation issues such as those explored in (Polifroni and Walker, 2008), CheckItOut followed a simple strategy of offering each next candidate book in a query return, and user studies with CheckItOut restricted query return size to a maximum of three books.","For the simulations, we expect an inverse relationship between specificity and dialogue length.",http://www.aclweb.org/anthology/W/W12/W12-1635.pdf
(2006),W07-1402,This paper reports on the system we used in the third PASCAL challenge on Recognizing Textual Entailment.,"The system is based to a large extent on Burchardt and Frank's system (2006) used in the second RTE challenge (Bar-Haim et al., 2006); it relies on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap.","As the system has been described earlier, we concentrate on a more systematic discussion of the system behaviour, aiming at spotting promising anchors for future extensions and improvements.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Bar-Haim et al., 2006)",W07-1402,This paper reports on the system we used in the third PASCAL challenge on Recognizing Textual Entailment.,"The system is based to a large extent on Burchardt and Frank's system (2006) used in the second RTE challenge (Bar-Haim et al., 2006); it relies on a relatively deep linguistic analysis, which we complement with a shallow component based on word overlap.","As the system has been described earlier, we concentrate on a more systematic discussion of the system behaviour, aiming at spotting promising anchors for future extensions and improvements.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Hickl et al., 2006",W07-1402,"As the system has been described earlier, we concentrate on a more systematic discussion of the system behaviour, aiming at spotting promising anchors for future extensions and improvements.","It has been observed for related systems that a combination of separately trained features in the machine learning component can lead to an overall improvement in system performance, in particular if features from a more ""informed"" component and shallow ones are combined (Hickl et al., 2006; Bos and Markert, 2006).",We provide a detailed analysis of our system's behaviour on different training and test sets.,http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"Bos and Markert, 2006)",W07-1402,"As the system has been described earlier, we concentrate on a more systematic discussion of the system behaviour, aiming at spotting promising anchors for future extensions and improvements.","It has been observed for related systems that a combination of separately trained features in the machine learning component can lead to an overall improvement in system performance, in particular if features from a more ""informed"" component and shallow ones are combined (Hickl et al., 2006; Bos and Markert, 2006).",We provide a detailed analysis of our system's behaviour on different training and test sets.,http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Burchardt and Frank, 2006)",W07-1402,"In this Section, we review the basic architecture of the SALSA RTE system, and report on some improvements and extensions.","More details can be found in (Burchardt and Frank, 2006).",2.1 Architecture,http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Baker et al., 1998)",W07-1402,2.1 Architecture,"The SALSA RTE system is based on three main components: (i) a linguistic analysis of text and hypothesis based primarily on LFG and Frame Semantics (Baker et al., 1998), (ii) the computation of a match graph that encodes the ""semantic overlap"" between text and hypothesis, and (iii) a statistical entailment decision.","Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 10-15, Prague, June 2007.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Riezler et al., 2002)",W07-1402,Linguistic analysis.,"The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al., 2005).",Frame semantic analysis is especially interesting for the task of recognising textual entailment as it offers a robust yet relatively precise measure for semantic overlap.,http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Erk and Pado, 2006)",W07-1402,Linguistic analysis.,"The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al., 2005).",Frame semantic analysis is especially interesting for the task of recognising textual entailment as it offers a robust yet relatively precise measure for semantic overlap.,http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Burchardt et al., 2005)",W07-1402,Linguistic analysis.,"The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al., 2005).",Frame semantic analysis is especially interesting for the task of recognising textual entailment as it offers a robust yet relatively precise measure for semantic overlap.,http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Fellbaum, 1997)",W07-1402,"The predicate discover is associated with the frame ACxIEVING_FIRST, the semantic role COGNIZER points to the pseudo-predicate Leloir and NEW_IDEA points to the PROCESS frame evoked by metabolism.","Semantic nodes are further projected into an ontological analysis layer containing WordNet (Fellbaum, 1997) senses and SUMO (Niles and Pease, 2001) classes.","Semantic phenomena not treated by FrameNet like anaphora, negation or modality are (approximately) encoded with special operators.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Niles and Pease, 2001)",W07-1402,"The predicate discover is associated with the frame ACxIEVING_FIRST, the semantic role COGNIZER points to the pseudo-predicate Leloir and NEW_IDEA points to the PROCESS frame evoked by metabolism.","Semantic nodes are further projected into an ontological analysis layer containing WordNet (Fellbaum, 1997) senses and SUMO (Niles and Pease, 2001) classes.","Semantic phenomena not treated by FrameNet like anaphora, negation or modality are (approximately) encoded with special operators.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Schafer, 2005)",W07-1402,Sentence splitter.,"To cope with longer texts, we integrated the sentence splitter of the JTok tokeniser (Schafer, 2005) into the system.",WordNet interface.,http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Schmid, 1994)",W07-1402,The shallow system measures the relative number of words in the hypothesis that also occur in the text.,"Both text and hypothesis are tagged and lemmatised using Tree Tagger (Schmid, 1994), taking only nouns, non-auxiliary verbs, adjectives and adverbs into account.","Training a decision tree on the relative word-overlap as single feature yields a system which performs comparable to earlier word-overlap based systems, achieving an accuracy of 60.6 % if trained and tested on the RTE 2 development and test set, respectively (using Weka's J48 classifier), or 57.5 % if we use Weka's LogitBoost classifier.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Witten and Frank, 2005)",W07-1402,Weka Interface.,"Finally, we improved the machine learning back-end which feeds our extracted features into the Weka toolkit (Witten and Frank, 2005).","This allows to train features in arbitrary combinations, with different machine learners.'",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Hickl et al., 2006)",W07-1402,"3In terms of machine learning, extending a training set by factor 2 (from 800 to 1.600 items) does not make a qualitative difference.","The improvement observed by (Hickl et al., 2006) was achieved by going to 10.000 items.","As can be seen from Table 2, in most of the cases, IV performs best, e.g.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Zaenen et al., 2005)",W07-1402,Although the hypothesis is logically entailed by the text (if we ignore the report context) - 'kill' implies 'possibly kill' - pragmatic principles seem to block entailment here.,"The observation that standard logical entailment and textual entailment deviate in certain respects is not surprising and has also been addressed in a discussion initiated by (Zaenen et al., 2005).","Still, there is no consensus regarding the precise mechanisms involved in the latter such as ""general principles of plausibility"" or pragmatic principles.",http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
"(Reiter, 2007)",W07-1402,"As we cannot expect the necessary amount of training data to be available in the near future, we currently investigate the data more closely in order to arrive at a more controlled model of textual entailment.","In another current effort, we work on an interface to upper-level ontologies (Reiter, 2007) in order to access more ""world-knowledge"" which is a desideratum in natural language processing in general, as in many approaches to textual entailment.",Acknowledgements,http://www.aclweb.org/anthology/W/W07/W07-1402.pdf
(2009),W10-4231,"The corpus is a collection of 2,000 introductory sections from Wikipedia articles about individual people in which all mentions of person entities have been annotated.","An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al (2009).","Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b).",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf
"(Greenbacker and McCoy, 2009a)",W10-4231,"An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al (2009).","Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b).","Although our system performed reasonably-well in predicting REG08Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task.",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf
"(Greenbacker and McCoy, 2009b)",W10-4231,"An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al (2009).","Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b).","Although our system performed reasonably-well in predicting REG08Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task.",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf
"(Belz et al., 2009)",W10-4231,"Although our system performed reasonably-well in predicting REG08Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task.","As suggested by the evaluators (Belz et al., 2009), this was due in large part to our reliance on the list of REs being in a particular order, which had changed for the NEG task.",2 Method,http://www.aclweb.org/anthology/W/W10/W10-4231.pdf
"(Greenbacker and McCoy, 2009c)",W10-4231,The first improvement we made to our existing methods related to the manner by which we selected the specific RE to employ.,"In 2009, we trained a series of decision trees to predict REG08Type based on our psycholinguistically-inspired feature set (described in (Greenbacker and McCoy, 2009c)), and then simply chose the first option in the list of REs matching the predicted type.","For 2010, we incorporated the case of each RE into our target attribute so that the decision tree classifier would predict both the type and case for the given reference.",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf
(2003),W10-4231,"For 2010, we incorporated the case of each RE into our target attribute so that the decision tree classifier would predict both the type and case for the given reference.","Then, we applied a series of rules governing the length of initial and subsequent REs involving a person's name (following Nenkova and McKeown (2003)), as well as 'backoffs' if the predicted type or case were not available.",Another improvement we made involved our method of determining whether the use of a pronoun would introduce ambiguity in a given context.,http://www.aclweb.org/anthology/W/W10/W10-4231.pdf
"(Favre and Bohnet, 2009)",W10-4231,Our efforts during this iteration of the NEG task were primarily focused on enhancing our methods of choosing the best RE once the reference type was selected.,"We remain several points below the bestperforming team from 2009 (ICSI-Berkeley), possibly due to the inclusion of additional items in their feature set, or the use of Conditional Random Fields as their learning technique (Favre and Bohnet, 2009).",5 Future Work,http://www.aclweb.org/anthology/W/W10/W10-4231.pdf
"(Favre and Bohnet, 2009)",W10-4231,5 Future Work,"Moving forward, we hope to expand our feature set by including the morphology of words immediately surrounding the reference, as well as a more extensive reference history, as suggested by (Favre and Bohnet, 2009).","We suspect that these features may play a significant role in determining the type of referenced used, the prediction of which acts as a 'bottleneck' in generating exact REs.",http://www.aclweb.org/anthology/W/W10/W10-4231.pdf
"Traitement Automatique des Langues Naturelles, Marseille, 2014",W14-6402,,"21' Traitement Automatique des Langues Naturelles, Marseille, 2014 [FondamenTAL-O.2]",Representation ontologique du LVF et son utilisation en traitement automatique de la langue,http://www.aclweb.org/anthology/W/W14/W14-6402.pdf
(2010),W14-6402,"E cause de problemes de diffusion et de distribution, le LVF n'a malheureusement pas pu etre exploit& par les chercheurs et les linguistes qui, pour plusieurs, en ignoraient meme l'existence.","Certains travaux ont rendu le LVF plus accessible en termes d'encodage et de format de donn&es : Denis Le Pesant en a cr&& une version sous format Excel pour faciliter sa consultation manuelle, mais ce mode d'acc&s ne s'est pas av&r& pratique pour les applications informatiques ; Guy Lapalme en a alors propos& une version XML qui en facilite l'exploitation par les applications de traitement automatique de la langue et Hadouche et Lapalme (2010) l'ont compar&  d'autres ressources lexicales.","Ces derni&res ann&es, il y a eu un regain d'int&ret pour la notion d'ontologie, sous l'impulsion du web s&mantique.",http://www.aclweb.org/anthology/W/W14/W14-6402.pdf
"(Noy et McGuinness, 2000)",W14-6402,On fournit ainsi des ontologies qui sont des ressources conceptuelles repr&sent&es par ces langages mod&lisant les domaines des connaissances et on facilite leur acc&s et leur partage.,"Les ontologies repr&sentent des ressources de mod&lisation et de conceptualisation tr&s importantes (Noy et McGuinness, 2000).","Elles constituent en soi un modele de donn&es repr&sentatif d'un ensemble de concepts dans un domaine, ainsi que des relations entre ces concepts.",http://www.aclweb.org/anthology/W/W14/W14-6402.pdf
(1999),W14-6402,"Certaines approches proposent une m&thode g&n&rique de transformation XML en modele OWL  partir d'un sch&ma XML et des donn&es du fichier XML, d'autres pensent qu'il est impossible de proposer une approche automatique convenable pour une transformation automatique complete de XML vers OWL, car XML ne d&finit aucune contrainte s&mantique.","Contrairement  cela, d'autres approches consid&rent qu'il y a une s&mantique dans les documents XML qui peut etre d&couverte  partir de la structure des documents, en l'occurrence l'approche de Melnik (1999).","Meme si XML n'est pas cens& repr&senter d'informations s&mantiques ou de s&mantique entre les donn&es, les balises imbriqu&es peuvent repr&senter une relation is-a ou part-of ou subType-of.",http://www.aclweb.org/anthology/W/W14/W14-6402.pdf
"(Enright et al., 2002",W11-1403,national project on item generation for testing student competencies in solving probability problems.,"Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).",The goal of our item generation project is to develop a model to support optimal problem and test construction.,http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"Deane and Sheehan, 2003",W11-1403,national project on item generation for testing student competencies in solving probability problems.,"Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).",The goal of our item generation project is to develop a model to support optimal problem and test construction.,http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"Arendasy et al., 2006",W11-1403,national project on item generation for testing student competencies in solving probability problems.,"Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).",The goal of our item generation project is to develop a model to support optimal problem and test construction.,http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"Holling et al., 2009)",W11-1403,national project on item generation for testing student competencies in solving probability problems.,"Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009).",The goal of our item generation project is to develop a model to support optimal problem and test construction.,http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"iron and Williamson, 2002",W11-1403,"(Left: German original, right: English translation.)","iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).","A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"Arendasy et al., 2006",W11-1403,"(Left: German original, right: English translation.)","iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).","A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"Holling et al., 2009)",W11-1403,"(Left: German original, right: English translation.)","iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).","A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"(Deane and Sheehan, 2003",W11-1403,"iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).","A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).","However, this system focuses on semantic factors influencing the expression of events with different participants (e.g., different types of vehicles) rather than on generating linguistic variations.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"Higgins et al., 2005)",W11-1403,"iron and Williamson, 2002; Arendasy et al., 2006; Holling et al., 2009).","A system that uses a linguistically sophisticated approach, thus in principle allowing for similar text variations as Genpex, is ModelCreator (Deane and Sheehan, 2003; Higgins et al., 2005).","However, this system focuses on semantic factors influencing the expression of events with different participants (e.g., different types of vehicles) rather than on generating linguistic variations.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
(2011),W11-1403,A warning is also issued if the edited problem contains properties for which no lexical information is available.,See Boer Rookhuiszen (2011) for more details on how probability problems are constructed.,3 Language Generation,http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
(2000),W11-1403,An overview of the NLG component of Genpex is given in Figure 3.,"Its architecture reflects the language generation pipeline of Reiter and Dale (2000), with three modules: Document Planner, Microplanner and Surface Realizer.","Information between the modules is exchanged in the form of a list of sentence trees, each defining the content and grammatical structure of a sentence.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
(2003),W11-1403,"When introducing variation in the narrative form of the exercise, it is important that variations of the same exercise should all have the same meaning and approximately the same difficulty.","According to Deane and Sheehan (2003), it is possible to change the wording of a text without changing its difficulty.","Reiter and Dale (2000) state that for example aggregating multiple sentences does not change the information they express, but improves the readability and fluency of the text.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
(2000),W11-1403,"According to Deane and Sheehan (2003), it is possible to change the wording of a text without changing its difficulty.","Reiter and Dale (2000) state that for example aggregating multiple sentences does not change the information they express, but improves the readability and fluency of the text.",This is what we want to achieve: adding variation to the text without affecting its interpretation.,http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
(2002),W11-1403,"Given that understanding the question is crucial for solving the exercise, and that varying the way the questions are asked might cause confusion, we chose to adhere to a fixed format for the questions, cf.",Fairon and Williams (2002).,Aggregation.,http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"(Harbusch and Kempen, 2009)",W11-1403,Ellipsis.,"This is the removal of duplicate words from sentences, which typically applies to aggregated sentences (Harbusch and Kempen, 2009).","Genpex can apply different types of ellipsis, such as Gapping and Conjunction Reduction.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
(2010),W11-1403,"As a consequence, the system frequently applies too much or too little ellipsis to the generated sentences, with less than ideal (though not ungrammatical) results.","The existence of such preferred formulations is in line with the results of Cahill and Forst (2010), who carried out an experiment in which native speakers of German evaluated a number of alternative realisations of the same sentence.","Their subjects accepted some variation in word order, but showed a clear preference for some of the alternatives.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
(2005),W11-1403,"The underlying probability problem is saved together with the text as well, so all factors that certainly or potentially influence item difficulty are known.","This makes it relatively easy to test the influence of those factors on the difficulty of the exercise, for example by carrying out the kind of statistical and cognitive analysis advocated by Graf et al (2005).","The effect of the main parameters of the probability problems in Genpex (i.e., the type of question being asked) was already statistically analyzed by Holling et al (2009) and Zeuch (In preparation).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
(2009),W11-1403,"This makes it relatively easy to test the influence of those factors on the difficulty of the exercise, for example by carrying out the kind of statistical and cognitive analysis advocated by Graf et al (2005).","The effect of the main parameters of the probability problems in Genpex (i.e., the type of question being asked) was already statistically analyzed by Holling et al (2009) and Zeuch (In preparation).","They used automatically generated items similar to the exercises generated by Genpex, except that their exercises did not have variations in wording apart from context-related ones.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
(2009),W11-1403,"They used automatically generated items similar to the exercises generated by Genpex, except that their exercises did not have variations in wording apart from context-related ones.","Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.","Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"(Enright et al., 2002",W11-1403,"Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.","Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).","The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"Fairon and Williamson, 2002",W11-1403,"Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.","Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).","The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"Deane and Sheehan, 2003",W11-1403,"Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.","Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).","The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"Arendasy et al., 2006",W11-1403,"Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.","Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).","The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"Holling et al., 2008",W11-1403,"Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.","Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).","The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"Holling et al., 2009)",W11-1403,"Also, the exercises used by Holling et al (2009) mentioned probabilities instead of counts in the statements.","Once we know more about the effects of the textual variations, Genpex can be of great value to test developers, given that there exists a great need for large amounts of learning and assessment materials with a controlled level of difficulty (Enright et al., 2002; Fairon and Williamson, 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2008; Holling et al., 2009).","The initial development and testing of the system is a one-time investment, which we expect will pay off afterward when large amounts of test items can be created with little effort.",http://www.aclweb.org/anthology/W/W11/W11-1403.pdf
"(Marcus, 1980)",P97-1062,2 Basic Parsing Paradigm,"As the basic mechanism for parsing text into a shallow semantic representation, we choose a shiftreduce type parser (Marcus, 1980).",It breaks parsing into an ordered sequence of small and manageable parse actions such as shift and reduce.,http://www.aclweb.org/anthology/P/P97/P97-1062.pdf
"(Quinlan, 1986)",P97-1062,"A set of parse examples, as already described in the previous section, is then fed into an 1D3-based learning routine that generates a decision structure, which can then 'classify' any given parse state by proposing what parse action to perform next.","We extended the standard ID3 model (Quinlan, 1986) to more general hybrid decision structures.","In our tests, the best performing structure was a decision list (Rivest, 1987) of hierarchical decision trees, whose simplified basic structure is illustrated in figure 3.",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf
"(Rivest, 1987)",P97-1062,"We extended the standard ID3 model (Quinlan, 1986) to more general hybrid decision structures.","In our tests, the best performing structure was a decision list (Rivest, 1987) of hierarchical decision trees, whose simplified basic structure is illustrated in figure 3.","Note that in the 'reduce operation tree', the system first decides whether or not to perform a reduction before deciding on a specific reduction.",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf
"(Smadja et al., 1996)",P97-1062,"ambiguous with respect to German, but the English compound conclusively maps to the German compound ""Zinssatz"".","We believe that an extensive collection of complex translation pairs in the bilingual dictionary is critical for translation quality and we are confident that its acquisition can be at least partially automated by using techniques like those described in (Smadja et al., 1996).",Complex translation entries are preprocessed using the same parser as for normal text.,http://www.aclweb.org/anthology/P/P97/P97-1062.pdf
"(Simmons and Yu, 1992)",P97-1062,7 Related Work,"Our basic parsing and interactive training paradigm is based on (Simmons and Yu, 1992).","We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component.",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf
"(Magerman, 1995)",P97-1062,"We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component.","(Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993).","Questioning the traditional n-grams, Magerman already advocates a heavier reliance on contextual information.",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf
"(Marcus et al., 1993)",P97-1062,"We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component.","(Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993).","Questioning the traditional n-grams, Magerman already advocates a heavier reliance on contextual information.",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf
"(Collins, 1996)",P97-1062,"While this necessitates the involvement of a parsing supervisor for training, we are able to perform deterministic parsing and get already very good test results for only 256 training sentences.","(Collins, 1996) focuses on bigram lexical dependencies (BLD).","Trained on the same 40,000 sentences as Spatter, it relies on a much more limited type of context than our system and needs little background knowledge.",http://www.aclweb.org/anthology/P/P97/P97-1062.pdf
(1999),W07-2203,"Extant statistical parsers require extensive and detailed treebanks, as many of their lexical and structural parameters are estimated in a fullysupervised fashion from treebank derivations.",Collins (1999) is a detailed exposition of one such ongoing line of research which utilizes the Wall Street Journal (WSJ) sections of the Penn Treebank (PTB).,"However, there are disadvantages to this approach.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(2003),W07-2203,"Secondly, the richer the annotation required, the harder it is to adapt the treebank to train parsers which make different assumptions about the structure of syntactic analyses.","For example, Hockenmeier (2003) trains a statistical parser based on Combinatory Categorial Grammar (CCG) on the WSJ PTB, but first maps the treebank to CCG derivations semi-automatically.","Thirdly, many (lexical) parameter estimates do not generalize well between domains.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(2001),W07-2203,"Thirdly, many (lexical) parameter estimates do not generalize well between domains.","For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.","Tadayoshi et al (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(1999),W07-2203,"Thirdly, many (lexical) parameter estimates do not generalize well between domains.","For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.","Tadayoshi et al (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(2005),W07-2203,"For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins' (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus.","Tadayoshi et al (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format.","To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"(Baker, 1979",W07-2203,"To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain.","Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001).","However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"Prescher, 2001)",W07-2203,"To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain.","Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001).","However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(1992),W07-2203,"Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001).","However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB.",They constrain the training data (parses) considered within the IOA to those consistent with the constituent boundaries defined by the bracketing.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(2002),W07-2203,Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank.,"More recently, both Riezler et al (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation.","In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(2004),W07-2203,Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank.,"More recently, both Riezler et al (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation.","In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"(Inui et al., 1997)",W07-2203,We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data.,"We contrast an IOA-based EM method for training a PGLR parser (Inui et al., 1997), similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below.","The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs).",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(1994),W07-2203,"The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs).",Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting.,"These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(1994),W07-2203,"The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs).",Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting.,"These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(1992),W07-2203,Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting.,"These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models.",Another motivation to explore alternative non-iterative methods is that the derivation space over partiallybracketed data can remain large (>1K) while the confidence-based methods we explore have a total processing overhead equivalent to one iteration of an IOA-based EM algorithm.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"(Tomita, 1987)",W07-2203,24,"constructed from this backbone (Tomita, 1987).",The residue of features not incorporated into the backbone are unified on each reduce action and if unification fails the associated derivation paths also fail.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"(Inui et al., 1997)",W07-2203,The residue of features not incorporated into the backbone are unified on each reduce action and if unification fails the associated derivation paths also fail.,"The parser creates a packed parse forest represented as a graph-structured stack.2 The parse selection model ranks complete derivations in the parse forest by computing the product of the probabilities of the (shift/reduce) parse actions (given LR state and lookahead item) which created each derivation (Inui et al., 1997).","Estimating action probabilities, consists of a) recording an action history for the correct derivation in the parse forest (for each sentence in a treebank), b) computing the frequency of each action over all action histories and c) normalizing these frequencies to determine probability distributions over conflicting (i.e.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(1997),W07-2203,shift/reduce or reduce/reduce) actions.,"Inui et al (1997) describe the probability model utilized in the system where a transition is represented by the probability of moving from one stack state, i1, (an instance of the graph structured stack) to another, i.","They estimate this probability using the stack-top state si1, next input symbol li and next action ai.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(2002),W07-2203,"Therefore, normalization is performed over all lookaheads for a state or over each lookahead for the state depending on whether the state is a member of Ss or Sr, respectively (hereafter the I function).","In addition, Laplace estimation can be used to ensure that all actions in the 2The parse forest is an instance of a feature forest as defined by Miyao and Tsujii (2002).",We will use the term 'node' herein to refer to an element in a derivation tree or in the parse forest that corresponds to a (sub-)analysis whose label is the mother's label in the corresponding CF 'backbone' rule.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(1992),W07-2203,"In this case, equality between the derivation tree and the treebank annotation A identifies the correct derivation.","Following Pereira and Schabes (1992) given t = (s, U), a node's span in the parse forest is valid if it does not overlap with any span outlined in U, and hence, a derivation is correct if the span of every node in the derivation is valid in U.","That is, if no crossing brackets are present in the derivation.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"(Sampson, 1995)",W07-2203,3.2 The Susanne Treebank and Baseline Training Data,"The Susanne Treebank (Sampson, 1995) is utilized to create fully annotated training data.",25,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(2003),W07-2203,3.4 The DepBank Test Data,"King et al (2003) describe the development of the PARC 700 Dependency Bank, a goldstandard set of relational dependencies for 700 sentences (from the PTB) drawn at random from section 23 of the WSJ (the de facto standard test set for statistical parsing).","In all the evaluations reported in this paper we test our parser over a gold-standard set of relational dependencies compatible with our parser output derived (Briscoe and Carroll, 2006) from the PARC 700 Dependency Bank (DepBank, henceforth).",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"(Briscoe and Carroll, 2006)",W07-2203,"King et al (2003) describe the development of the PARC 700 Dependency Bank, a goldstandard set of relational dependencies for 700 sentences (from the PTB) drawn at random from section 23 of the WSJ (the de facto standard test set for statistical parsing).","In all the evaluations reported in this paper we test our parser over a gold-standard set of relational dependencies compatible with our parser output derived (Briscoe and Carroll, 2006) from the PARC 700 Dependency Bank (DepBank, henceforth).",The Susanne Corpus is a (balanced) subset of the Brown Corpus which consists of 15 broad categories of American English texts.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(2001),W07-2203,All but one category (reportage text) is drawn from different domains than the WSJ.,"We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-ofdomain training data.",4 The Evaluation Scheme,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"(Carroll, et al., 1998",W07-2203,4 The Evaluation Scheme,"The parser's output is evaluated using a relational dependency evaluation scheme (Carroll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1.",Relations are organized into a hierarchy with the root node specifying an unlabeled dependency.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"Lin, 1998)",W07-2203,4 The Evaluation Scheme,"The parser's output is evaluated using a relational dependency evaluation scheme (Carroll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1.",Relations are organized into a hierarchy with the root node specifying an unlabeled dependency.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"(Briscoe et al., 2006)",W07-2203,"The microaveraged precision, recall and F1 scores are calculated from the counts for all relations in the hierarchy which subsume the parser output.","The microaveraged F1 score for the baseline system using this evaluation scheme is 75.61%, which - over similar sets of relational dependencies - is broadly comparable to recent evaluation results published by King and collaborators with their state-of-theart parsing system (Briscoe et al., 2006).","3The pipeline is the same as that used for creating S though we do not automatically map the bracketing to be more consistent with the system grammar, instead, we simply removed unary brackets.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(1988),W07-2203,"For example, to compare the accuracy of two parsers over the same data set.",As the number of samples (sentences) is large we use the normal approximation for z. Siegel and Castellan (1988) describe and motivate this test.,"We use a 0.05 level of significance, and provide z-value probabilities for significant results reported below.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"(Watson and Briscoe, 2007)",W07-2203,"We leave for the future a more extensive investigation of these cases which, in principle, would allow us to make more use of this training data.","An alternative approach that we have also explored is to utilize a similar bootstrapping approach with data partially-annotated for grammatical relations (Watson and Briscoe, 2007).",5.1 Confidence-Based Approaches,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"(Watson et al., 2005)",W07-2203,$represents the statistical significance of the system against the baseline model.,"corresponding normalized inside-outside weight for each node (Watson et al., 2005).","We perform EM starting from two initial models; either a uniform probability model, IL(), or from models derived from unambiguous training data, 'y.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(1994),W07-2203,"In most cases, these results are significant, even when we manually select the best model (iteration) for EM.",The graphs of EM performance from iteration 1 illustrate the same 'classical' and 'initial' patterns observed by Elworthy (1994).,"When EM is initialized from a relatively poor model, such as that built from S (Figure 2), a 'classical'",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(2006),W07-2203,"In this case, the graph illustrates a combination of Elworthy's 'initial' and 'classical' patterns.","The steep drop in performance (down to 69.93% F1) after the first iteration is probably due to loss of information from S. However, this run also eventually converges to similar performance, suggesting that the information in S is effectively disregarded as it forms only a small portion of SW, and that these runs effectively converge to a local maximum over W. Bacchiani et al (2006), working in a similar framework, explore weighting the contribution (frequency counts) of the in-domain and out-ofdomain training datasets and demonstrate that this can have beneficial effects.","Furthermore, they also tried unsupervised tuning to the indomain corpus by weighting parses for it by their normalized probability.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(2002),W07-2203,Mapping an incompatible annotated treebank to a compatible partially-bracketed corpus is relatively easy compared to mapping to a compatible fully-annotated corpus.,"An immediate benefit of this work is that (re)training parsers with incrementally-modified grammars based on different linguistic frameworks should be much more straightforward - see, for example Oepen et al (2002) for a good discussion of the problem.","Furthermore, it suggests that it may be possible to usefully tune",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(1994),W07-2203,a parser to a new domain with less annotation effort.,Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available).,The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(1994),W07-2203,a parser to a new domain with less annotation effort.,Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available).,The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents.,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(1999),W07-2203,The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents.,These findings may not hold if the level of bracketing available does not adequately constrain the parses considered - see Hwa (1999) for a related investigation with EM.,"In future work we intend to further investigate the problem of tuning to a new domain, given that minimal manual effort is a major priority.",http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
(2006),W07-2203,"Finally, further experiments on weighting the contribution of each dataset might be beneficial.","For instance, Bacchiani et al (2006) demonstrate imrpovements in parsing accuracy with unsupervised adaptation from unannotated data and explore the effect of different weighting of counts derived from the supervised and unsupervised data.",Acknowledgements,http://www.aclweb.org/anthology/W/W07/W07-2203.pdf
"(Gruber, 1993)",W12-5209,1 Introduction,"Ontology is defined as 'Explicit specification of conceptualization' (Gruber, 1993).","As a knowledge representation formalism, ontologies have found a wide range of applications in the areas like knowledge management, information retrieval and information extraction.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"(Ahmad et al., 1999",W12-5209,The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.,"Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.",Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"Kozakov et al., 2004",W12-5209,The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.,"Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.",Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"Sclano and Velardi, 2007",W12-5209,The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.,"Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.",Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"Frantzi et al., 1998",W12-5209,The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.,"Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.",Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"Gacitua et al., 2011)",W12-5209,The ontology learning process involves two basic tasks- domain specific concept identification and constrution of concept hierarchy.,"Most of the existing algorithms extract relevant terms from the documents using various term extraction methods (Ahmad et al., 1999; Kozakov et al., 2004; Sclano and Velardi, 2007; Frantzi et al., 1998; Gacitua et al., 2011) and then construct ontology by identifying subsumption relations between terms.",Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(2012),W12-5209,Identifying top level concepts and creating a good concept hierarchy are the major challenges involved in the ontology learning tasks.,"As noted by Fountain and Lapata (2012), 'Most of the existing approaches construct flat structure rather than a taxonomy.","Also, the automatically constructed ontologies often create false association between terms and result in erroneous concept hierarchy (Zhou, 2007).",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"(Zhou, 2007)",W12-5209,"As noted by Fountain and Lapata (2012), 'Most of the existing approaches construct flat structure rather than a taxonomy.","Also, the automatically constructed ontologies often create false association between terms and result in erroneous concept hierarchy (Zhou, 2007).","In order to handle the above mentioned issues, we propose a graph-based ontology learning algorithm.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"(Resnik, 1999)",W12-5209,Our approach is based on the information content of the term.,"'Terms with high information content remain lower in the concept hierarchy and terms with low information content remain higher in the concept hierarchy' (Resnik, 1999).",Caraballo and Charniak (1999) have shown that the term frequency is a good indicator of determining specificity of a term.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(1999),W12-5209,"'Terms with high information content remain lower in the concept hierarchy and terms with low information content remain higher in the concept hierarchy' (Resnik, 1999).",Caraballo and Charniak (1999) have shown that the term frequency is a good indicator of determining specificity of a term.,We divide the initial set of terms into different partitions based on the term frequency and then construct k-partite graph by finding subsumption relation between the terms of different partitions.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"(Fellbaum, 1998)",W12-5209,This early identification of hierarchy creates a better taxonomic structure and avoids false association between the terms.,"The proposed approach combines evidences from linguistic patterns and WordNet (Fellbaum, 1998) to detect subsumption relation.",The patterns used in the system are generic and can be used across languages.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"(Bhattacharyya, 2010)",W12-5209,The patterns used in the system are generic and can be used across languages.,"Wordnets of Indian languages are linked with each other and English WordNet through a common index (Bhattacharyya, 2010), which makes it possible to share concept definitions across languages.",Following are the major features of the proposed system:,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(2005),W12-5209,2 Related work,"As noted by Leenheer and Moor (2005), 'No matter how expressive ontologies might be, they are all in fact lexical representations of concepts'.",The linguistic basis of formal ontology is such that a significant portion of domain ontology can be extracted automatically from the domain related texts using language processing techniques.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"(Hearst, 1992",W12-5209,"Ontology learning approaches can be divided into three categories: heuristic based, statistical and hybrid techniques.","Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.",Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"Berland and Charniak, 1999",W12-5209,"Ontology learning approaches can be divided into three categories: heuristic based, statistical and hybrid techniques.","Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.",Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"Girju et al., 2003)",W12-5209,"Ontology learning approaches can be divided into three categories: heuristic based, statistical and hybrid techniques.","Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.",Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(1992),W12-5209,"Heuristic approach (Hearst, 1992; Berland and Charniak, 1999; Girju et al., 2003) primarily relies on the fact that ontological relations are typically expressed in language via a set of linguistic patterns.",Hearst (1992) outlined a variety of lexico-syntactic patterns that can be used to find out ontological relations from a text.,"She described a syntagmatic technique for identifying hyponymy relations in free text by using frequently occurring patterns like 'NP0 such as NP1, NP2, ...,NPn'.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(1999),W12-5209,"She described a syntagmatic technique for identifying hyponymy relations in free text by using frequently occurring patterns like 'NP0 such as NP1, NP2, ...,NPn'.","Berland and Charniak (1999) used a pattern-based approach to find out part-whole relationships (such as between car and door, or car and engine) in a text.",Heuristic approaches rely on language-specific rules which cannot be transferred from one language to another.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"(Harris, 1968)",W12-5209,Statistical approaches model ontology learning as a classification or clustering problem.,"Statistical methods relate concepts based on distributional hypothesis (Harris, 1968), that is 'similar terms appear in the similar context.'",Hindle (1990) performed semantic clustering to find semantically similar nouns.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(1990),W12-5209,"Statistical methods relate concepts based on distributional hypothesis (Harris, 1968), that is 'similar terms appear in the similar context.'",Hindle (1990) performed semantic clustering to find semantically similar nouns.,They calculated the co-occurrence weight for each verb-subject and verb-object pair.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(1993),W12-5209,Verb-wise similarity of two nouns is calculated as the minimum shared weight and the similarity of two nouns is the sum of all verb-wise similarities.,Pereira et al (1993) proposed a divisive clustering method to induce noun hierarchy from an encyclopedia.,"Hybrid approaches leverage the strengths of both statistical and heuristic based approaches and often use evidences from existing knowledge bases such as wordnet, wikipedia, etc.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(1999),W12-5209,"Hybrid approaches leverage the strengths of both statistical and heuristic based approaches and often use evidences from existing knowledge bases such as wordnet, wikipedia, etc.",Caraballo (1999) combined the lexico-syntactic patterns and distributional similarity based methods to construct ontology.,Similarity between two nouns is calculated by computing the cosine between their respective vectors and used for hierarchical bottom-up clustering.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(2005),W12-5209,Hearst-patterns are used to detect hypernymy relation between similar nouns.,"In a similar approach, Cimiano et al (2005) clustered nouns based on distributional similarity and used Hearst-patterns, WordNet (Fellbaum, 1998) and patterns on the web as a hypernymy oracle for constructing a hierarchy.","Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"(Fellbaum, 1998)",W12-5209,Hearst-patterns are used to detect hypernymy relation between similar nouns.,"In a similar approach, Cimiano et al (2005) clustered nouns based on distributional similarity and used Hearst-patterns, WordNet (Fellbaum, 1998) and patterns on the web as a hypernymy oracle for constructing a hierarchy.","Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"(Caraballo, 1999)",W12-5209,"In a similar approach, Cimiano et al (2005) clustered nouns based on distributional similarity and used Hearst-patterns, WordNet (Fellbaum, 1998) and patterns on the web as a hypernymy oracle for constructing a hierarchy.","Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy.",Dominguez Garcia et al (2012) used wikipedia to extract ontology for different languages.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(2012),W12-5209,"Unlike (Caraballo, 1999), the hypernymy sources are directly integrated into the clustering, deciding for each pair of nouns how they should be arranged into the hierarchy.",Dominguez Garcia et al (2012) used wikipedia to extract ontology for different languages.,"Like Cimiano et al (2005), we follow a hybrid approach and construct a concept hierarchy using distributional similarity, patterns and WordNet.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(2005),W12-5209,Dominguez Garcia et al (2012) used wikipedia to extract ontology for different languages.,"Like Cimiano et al (2005), we follow a hybrid approach and construct a concept hierarchy using distributional similarity, patterns and WordNet.","However, instead of performing top-down or bottom-up clustering, we pose ontology learning as a k-partite graph construction problem.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(2006),W12-5209,We use term frequency to determine the position of a concept in the hierarchy.,"Ryu and Choi (2006) also used term frequency as a measure of domain specificity, but instead of partitioning they combined term frequency and distributional similarity to construct hierarchy.",Other method similar to our work is proposed in Fountain and Lapata (2012).,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(2012),W12-5209,"Ryu and Choi (2006) also used term frequency as a measure of domain specificity, but instead of partitioning they combined term frequency and distributional similarity to construct hierarchy.",Other method similar to our work is proposed in Fountain and Lapata (2012).,Fountain and Lapata (2012) proposed a graph based approach that does not require a separate term extraction step.,http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
(2012),W12-5209,Other method similar to our work is proposed in Fountain and Lapata (2012).,Fountain and Lapata (2012) proposed a graph based approach that does not require a separate term extraction step.,"However, their approach works with a predefined set of seed terms.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"(Ahmad et al., 1999)",W12-5209,Relevance of the key term in the corpus is calculated by counting the frequency of the term.,"Terms are filtered out using weirdness measure (Ahmad et al., 1999).","Feature vector for each term is created by including co-occurring nouns, verbs and adjectives.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"(Cimiano, 2006)",W12-5209,Two patterns are used to detect subsumption and neighbor relations.,"Head word heuristic (Cimiano, 2006) based pattern (NP)  (NP) is used to identify subsumption relation.","As per head word heuristic (NP1)(NP2) implies (NP2) subsumes (NP1NP2), e.g.",http://www.aclweb.org/anthology/W/W12/W12-5209.pdf
"(Kincaid, Fishburne, Rogers, & Chissom, 1975",W14-5605,Anybody who has seen patent claims at least once will find it unnecessary to calculate claim readability indices to get persuaded that the claim text is extremely low readable.,"Traditional readability formulas normally take into account the number of words per sentence or/and the number of ""hard"", be it long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 1998; Greenfield, 2004).","Both the first and the second ratio will be equal to the number of words in a claim sentence where practically all words are ""hard"" terms, some of them used for the first time.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"Brown, 1998",W14-5605,Anybody who has seen patent claims at least once will find it unnecessary to calculate claim readability indices to get persuaded that the claim text is extremely low readable.,"Traditional readability formulas normally take into account the number of words per sentence or/and the number of ""hard"", be it long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 1998; Greenfield, 2004).","Both the first and the second ratio will be equal to the number of words in a claim sentence where practically all words are ""hard"" terms, some of them used for the first time.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"Greenfield, 2004)",W14-5605,Anybody who has seen patent claims at least once will find it unnecessary to calculate claim readability indices to get persuaded that the claim text is extremely low readable.,"Traditional readability formulas normally take into account the number of words per sentence or/and the number of ""hard"", be it long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 1998; Greenfield, 2004).","Both the first and the second ratio will be equal to the number of words in a claim sentence where practically all words are ""hard"" terms, some of them used for the first time.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"Orletta et al., 2011)",W14-5605,"The mainstream of text simplification is developing methodologies and tools for general types of texts that address people with special needs, such as poor literacy readers (Aluisio et al.","2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just ""regular"" readers (Graesser et al., 2004).",Text simplification is most often performed on the sentence level.,http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Bott et al., 2012)",W14-5605,"The mainstream of text simplification is developing methodologies and tools for general types of texts that address people with special needs, such as poor literacy readers (Aluisio et al.","2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just ""regular"" readers (Graesser et al., 2004).",Text simplification is most often performed on the sentence level.,http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Crossley and McNamara, 2008)",W14-5605,"The mainstream of text simplification is developing methodologies and tools for general types of texts that address people with special needs, such as poor literacy readers (Aluisio et al.","2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just ""regular"" readers (Graesser et al., 2004).",Text simplification is most often performed on the sentence level.,http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Graesser et al., 2004)",W14-5605,"The mainstream of text simplification is developing methodologies and tools for general types of texts that address people with special needs, such as poor literacy readers (Aluisio et al.","2010), readers with mild cognitive impairment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels (Crossley and McNamara, 2008) or just ""regular"" readers (Graesser et al., 2004).",Text simplification is most often performed on the sentence level.,http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Allen, 2009)",W14-5605,Simplifying texts to provide more comprehensible input to a targeted audience the developers generally work within two approaches: an intuitive approach and a structural approach.,"An intuitive approach relies mainly on the developers' intuition and experience (Allen, 2009) that leads to using less lexical diversity, less sophisticated words, less syntactic complexity, and greater cohesion.","A structural approach depends on the use of structure and word lists that are predefined by the intelligence level, as typically found in targeted readers.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Siddharthan, 2002)",W14-5605,"Automated text simplification tools are trying to achieve this purpose by combining linguistic and statistical techniques and penalize writers for polysyllabic words and long, complex sentences.","(Siddharthan, 2002) describe the implementation of the three stages - analysis, transforma",42,http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Petersen & Ostendorf, 2007)",W14-5605,"tion and regeneration, system that lay particular emphasis on the discourse level aspects of syntactic simplification.","Some works on text simplification use parallel corpora of original and simplified sentences (Petersen & Ostendorf, 2007).","There are works where text simplification is treated as a ""translation task within a RBMT (Takao and Sumita.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Specia, 2010)",W14-5605,2003).,"In (Specia, 2010) text simplification is developed in the Statistical Machine Translation framework, given a parallel corpus of original and simplified texts, aligned at the sentence level.","In (Poornima et al.2011) a rule based technique is proposed to simplify the complex sentences based on connectives like relative pronouns, coordinating and subordinating conjunctions.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Bott, et al., 2012)",W14-5605,Sentence simplification is expressed as the list of sub-sentences that are portions of the original sentence.,"(Bott, et al., 2012) describe a hybrid automatic text simplification system which combines a rule based core module with a statistical support module that controls the application of rules in the wrong contexts.",The approaches to patent claim simplification can be roughly put into two groups.,http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Shinmori et al., 2003)",W14-5605,"Studies of the first group try to adapt to the patent domain general text simplification techniques and involve lexical and/or structural substitution, pruning, paraphrasing, etc.","For example, in (Shinmori et al., 2003) the discourse structure of the patent claim is built by means of a rule-based technique; each discourse segment is then paraphrased.","In (Mille and Wanner, 2008) the claim sentence (by means of lexical and punctuation clues) is segmented into clausal units, that are then compressed into a summary.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Mille and Wanner, 2008)",W14-5605,"For example, in (Shinmori et al., 2003) the discourse structure of the patent claim is built by means of a rule-based technique; each discourse segment is then paraphrased.","In (Mille and Wanner, 2008) the claim sentence (by means of lexical and punctuation clues) is segmented into clausal units, that are then compressed into a summary.","The simplification methods proposed by this group of researches to some extent change the original content of the claim that might not always be desirable, especially for patent experts.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Sheremetyeva, 2003)",W14-5605,"Another group of studies focuses on segmenting, reformatting or highlighting certain parts of the patent claim without changing the content of the original.","For example, in one of the earlier works a rule-based technique was developed for decomposing the complex sentence of a claim into a set of simple sentences while preserving the initial content (Sheremetyeva, 2003).","Most recently (Shinmori et al., 2012) suggested aligning claim phrases with explanatory text from the description section, while (Ferraro et al., 2014) proposed an approach that involves highlighting the claim segments borders and reformatting the original text so as to emphasis segments with the identified border marker.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Shinmori et al., 2012)",W14-5605,"For example, in one of the earlier works a rule-based technique was developed for decomposing the complex sentence of a claim into a set of simple sentences while preserving the initial content (Sheremetyeva, 2003).","Most recently (Shinmori et al., 2012) suggested aligning claim phrases with explanatory text from the description section, while (Ferraro et al., 2014) proposed an approach that involves highlighting the claim segments borders and reformatting the original text so as to emphasis segments with the identified border marker.","This approach does not involve any syntactic restructuring, just visualization of claim segments.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Ferraro et al., 2014)",W14-5605,"For example, in one of the earlier works a rule-based technique was developed for decomposing the complex sentence of a claim into a set of simple sentences while preserving the initial content (Sheremetyeva, 2003).","Most recently (Shinmori et al., 2012) suggested aligning claim phrases with explanatory text from the description section, while (Ferraro et al., 2014) proposed an approach that involves highlighting the claim segments borders and reformatting the original text so as to emphasis segments with the identified border marker.","This approach does not involve any syntactic restructuring, just visualization of claim segments.",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"Radack, 1995)",W14-5605,In preparing for this research we have investigated professional instructions (Pressman.,"2006; Radack, 1995) on how to read patent claims and conducted extensive interviews with patent experts of several companies in the US and Europe handling intellectual property1.",The recommendations are as follows.,http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Sheremetyeva, 1999",W14-5605,Micro-level simplification at each of its stages is done by means of a specific combination of rulebased and statistical techniques and relies on linguistic knowledge of different depth.,"This knowledge is structured following the methodology described in (Sheremetyeva, 1999; Sheremetyeva, 2003) and is mostly coded in the system lexicon as well as in analysis and generation rules.",Different modules of the micro-level simplification component use specific parts and types of linguistic knowledge included in the lexicon and their own specific sets of rules.,http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"Sheremetyeva, 2003)",W14-5605,Micro-level simplification at each of its stages is done by means of a specific combination of rulebased and statistical techniques and relies on linguistic knowledge of different depth.,"This knowledge is structured following the methodology described in (Sheremetyeva, 1999; Sheremetyeva, 2003) and is mostly coded in the system lexicon as well as in analysis and generation rules.",Different modules of the micro-level simplification component use specific parts and types of linguistic knowledge included in the lexicon and their own specific sets of rules.,http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Sheremetyeva, 2009)",W14-5605,This task is performed based on the results of a shallow analysis performed by a hybrid NP extractor and NP and predicate term chunkers which in succession run on the same claim text.,"To extract (and then highlight) nominal terminology we use the NP extractor described in (Sheremetyeva, 2009).","The extraction methodology combines statistical techniques, heuristics and a very shallow linguistic knowledge extracted from the main system lexicon (see Section 5.1).",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Sheremetyeva, 2007)",W14-5605,Simplification of a claim text into a diagram is performed based of the internal claim representation as shown in Section 5.3.,"We here used the automatic text planner of the claim generator that was developed as a module of a patent MT system (Sheremetyeva, 2007).",Figure 4.,http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Mille and Wanner, 2008)",W14-5605,Given that no reliable evaluation metrics exist so far for text simplification we performed a preliminary qualitative evaluation of our methodology based on human judgment (as in all cited works on claim simplification).,"Some of the researchers admit avoiding qualitative evaluation due to the lack of resources that would have made it possible (Mille and Wanner, 2008).","The number of patents the authors use to evaluate their methodologies might seem quite limited, e.g., (Mille and Wanner, 2008) report evaluation results based on 30 patents; in (Bouayad-Agha et al.)",http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Mille and Wanner, 2008)",W14-5605,"Some of the researchers admit avoiding qualitative evaluation due to the lack of resources that would have made it possible (Mille and Wanner, 2008).","The number of patents the authors use to evaluate their methodologies might seem quite limited, e.g., (Mille and Wanner, 2008) report evaluation results based on 30 patents; in (Bouayad-Agha et al.)",the test corpus consisted of 29 patents; (Ferraro et al.,http://www.aclweb.org/anthology/W/W14/W14-5605.pdf
"(Niehues and Vogel, 2008)",W10-1719,2 Baseline System,"The baseline systems for the translation directions German-English and English-German are both developed using Discriminative Word Alignment (Niehues and Vogel, 2008) and the Moses Toolkit (Koehn et al., 2007) for extracting phrase pairs and generating the phrase table from the discriminative word alignments.",The difficult reordering between German and English was modeled using POS-based reordering rules.,http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
"(Koehn et al., 2007)",W10-1719,2 Baseline System,"The baseline systems for the translation directions German-English and English-German are both developed using Discriminative Word Alignment (Niehues and Vogel, 2008) and the Moses Toolkit (Koehn et al., 2007) for extracting phrase pairs and generating the phrase table from the discriminative word alignments.",The difficult reordering between German and English was modeled using POS-based reordering rules.,http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
"(Schmid, 1994)",W10-1719,These rules were learned using a word-aligned parallel corpus.,"The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages.","Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al (2005).",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
"(Vogel, 2003)",W10-1719,"The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages.","Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al (2005).","2.1 Training, Development and Test Data",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
(2005),W10-1719,"The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages.","Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al (2005).","2.1 Training, Development and Test Data",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
(2003),W10-1719,"If the new word is a correct word according to the hunspell lexicon using the new spelling rules, we map the words.","When translating from German to English, we apply compound splitting as described in Koehn and Knight (2003) to the German corpus.",As a last preprocessing step we remove sentences that are too long and empty lines to obtain the final training corpus.,http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
"(Rottmann and Vogel, 2007)",W10-1719,These possible reorderings were learned based on the POS of the source language words in the training corpus and the information about alignments between source and target language words in the corpus.,"For short-range reorderings, continuous reordering rules were applied to the test sentences (Rottmann and Vogel, 2007).","To model the long-range reorderings between German and English, different types of noncontinuous reordering rules were applied depending on the translation direction.",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
"(Niehues and Kolss, 2009)",W10-1719,"To model the long-range reorderings between German and English, different types of noncontinuous reordering rules were applied depending on the translation direction.","(Niehues and Kolss, 2009).","When translating from English to German, most of the changes in word order consist in a shift to the right while typical word shifts in German to English translations take place in the reverse direction.",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
(2006),W10-1719,"The phrase table was trained using the Moses training scripts, but for the German to English system we used a different phrase extraction method described in detail in Section 4.2.","In addition, we applied phrase table smoothing as described in Foster et al (2006).","Furthermore, we extended the translation model by additional features for unaligned words and introduced bilingual language models.",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
(2008),W10-1719,Then these alignments are used to extract the phrase pairs.,We used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead.,This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++2 Toolkit and POS information.,http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
(2008),W10-1719,"The model was trained in three steps, first using maximum likelihood optimization and afterwards it was optimized towards the alignment error rate.",For more details see Niehues and Vogel (2008).,4.2 Lattice Phrase Extraction,http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
(2009),W10-1719,139,"Therefore, we build lattices that encode the different reorderings for every training sentence, as described in Niehues et al (2009).","Then we can not only extract phrase pairs from the monotone source path, but also from the reordered paths.",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
(2009),W10-1719,4.3 Unaligned Word Feature,Guzman et al (2009) analyzed the role of the word alignment in the phrase extraction process.,"To better model the relation between word alignment and the phrase extraction process, they introduced two new features into the log-linear model.",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
(2009),W10-1719,4.4 Bilingual Word language model,"Motivated by the improvements in translation quality that could be achieved by using the n-gram based approach to statistical machine translation, for example by Allauzen et al (2009), we tried to integrate a bilingual language model into our phrase-based translation system.","To be able to integrate the approach easily into a standard phrase-based SMT system, a token in the bilingual language model is defined to consist of a target word and all source words it is aligned to.",http://www.aclweb.org/anthology/W/W10/W10-1719.pdf
"(Schmid, 1994)",W12-0108,The phrases are assumed to be flat and linguistically valid.,"As a parser, any available tool may be used (the TreeTagger (Schmid, 1994) is used in the present implementation for English).","PAM processes a bilingual corpus of SL - TL sentence pairs, taking into account the parsing information in one language (in the current implementation the TL side) and making use of a bilingual lexicon and information on potential phrase heads; the output being the bilingual corpus aligned at word, phrase and clause level.",http://www.aclweb.org/anthology/W/W12/W12-0108.pdf
(2011),W12-0108,"For instance, based on a sentence pair from the parallel corpus, the SL sentence with structure A-B-C-D is transformed into A'-C'-D'-B', where X is a phrase in SL and X' is a phrase in TL.",Further PAM details are reported in Tambouratzis et al (2011).,"The PAM output in terms of SL phrases is then handed over to the Phrasing model generator (PMG), which is trained to determine the phrasal structure of an input sentence.",http://www.aclweb.org/anthology/W/W12/W12-0108.pdf
"(Lafferty et al., 1999)",W12-0108,This phrasing model is then applied in segmenting any arbitrary SL text being input to the PRESEMT system for translation.,"PMG is based on the Conditional Random Fields model (Lafferty et al., 1999) which has been found to provide the highest accuracy.",The SL text segmented into phrases by PMG is then input to the 1st translation phase.,http://www.aclweb.org/anthology/W/W12/W12-0108.pdf
"(Smith and Waterman, 1981)",W12-0108,"), the values of which are optimised by employing the optimisation module.","The implementation is based on the SmithWaterman algorithm (Smith and Waterman, 1981), initially proposed for determining similar regions between two protein or DNA sequences.",The algorithm is guaranteed to find the optimal local alignment between the two input sequences at clause level.,http://www.aclweb.org/anthology/W/W12/W12-0108.pdf
"(Gale and Shapley, 1962)",W12-0108,"In the second task, the most similar phrases to the TL structure phrases are retrieved from the monolingual corpus to provide local structural information such as word-reordering.","A matching algorithm selects the most similar from the set of the retrieved TL phrases through a comparison process, which is viewed as an assignment problem, using the Gale-Shapley algorithm (Gale and Shapley, 1962).",6.,http://www.aclweb.org/anthology/W/W12/W12-0108.pdf
"(Koehn et al., 2007)",W12-0108,"Table 1 illustrates an indicative set of results obtained by running automatic evaluation metrics on test data translated by the 1st PRESEMT prototype for a selection of language pairs, due to space restrictions.","In the case of the language pair English-toGerman, these results are contrasted to the ones obtained when translating the same test set with Moses (Koehn et al., 2007).It is observed that for the English-to-German language pair, PRESEMT achieved approximately 50% of the MOSES BLEU score and 80% of the MOSES with respect to the Meteor and TER scores.",These are reasonably competitive results compared to an established system such as Moses.,http://www.aclweb.org/anthology/W/W12/W12-0108.pdf
[1993],J03-3001,"Arguments raged, and it was not clear whether corpus work was an acceptable","1 Across the Atlantic, a resurgence in empiricism was led by the success of the noisy-channel model in speech recognition (see Church and Mercer [1993] for references).",334,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(1999),J03-3001,The Web walked in on ACL meetings starting in 1999.,"Rada Mihalcea and Dan Moldovan (1999) used hit counts for carefully constructed search engine queries to identify rank orders for word sense frequencies, as an input to a word sense disambiguation engine.",Philip Resnik (1999) showed that parallel corpora-until then a promising research avenue but largely constrained to the English-French Canadian Hansard-could be found on the Web: We can grow our own parallel corpus using the many Web pages that exist in parallel in local and in major languages.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(1999),J03-3001,"Rada Mihalcea and Dan Moldovan (1999) used hit counts for carefully constructed search engine queries to identify rank orders for word sense frequencies, as an input to a word sense disambiguation engine.",Philip Resnik (1999) showed that parallel corpora-until then a promising research avenue but largely constrained to the English-French Canadian Hansard-could be found on the Web: We can grow our own parallel corpus using the many Web pages that exist in parallel in local and in major languages.,We are glad to have the further development of this work (co-authored by Noah Smith) presented in this special issue.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2001),J03-3001,We are glad to have the further development of this work (co-authored by Noah Smith) presented in this special issue.,"In the student session of ACL 2000, Rosie Jones and Rayid Ghani (2001) showed how, using the Web, one can build a language-specific corpus from a single document in that language.","In the main session Atsushi Fujii and Tetsuya Ishikawa (2000) demonstrated that descriptive, definition-like collections can be acquired from the Web.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2000),J03-3001,"In the student session of ACL 2000, Rosie Jones and Rayid Ghani (2001) showed how, using the Web, one can build a language-specific corpus from a single document in that language.","In the main session Atsushi Fujii and Tetsuya Ishikawa (2000) demonstrated that descriptive, definition-like collections can be acquired from the Web.",2.1 Some Current Themes,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2001),J03-3001,The Web is being used to address data sparseness for language modeling.,"In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al (2003) ""balance"" their corpus using Web documents.",The information retrieval community now has a Web track as a component of its TREC evaluation initiative.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2003),J03-3001,The Web is being used to address data sparseness for language modeling.,"In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al (2003) ""balance"" their corpus using Web documents.",The information retrieval community now has a Web track as a component of its TREC evaluation initiative.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2000),J03-3001,2000) are exploring the automatic population of existing ontologies using the Web as a source for new instances.,"Varantola (2000) shows how translators can use ""just-in-time"" sublanguage corpora to choose correct target language terms for areas in which they are not expert.",Fletcher (2002) demonstrates methods for gathering and using Web corpora in a language-teaching context.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2002),J03-3001,"Varantola (2000) shows how translators can use ""just-in-time"" sublanguage corpora to choose correct target language terms for areas in which they are not expert.",Fletcher (2002) demonstrates methods for gathering and using Web corpora in a language-teaching context.,2.2 The 100M Words of the BNC,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2001),J03-3001,"They find that probabilistic models of language based on very large quantities of data, even if those data are noisy, are better than ones based on estimates (using sophisticated smoothing techniques) from smaller, cleaner data sets.",Another argument is made vividly by Banko and Brill (2001).,They explore the performance of a number of machine learning algorithms (on a representative disambiguation task) as the size of the training corpus grows from a million to a billion words.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(1999),J03-3001,Linguistic aspects take a little more work and can be estimated only by sampling and extrapolation.,"Lawrence and Giles (1999) compared the overlap between page lists returned by different Web browsers over the same set of queries and estimated that, in 1999, there were 800 million indexable Web pages available.","By sampling pages, and estimating an average page length of seven to eight kilobytes of nonmarkup text, they concluded that there might be six terabytes of text available then.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2000),J03-3001,How much of it is English?,"Xu (2000) estimated that 71% of the pages (453 million out of 634 million Web pages indexed by the Excite engine at that time) were written in English, followed by Japanese (6.8%), German (5.1%), French (1.8%), Chinese (1.5%), Spanish (1.1%), Italian (0.9%), and Swedish (0.7%).","We have measured the counts of some English phrases according to various search engines over time and compared them with counts in the BNC, which we know has 100 million words.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
[1999],J03-3001,"The numbers presented in Table 3 are lower bounds, for a number of reasons:", AltaVista covers only a fraction of the indexable Web pages available (the fraction was estimated at just 15% by Lawrence and Giles [1999]).,AltaVista may be biased toward North American (mainly English-language) pages by the strategy it uses to crawl the Web.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
[2001],J03-3001,"AltaVista indexes only pages that can be directly called by a URL and does not index text found in databases that are accessible through dialog windows on Web pages (the ""hidden Web"").","This hidden Web is vast (consider MedLine,8 just one such database, with more than five billion words; see also Ipeirotis, Gravano, and Sahami [2001]), and it is not considered at all in the AltaVista estimates.","Repeating the procedure after an interval, the second author and Nioche showed that the proportion of non-English text to English is growing.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(1999),J03-3001,The phrase work group is 15 times more frequent than any other and is also the best translation among the tested possibilities.,A set of controlled experiments of this form is described in Grefenstette (1999).,"In Grefenstette's study, a good translation was found in 87% of ambiguous cases from German to English and 86% of ambiguous cases from Spanish to English.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
[2003],J03-3001,"In the text domain, organizations such as Reuters produce news feeds that are typically adapted to the style of a particular newspaper and then republished: Is each republication a new writing event?","(These issues, and related themes of cut-and-paste authorship, ownership, and plagiarism, are explored in Wilks [2003].)",4.2 Technology,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(1997),J03-3001,There is little work on assessing how well one language model fares when applied to a text type that is different from that of the training corpus.,"Two studies in this area are Sekine (1997) and Gildea (2001), both of which show substantial variation in model performance",341,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2001),J03-3001,There is little work on assessing how well one language model fares when applied to a text type that is different from that of the training corpus.,"Two studies in this area are Sekine (1997) and Gildea (2001), both of which show substantial variation in model performance",341,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(1992),J03-3001,"To date, corpus developers have been obliged to make pragmatic decisions about the sorts of text to go into a corpus.","Atkins, Clear, and Ostler (1992) describe the desiderata and criteria used for the BNC, and this stands as a good model for a general-purpose, general-language corpus.","The word representative has tended to fall out of discussions, to be replaced by the meeker balanced.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(1988),J03-3001,Kilgarriff and Grefenstette Web as Corpus: Introduction,"The recent history of mathematically sophisticated modeling of language variation begins with Biber (1988), who identifies and quantifies the linguistic features associated with different spoken and written text types.",Habert and colleagues (Folch et al.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2001),J03-3001,"2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others.","In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.","As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2002),J03-3001,"2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others.","In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.","As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(1997),J03-3001,"In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.","As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).",Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2001),J03-3001,"In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.","As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).",Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2000),J03-3001,"In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.","As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).",Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2001),J03-3001,"As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al (2000).",Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.,"A practical discussion of a central technical concern is Vossen (2001), which tailors a general-language resource for a domain.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2001),J03-3001,Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation.,"A practical discussion of a central technical concern is Vossen (2001), which tailors a general-language resource for a domain.","Baayen (2001) presents sophisticated mathematical models for word frequency distributions, and it is likely that his mixture models have potential for modeling sublanguage mixtures.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(2001),J03-3001,"A practical discussion of a central technical concern is Vossen (2001), which tailors a general-language resource for a domain.","Baayen (2001) presents sophisticated mathematical models for word frequency distributions, and it is likely that his mixture models have potential for modeling sublanguage mixtures.","His models have been developed with a specific, descriptive goal in mind and using a small number of short texts: It is unclear whether they can be usefully applied in NLP.",http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(1997),J03-3001,"Also, the focus is usually on content words and topics or domains, with other differences of genre or sublanguage remaining unexamined.","Exceptions focusing on genre include Kessler, Nunberg, and Schutze (1997) and Karlgren and Cutting (1994).",4.7 Representativeness: Conclusion,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
(1994),J03-3001,"Also, the focus is usually on content words and topics or domains, with other differences of genre or sublanguage remaining unexamined.","Exceptions focusing on genre include Kessler, Nunberg, and Schutze (1997) and Karlgren and Cutting (1994).",4.7 Representativeness: Conclusion,http://www.aclweb.org/anthology/J/J03/J03-3001.pdf
"(Gollins and Sanderson, 2001)",N13-1057,Existing algorithms for creating new bilingual dictionaries use intermediate languages or intermediate dictionaries to find chains of words with the same meaning.,"For example, (Gollins and Sanderson, 2001) use lexical triangulation to translate in parallel across multiple intermediate languages and",1http://www.ethnologue.com/ 2http://dsal.uchicago.edu/dictionaries/list.html,http://www.aclweb.org/anthology/N/N13/N13-1057.pdf
"(Mausam et al., 2010)",N13-1057,"They use four pivot languages, German, Spanish, Dutch and Italian, as intermediate languages.","Another existing approach for creating bilingual dictionaries is using probabilistic inference (Mausam et al., 2010).",They organize dictionaries in a graph topology and use random walks and probabilistic graph sampling.,http://www.aclweb.org/anthology/N/N13/N13-1057.pdf
"(Shaw et al., 2011)",N13-1057,They organize dictionaries in a graph topology and use random walks and probabilistic graph sampling.,"(Shaw et al., 2011) propose a set of algorithms to create a reverse dictionary in the context of single language by using converse mapping.","In particular, given an English-English dictionary, they attempt to find the original words or terms given a synonymous word or phrase describing the meaning of a word.",http://www.aclweb.org/anthology/N/N13/N13-1057.pdf
"(Landau, 1984)",N13-1057,"A dictionary entry, called LexicalEntry, is a 2-tuple <LexicalUnit, Definition>.","A LexicalUnit is a word or a phrase being defined, also called definiendum (Landau, 1984).",A list of entries sorted by the LexicalUnit is called a lexicon or a dictionary.,http://www.aclweb.org/anthology/N/N13/N13-1057.pdf
"(Miller, 1995)",N13-1057,"In this section, we propose a series of algorithms, each one of which automatically creates a reverse dictionary, or ReverseDictionary, from a dictionary that translates a word in language L1 to a word or phrase in language L2.","We require that at least one of two these languages has a Wordnet type lexical ontology (Miller, 1995).",Our algorithms are used to create reverse dictionaries from them at various levels of accuracy and sophistication.,http://www.aclweb.org/anthology/N/N13/N13-1057.pdf
"(Wu and Palmer, 1994)",N13-1057,"The distance between the two LexicalEntrys is the distance between the two LexicalUnits if the LexicalUnits occur in Wordnet ontology; otherwise, it is the distance between the two Senses.","The distance between each phrase pair is the average of the total distances between every word pair in the phrases (Wu and Palmer, 1994).","If the distance between two words or phrases is 1.00, there is no similarity between these words or phrases, but if they have the same meaning, the distance is 0.00.",http://www.aclweb.org/anthology/N/N13/N13-1057.pdf
"(Cocke and Schwartz, 1970",W11-2921,"Most syntactic constituency parsers employ a weighted context-free grammar (CFG), that is learned from a treebank.","The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n') time.","While often ignored, the grammar constant |G |typically dominates the runtime in practice.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Kasami, 1965",W11-2921,"Most syntactic constituency parsers employ a weighted context-free grammar (CFG), that is learned from a treebank.","The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n') time.","While often ignored, the grammar constant |G |typically dominates the runtime in practice.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Younger, 1967)",W11-2921,"Most syntactic constituency parsers employ a weighted context-free grammar (CFG), that is learned from a treebank.","The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n') time.","While often ignored, the grammar constant |G |typically dominates the runtime in practice.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(Collins, 1999",W11-2921,"While often ignored, the grammar constant |G |typically dominates the runtime in practice.","This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.","Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Charniak, 2000",W11-2921,"While often ignored, the grammar constant |G |typically dominates the runtime in practice.","This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.","Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Petrov et al., 2006)",W11-2921,"While often ignored, the grammar constant |G |typically dominates the runtime in practice.","This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.","Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(Asanovic et al., 2006)",W11-2921,"This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.","Meanwhile, we have entered a manycore computing era, where the number of processing cores in computer systems doubles every second year, while the clock frequency has converged somewhere around 3 GHz (Asanovic et al., 2006).",This opens up new opportunities for increasing the speed of parsers.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
(2006),W11-2921,We present a general approach for parallelizing the CKY algorithm that can handle arbitrary context-free grammars (Section 2).,We make no assumptions about the size of the grammar and we demonstrate the efficacy of our approach by implementing a decoder for the state-of-the-art latent variable grammars of Petrov et al (2006) (a.k.a.,Berkeley Parser) on a Graphics Processor Unit (GPU).,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(Nickolls et al., 2008)",W11-2921,Berkeley Parser) on a Graphics Processor Unit (GPU).,"We first present an overview of the general architecture of GPUs and the efficient synchronization provided by the Compute Unified Device Architecture (CUDA (Nickolls et al., 2008)) programming model (Section 3).",We then discuss how the hundreds of cores available on a GPU can enable a fine-grained parallel execution of the CKY algorithm.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(Petrov et al., 2006)",W11-2921,In this work we focus our attention on constituency parsing and assume that a weighted CFG is available to us.,"In our experiments we will use a probabilistic latent variable CFG (Petrov et al., 2006).","However, our algorithms can be used with any weighted CFG, including discriminative ones, such as the ones in Petrov and Klein (2007a) and",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
(2007a),W11-2921,"In our experiments we will use a probabilistic latent variable CFG (Petrov et al., 2006).","However, our algorithms can be used with any weighted CFG, including discriminative ones, such as the ones in Petrov and Klein (2007a) and","Figure 2: The chart that visualizes the bottom-up process of CKY parsing for the sentence ""I love you .""",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
(2008),W11-2921,"Figure 2: The chart that visualizes the bottom-up process of CKY parsing for the sentence ""I love you .""",Finkel et al (2008).1 The grammars in our experiments have on the order of thousands of nonterminals and millions of productions.,Figure 1(a) shows a constituency parse tree.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
(2005),W11-2921,"We found this to be more efficient than keeping backpointers.2 One should also note that many real-world applications benefit from, or even expect n-best lists of possible parse trees.",Using the lazy evaluation algorithm of Huang and Chiang (2005) the extrac,1For feature-rich discriminative models a trivially parallelizable pass can be used to pre-compute the rule-potentials.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(Lindholm et al., 2008)",W11-2921,"Graphics Processor Units (GPUs) were originally designed for processing graphics applications, where millions of operations can be executed in parallel.","In order to increase the efficiency by exploiting this parallelism, typical GPUs (Lindholm et al., 2008) have hundreds of processing cores.","For example, the NVIDIA GTX480 GPU has 480 processing cores called stream processors (SP).",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
(2008),W11-2921,3.1 Compute Unified Device Architecture,"Recently, Nickolls et al (2008) introduced the Compute Unified Device Architecture (CUDA).",It allows programmers to utilize GPUs to accelerate applications in domains other than graphics.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(Kirk and Hwu, 2010)",W11-2921,"Generally speaking, manycore architectures (like GPUs) have more ALUs in place of on-chip caches, making arithmetic operations relatively cheaper and global memory accesses relatively more expensive.","Thus, to achieve good performance, it is important to increase the ratio of Compute to Global Memory Access (CGMA) (Kirk and Hwu, 2010), which can be done in part by cleverly utilizing the different types of shared onchip memory in each SM.",Threads in a thread block are mapped onto the same SM and can cooperate with one another by sharing data through the on-chip shared memory of the SM (shown in Figure 5).,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
(2006),W11-2921,The detailed specifications of the experimental platforms are listed in Table 1.,The grammar we used is extracted from the publicly available parser of Petrov et al (2006).,"It has 1,120 nonterminal symbols including 636 preterminal symbols.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(Marcus et al., 1993)",W11-2921,"The grammar has 852,591 binary rules and 114,419 unary rules.","We used the first 1000 sentences from Section 22 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993) as our benchmark set.",We verified for each sentence that our parallel implementation obtains exactly the same parse tree and score as the sequential implementation.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
(2011),W11-2921,"The exhaustive sequential CKY parser was written in C and is reasonably optimized, taking 5.5 seconds per sentence (or 5,505 seconds for the 1000 benchmark sentences).",This is comparable to the better implementations presented in Dunlop et al (2011).,"As can be seen in Figure 11, the fastest configuration on the GTX285 is Block+PR+SS+tex:scores, which shows a 17.4x speedup against the sequential parser.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(NVIDIA, 2009)",W11-2921,"However, placing the rule information in texture memory improves the performance little as there are many more accesses to the scores array than to the rule information.","The GTX480 is the Fermi architecture (NVIDIA, 2009), with many features added to the GTX285.","The number of cores doubled from 240 to 480, but the number of SMs was halved from 30 to 15.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(van Lohuizen, 1999",W11-2921,6 Related Work,"A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).","However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Giachin and Rullent, 1989",W11-2921,6 Related Work,"A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).","However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Pontelli et al., 1998",W11-2921,6 Related Work,"A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).","However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Manousopoulou et al., 1997)",W11-2921,6 Related Work,"A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).","However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
(1999),W11-2921,"The parallel parsers in past work are implemented on multicore systems, where the limited parallelization possibilities provided by the systems restrict the speedups that can be achieved.","For example, van Lohuizen (1999) reports a 1.8x speedup, while Manousopoulou et al (1997) claims a 7-8x speedup.","In contrast, our parallel parser is implemented on a manycore system with an abundant number of threads and pro",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
(1997),W11-2921,"The parallel parsers in past work are implemented on multicore systems, where the limited parallelization possibilities provided by the systems restrict the speedups that can be achieved.","For example, van Lohuizen (1999) reports a 1.8x speedup, while Manousopoulou et al (1997) claims a 7-8x speedup.","In contrast, our parallel parser is implemented on a manycore system with an abundant number of threads and pro",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(van Lohuizen, 1999",W11-2921,We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.,"Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).",Agenda-based parsers maintain a queue of prioritized intermediate results and iteratively refine and combine these until the whole sentence is processed.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Giachin and Rullent, 1989",W11-2921,We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.,"Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).",Agenda-based parsers maintain a queue of prioritized intermediate results and iteratively refine and combine these until the whole sentence is processed.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Pontelli et al., 1998",W11-2921,We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.,"Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).",Agenda-based parsers maintain a queue of prioritized intermediate results and iteratively refine and combine these until the whole sentence is processed.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Manousopoulou et al., 1997)",W11-2921,We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.,"Another difference is that previous work has often focused on parallelizing agenda-based parsers (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997).",Agenda-based parsers maintain a queue of prioritized intermediate results and iteratively refine and combine these until the whole sentence is processed.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
(2002),W11-2921,Chart-based parsing on the other hand allows us to expose and exploit the abundant parallelism of the dynamic program.,Bordim et al (2002) present a CKY parser that is implemented on a field-programmable gate array (FPGA) and report a speedup of up to 750x.,"However, this hardware approach suffers from insufficient memory or logic elements and limits the number of rules in the grammar to 2,048 and the number of non-terminal symbols.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
(1997),W11-2921,"Their approach thus cannot be applied to real-world, state-of-theart grammars.","Ninomiya et al (1997) propose a parallel CKY parser on a distributed-memory parallel machine consisting of 256 nodes, where each node contains a single processor.","Using their parallel language, they parallelize over cells in the chart, assigning each chart cell to each node in the machine.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(Goodman, 1997",W11-2921,It should be noted that there are a also number of orthogonal approaches for accelerating natural language parsers.,"Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b).",These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Charniak and Johnson, 2005",W11-2921,It should be noted that there are a also number of orthogonal approaches for accelerating natural language parsers.,"Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b).",These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Petrov and Klein, 2007b)",W11-2921,It should be noted that there are a also number of orthogonal approaches for accelerating natural language parsers.,"Those approaches often rely on coarse approximations to the grammar of interest (Goodman, 1997; Charniak and Johnson, 2005; Petrov and Klein, 2007b).",These coarse models are used to constrain and prune the search space of possible parse trees before applying the final model of interest.,http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(Klein and Manning, 2003",W11-2921,Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements.,"There are also some optimality preserving approaches based on A*-search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference.","We suspect that most of the ideas therein are orthogonal to our approach, and therefore leave their integration into our GPU-based parser for future work.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"Pauls and Klein, 2009)",W11-2921,Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements.,"There are also some optimality preserving approaches based on A*-search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference.","We suspect that most of the ideas therein are orthogonal to our approach, and therefore leave their integration into our GPU-based parser for future work.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(Dunlop et al., 2011)",W11-2921,Our approach in contrast preserves optimality and could in principle be combined with such multi-pass approaches to yield additional speed improvements.,"There are also some optimality preserving approaches based on A*-search techniques (Klein and Manning, 2003; Pauls and Klein, 2009) or grammar refactoring (Dunlop et al., 2011) that aim to speed up CKY inference.","We suspect that most of the ideas therein are orthogonal to our approach, and therefore leave their integration into our GPU-based parser for future work.",http://www.aclweb.org/anthology/W/W11/W11-2921.pdf
"(Chiarcos et al., 2011)",W13-5507,1 Introduction,"In recent years, many linguistic resources have been released as Linked Data (Chiarcos et al., 2011).","Most of the datasets that are part of the so called Linguistic Linked Open Data (LLOD) cloud consist of dictionaries, written corpora or lexica.",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Windhouwer and Wright, 2012)",W13-5507,"Due to the challenging nature of the data, in particular that it contains annotations on multiple timelines, we developed a new model for the representation of this data, which we call FiESTA.","In order to express both established and new data categories and properties, from linguistics as well as from nonlinguistic communication, we developed a new data category registry, which contains links to other resources in the LLOD cloud, in particular to the ISOcat data category repository (Windhouwer and Wright, 2012), but also serves as a place where categories from novel research fields (mainly multimodal communication) can be collected, discussed, until they have settled down and are stable enough for an integration into more authoritative category registries, such as ISOcat.",By means of this we aim to make the re,http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Ruby et al., 2011)",W13-5507,source more widely available and to enable a long and successful lifecycle for the resource.,"Furthermore, we describe a software toolchain for easy extraction of RDF data from existing information structures, such as classes or database records, and delivery of this data via web applications and services based on the popular framework Rails (Ruby et al., 2011).","This tool chain is designed to be easy to integrate with existing libraries in a plugin-like fashion, in order to reduce the effort of integrating existing systems into Linked Data networks and infrastructures.",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Lehmann, 2005, ",W13-5507,"Based on this automatically generated data, several annotations have been created: 1Terms like primary and secondary data are problematic when we go beyond classical face-to-face dialogues preserved in audio and video recordings.","We use these terms in Lehmann's reading: ""Primary linguistic data are [...] representations of [...] speech events with their spatio-temporal coordinates"" (Lehmann, 2005, p. 187).","However, his distinction between raw (=non-symbolic) and processed (=symbolic) data (Lehmann, 2005, pp.",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Lehmann, 2005, ",W13-5507,"We use these terms in Lehmann's reading: ""Primary linguistic data are [...] representations of [...] speech events with their spatio-temporal coordinates"" (Lehmann, 2005, p. 187).","However, his distinction between raw (=non-symbolic) and processed (=symbolic) data (Lehmann, 2005, pp.",205ff.),http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Klein and Manning, 2002",W13-5507,2.,"Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008).",3.,http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"Klein and Manning, 2003)",W13-5507,2.,"Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008).",3.,http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Rafferty and Manning, 2008)",W13-5507,2.,"Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008).",3.,http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Bird and Liberman, 2001)",W13-5507,3.1 Internal representation,"We developed FiESTA (an acronym for ""format for extensive spatiotemporal annotations""), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).",There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.,http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Evert et al., 2003)",W13-5507,3.1 Internal representation,"We developed FiESTA (an acronym for ""format for extensive spatiotemporal annotations""), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).",There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.,http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(TEI Consortium, 2008)",W13-5507,3.1 Internal representation,"We developed FiESTA (an acronym for ""format for extensive spatiotemporal annotations""), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).",There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.,http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Ide et al., 2000)",W13-5507,3.1 Internal representation,"We developed FiESTA (an acronym for ""format for extensive spatiotemporal annotations""), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000).",There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.,http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Chiarcos, 2012)",W13-5507,There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.,"These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003).","One of the most pressing problems is the restriction to a single, flat stream or sequence ofprimary data (called ""text"" in some approaches), or a single, flat timeline.",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Ide et al., 2003)",W13-5507,There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures.,"These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003).","One of the most pressing problems is the restriction to a single, flat stream or sequence ofprimary data (called ""text"" in some approaches), or a single, flat timeline.",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Schmidt and Worner, 2005)",W13-5507,"With most of the given models, such an undertaking is either impossible, or it involves the alienation of model components (e.g., creation of phantom annotations being used as fake time points), which both inflates the resulting data structure and makes it less comprehensible.","For instance, the annotation tool EXMARaLDA provides a mechanism for creating time forks (Schmidt and Worner, 2005), but this is useful only for shorter stretches of simultaneous events surrounded by synchronised time points (e. g., for shorter segments of simultaneous speech), and not for timelines that might be completely independent from each other in the beginning and need to be merged and aligned later.","Also, there are various potential reasons in a scientific workflow that call for the use of an annotation tool different from EXMARaLDA.",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Stevens, 1946)",W13-5507,Figure 4: UML class diagram (simplified) of the FiESTA data model.,"and a different level of measurement, following (Stevens, 1946).","Scales can be left independent, or a synchronisation betweeen them can be expressed (e. g., a linear transformation between a video-frame-based scale and a millisecond-based one, or a manual alignment using explicit alignment points).",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"(Windhouwer and Wright, 2012)",W13-5507,"A category consists of (1) an identifier (which automatically is suffixed to the ontology URI to create an URI for the category), (2) a humanreadable label, (3) a human-readable definition (typically consisting of one or two sentences), (4) information about the class hierarchy, (S) information about possible domains and ranges, and (6) a number of relations, which express equivalence and similarity relations to other categories already existing outside the system (using appropriate vocabulary, such as rdfs:seeAlso or owl:sameAs).","We added some convenience methods for easy linking to some vocabularies or concept registries, among them, ISOcat (Windhouwer and Wright, 2012), XML Schema, Dublin Core, FOAF, and others.","At the moment, the ontology describing the FiESTA data model (cf.",http://www.aclweb.org/anthology/W/W13/W13-5507.pdf
"Kay and Roscheisen, 1988, ",P93-1003,Areas of investigation using bilingual corpora have included the following:," Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].","Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Brown et al., 1991a, ",P93-1003,Areas of investigation using bilingual corpora have included the following:," Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].","Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Gale and Church, 1991b]",P93-1003,Areas of investigation using bilingual corpora have included the following:," Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].","Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Dagan et al., 1991, ",P93-1003," Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].","Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].","Extracting word correspondences [Gale and Church, 1991a].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Brown et al., 1991b, ",P93-1003," Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].","Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].","Extracting word correspondences [Gale and Church, 1991a].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Church and Gale, 1991]",P93-1003," Automatic sentence alignment [Kay and Roscheisen, 1988, Brown et al., 1991a, Gale and Church, 1991b].","Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].","Extracting word correspondences [Gale and Church, 1991a].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Gale and Church, 1991a]",P93-1003,"Word-sense disambiguation [Dagan et al., 1991, Brown et al., 1991b, Church and Gale, 1991].","Extracting word correspondences [Gale and Church, 1991a].","Finding bilingual collocations [Smadja, 1992].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Smadja, 1992]",P93-1003,"Extracting word correspondences [Gale and Church, 1991a].","Finding bilingual collocations [Smadja, 1992].","Estimating parameters for statistically-based machine translation [Brown et al., 1992].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Brown et al., 1992]",P93-1003,"Finding bilingual collocations [Smadja, 1992].","Estimating parameters for statistically-based machine translation [Brown et al., 1992].","The work described here makes use of the aligned Canadian Hansards [Gale and Church, 1991b] to obtain noun phrase correspondences between the English and French text.",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Gale and Church, 1991b]",P93-1003,"Estimating parameters for statistically-based machine translation [Brown et al., 1992].","The work described here makes use of the aligned Canadian Hansards [Gale and Church, 1991b] to obtain noun phrase correspondences between the English and French text.","The term ""correspondence"" is used here to signify a mapping between words in two aligned sentences.",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Gale and Church, 1991a]",P93-1003,A word sequence in Ei is defined here as the correspondence of another sequence in Fi if the words of one sequence are considered to represent the words in the other.,"Single word correspondences have been investigated [Gale and Church, 1991a] using a statistic operating on contingency tables.","An algorithm for producing collocational correspondences has also been described [Smadja, 1992].",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Smadja, 1992]",P93-1003,"Single word correspondences have been investigated [Gale and Church, 1991a] using a statistic operating on contingency tables.","An algorithm for producing collocational correspondences has also been described [Smadja, 1992].",The algorithm involves several steps.,http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Kupiec, 1992",P93-1003,"Each tagger contains a hidden Markov model (HMM), which is trained using samples of raw text from the Hansards for each language.","The taggers are robust and operate with a low error rate [Kupiec, 19921.",Simple noun phrases (excluding pronouns and digits) are then extracted from the sentences by finite-state recognizers that are specified by regular expressions defined in terms of part-ofspeech categories.,http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Dempster et al., 1977]",P93-1003,An arbitrarily large corpus can be accommodated by segmenting it appropriately.,"The algorithm described here is an instance of a general approach to statistical estimation, represented by the EM algorithm [Dempster et al., 1977].","In contrast to reservations that have been expressed [Gale and Church, 1991a] about using the EM algorithm to provide word correspondences, there have been no indications that prohibitive amounts of memory might be required, or that the approach lacks robustness.",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Gale and Church, 1991a]",P93-1003,"The algorithm described here is an instance of a general approach to statistical estimation, represented by the EM algorithm [Dempster et al., 1977].","In contrast to reservations that have been expressed [Gale and Church, 1991a] about using the EM algorithm to provide word correspondences, there have been no indications that prohibitive amounts of memory might be required, or that the approach lacks robustness.","Unlike the other methods that have been mentioned, the approach has the capability to accommodate more context to improve performance.",http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
"Cutting et al., 1992]",P93-1003,"This situation is very similar to that involved in training HMM text taggers, where joint probabilities are computed that a particular word corresponds to a particular part-ofspeech, and the rest of the words in the sentence are also generated (e.g.","[Cutting et al., 1992]).",CONCLUSION,http://www.aclweb.org/anthology/P/P93/P93-1003.pdf
(2004),W10-2912,"Computational models present an opportunity to test the potentially innate constraints, structures, and algorithms that a child may be using to guide her acquisition.",In this work we develop a segmentation model from the constraints suggested by Yang (2004) and evaluate it in idealized conditions and conditions that better approximate the environment of a child learner.,We seek to determine how these limitations in the learner's input and memory affect the learner's performance and to demonstrate that the presented learner is robust even under non-ideal conditions.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Goldwater et al., 2009",W10-2912,2 Related Work,"Most recent work in word segmentation of childdirected speech has operated within statistical optimization frameworks, particularly Bayesian approaches (Goldwater et al., 2009; Johnson and Goldwater, 2009).",These models have established the state-of-the-art for the task of selecting appropriate word boundaries from a stream of unstructured phonemes.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"Johnson and Goldwater, 2009)",W10-2912,2 Related Work,"Most recent work in word segmentation of childdirected speech has operated within statistical optimization frameworks, particularly Bayesian approaches (Goldwater et al., 2009; Johnson and Goldwater, 2009).",These models have established the state-of-the-art for the task of selecting appropriate word boundaries from a stream of unstructured phonemes.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Brown, 1973)",W10-2912,Trying to find cognitive insight from these types of models is difficult because of the inherent mismatch in the quality and types of hypotheses they maintain during learning.,"Children are incremental learners (Brown, 1973), and learners relying on statistical optimization are generally not.","A child's competence grows gradually as she hears and produces more and more utterances, going through predictable changes to her working grammar (Marcus et al., 1992) that statistical optimization techniques typically do not go through and do not intend to replicate.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Marcus et al., 1992)",W10-2912,"Children are incremental learners (Brown, 1973), and learners relying on statistical optimization are generally not.","A child's competence grows gradually as she hears and produces more and more utterances, going through predictable changes to her working grammar (Marcus et al., 1992) that statistical optimization techniques typically do not go through and do not intend to replicate.","Statistical models provide excellent information about the features, distributional cues, and priors that can be used in learning, but provide little information about how a child learner can use this information and how her knowledge of language develops as the learning process evolves.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Yang, 2004)",W10-2912,"Statistical models provide excellent information about the features, distributional cues, and priors that can be used in learning, but provide little information about how a child learner can use this information and how her knowledge of language develops as the learning process evolves.","Previous simulations in word segmentation using the same type of distributional information as many statistical optimization-based learners but without an optimization model suggest that statistics alone are not sufficient for learning to succeed in a computationally efficient online manner; further constraints on the search space are needed (Yang, 2004).",Previous computational models have demanded tremendous memory and computational capacity from human learners.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
(1996),W10-2912,"Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 88-97, Uppsala, Sweden, 15-16 July 2010. c2010 Association for Computational Linguistics","of Brent & Cartwright (1996) produces a set of possible lexicons that describe the learning corpus, each of which is evaluated as the learner iterates until no further improvement is possible.",It is unlikely that an algorithm of this type is something a human learner is capable of using given the requirement to remember at the very least a long history of recent utterances encountered and constantly reanalyze them to find a optimal segmentation.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
(1999),W10-2912,"On the other hand, previous computational models often underestimate the human learner's knowledge of linguistic representations.","Most of these models are ""synthetic"" in the sense of Brent (1999): the raw material for segmentation is a stream of segments, which are then successively grouped into larger units and eventually, conjectured words.","This assumption may make the child learner's job unnecessarily hard; since syllables are hierarchical structures consisting of segments, treating the linguistic data as unstructured segment sequences makes the problem harder than it actually is.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Johnson and Goldwater, 2009)",W10-2912,"Modeling the corpus using hierarchical grammars that can model the input at varying levels (word collocations, words, syllables, onsets, etc.)","provide the learner the most flexibility, allowing the learner to build structure from the individual phonemes and apply distributions at each level of abstraction (Johnson and Goldwater, 2009).","While this results in state-of-the-art performance for segmentation performed at the phoneme level, this approach requires significant computational resources as each additional level of representation increases the complexity of learning.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Saffran et al., 1996a",W10-2912,"In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner.","A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).","The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"Saffran et al., 1996b)",W10-2912,"In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner.","A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).","The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Swingley, 2005)",W10-2912,"In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner.","A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).","The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Bijeljac-Babic et al., 1993)",W10-2912,"A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).","The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.","Precision is 41.6%, and recall is 23.3%, which we will show is worse than a simple baseline of assuming every syllable is a word.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Yang, 2004)",W10-2912,"A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005).","The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly.","Precision is 41.6%, and recall is 23.3%, which we will show is worse than a simple baseline of assuming every syllable is a word.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Yang, 2004)",W10-2912,"While the pseudo-words used in infant studies measuring the ability to use transitional probability information are uniformly three-syllables long, much of child-directed English consists of sequences of monosyllabic words.","Corpus statistics reveal that on average a monosyllabic word is followed by another monosyllabic word 85% of time (Yang, 2004), and thus learners that use only local transitional probabilities without any global optimization are unlikely to succeed.","This problem does not affect online approaches that use global information, such as computing the maximum likelihood of the corpus incrementally (Venkataraman, 2001).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Venkataraman, 2001)",W10-2912,"Corpus statistics reveal that on average a monosyllabic word is followed by another monosyllabic word 85% of time (Yang, 2004), and thus learners that use only local transitional probabilities without any global optimization are unlikely to succeed.","This problem does not affect online approaches that use global information, such as computing the maximum likelihood of the corpus incrementally (Venkataraman, 2001).","Since these approaches do not require each boundary be a local minimum, they are able to correctly handle a sequence of monosyllable words.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Gold, 1967",W10-2912,3 Constraining the Learning Space,"Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning.","If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"Valiant, 1984",W10-2912,3 Constraining the Learning Space,"Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning.","If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"Vapnik, 2000)",W10-2912,3 Constraining the Learning Space,"Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning.","If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
(2004),W10-2912,"It is important to identify such constraints to see to what extent they complement, or even replace, domain neutral learning mechanisms.","A particularly useful constraint for word segmentation, introduced to the problem of word segmentation by Yang (2004) but previously discussed by Halle and Vergnaud (1987), is as follows: Unique Stress Constraint (USC): A word can bear at most one primary stress.",A simple example of how adult learners might use the USC is upon hearing novel names or words.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
(1987),W10-2912,"It is important to identify such constraints to see to what extent they complement, or even replace, domain neutral learning mechanisms.","A particularly useful constraint for word segmentation, introduced to the problem of word segmentation by Yang (2004) but previously discussed by Halle and Vergnaud (1987), is as follows: Unique Stress Constraint (USC): A word can bear at most one primary stress.",A simple example of how adult learners might use the USC is upon hearing novel names or words.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
(2004),W10-2912,"Moreover, it also knows that in the window between S1 and S2 there must be one or more word boundaries.",Yang (2004) evaluates the effectiveness of the USC in conjunction with a simple approach to using transitional probabilities.,The performance of the approach presented there improves dramatically if the learner is equipped with the assumption that each word can have only one primary stress.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Jusczyk, 1999)",W10-2912,"If the learner knows this, then it may limit the search for local minima to only the window between two syllables that both bear primary stress, e.g., between the two a's in the sequence languageacquisition.","This assumption is plausible given that 7.5-month-old infants are sensitive to strong/weak prosodic distinctions (Jusczyk, 1999).","Yang's stress-delimited algorithm achieves the precision of 73.5% and recall of 71.2%, a significant improvement over using TPs alone, but still below the baseline presented in our results.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Liberman and Prince, 1977)",W10-2912,"It should be noted that the classification of every syllable as ""weak"" or ""strong"" is a significant simplification.","Stress is better organized into hierarchical patterns constructed on top of syllables that vary in relative prominence based on the domain of each level of the hierarchy, and generally languages avoid adjacent strong syllables (Liberman and Prince, 1977).",We later discuss a manipulation of the corpus used by Yang (2004) to address this concern.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
(2004),W10-2912,"Stress is better organized into hierarchical patterns constructed on top of syllables that vary in relative prominence based on the domain of each level of the hierarchy, and generally languages avoid adjacent strong syllables (Liberman and Prince, 1977).",We later discuss a manipulation of the corpus used by Yang (2004) to address this concern.,"Additionally, there are significant challenges in reconstructing stress from an acoustic signal (Van Kuijk and Boves, 1999).",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Van Kuijk and Boves, 1999)",W10-2912,We later discuss a manipulation of the corpus used by Yang (2004) to address this concern.,"Additionally, there are significant challenges in reconstructing stress from an acoustic signal (Van Kuijk and Boves, 1999).","For a child learner to use the algorithm presented here, she would need to have mechanisms for detecting stress in the speech signal and categorizing the gradient stress in utterances into a discrete level for each syllable.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Jusczyk et al., 1999)",W10-2912,90,"formation to detect word boundaries (Jusczyk et al., 1999), we believe that it is reasonable to assume that identifying syllabic stress is a task an infant learner can perform at the developmental stage of word segmentation.",4 A Simple Algorithm for Word Segmentation,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
(2004),W10-2912,4 A Simple Algorithm for Word Segmentation,We now present a simple algebraic approach to word segmentation based on the constraints suggested by Yang (2004).,The learner we present is algebraic in that it has a lexicon which stores previously segmented words and identifies the input as a combination of words already in the lexicon and novel words.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Anderson et al., 1998",W10-2912,"pr(word) is the probability of a word being retrieved,  is a constant, and c(word) is the number of times the word has been identified in segmentations thus far.","This type of memory function is a simplified representation of models of humans' memory recall capabilities (Anderson et al., 1998; Gillund and Shiffrin, 1984).","This memory function for the value of  = 0.05, the value used in our experiments, is given in Figure 1.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"Gillund and Shiffrin, 1984)",W10-2912,"pr(word) is the probability of a word being retrieved,  is a constant, and c(word) is the number of times the word has been identified in segmentations thus far.","This type of memory function is a simplified representation of models of humans' memory recall capabilities (Anderson et al., 1998; Gillund and Shiffrin, 1984).","This memory function for the value of  = 0.05, the value used in our experiments, is given in Figure 1.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Goldwater et al., 2006)",W10-2912,"Probabilistic word recall results in a ""rich get richer"" phenomenon as the learner segments; words that are used more often in segmentations are more likely to be reused in later segmentations.","While recent work from Bayesian approaches has used a Dirichlet Process to generate these distributions (Goldwater et al., 2006), in this learner the reuse of frequent items in learning is a result of the memory model rather than an explicit process of reusing old outcomes or generating new ones.",This growth is an inherent property of the cognitive model of memory used here rather than an externally imposed computational technique.,http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
(2004),W10-2912,Our computational model is designed to process child-directed speech.,The corpus we use to evaluate it is the same corpus used by Yang (2004).,"Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
(1973),W10-2912,The corpus we use to evaluate it is the same corpus used by Yang (2004).,"Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.","We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(MacWhinney, 2000)",W10-2912,The corpus we use to evaluate it is the same corpus used by Yang (2004).,"Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.","We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Weide, 1998)",W10-2912,"Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah.","We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word.","In CMUdict, lexical stress information is preserved by numbers: 0 for unstressed, 1 for primary stress, 2 for secondary stress.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Mattys and Jusczyk, 2001)",W10-2912,"While we performed syllabification as a preprocessing step outside of learning, a child learner would presumably learn the required phonotactics as a part of learning to segment words.","9month old infants are believed to have learned some phonotactic constraints of their native language (Mattys and Jusczyk, 2001), and learning these constraints can be done with only minimal exposure (Onishi et al., 2002).","Finally, spaces and punctuation between words were removed, but the boundaries between utterances-as indicated by line breaks in CHILDES-are retained.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Onishi et al., 2002)",W10-2912,"While we performed syllabification as a preprocessing step outside of learning, a child learner would presumably learn the required phonotactics as a part of learning to segment words.","9month old infants are believed to have learned some phonotactic constraints of their native language (Mattys and Jusczyk, 2001), and learning these constraints can be done with only minimal exposure (Onishi et al., 2002).","Finally, spaces and punctuation between words were removed, but the boundaries between utterances-as indicated by line breaks in CHILDES-are retained.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Elman, 1993)",W10-2912,But this slower learning is unlikely to be a concern for a child learner who would be exposed to much larger amounts of data than the corpora here provide.,"Cognitive literature suggests that limited memory during learning may be essential to a learner in its early stages (Elman, 1993).","But we do not see any notable improvement as a result of the probabilistic memory model used in our experiments, although the learner does do better in the Reduced Stress condition with Probabilistic Memory than Perfect Memory.",http://www.aclweb.org/anthology/W/W10/W10-2912.pdf
"(Jiao et al., 2006)",D15-1142,"However, the reliability of CWS that can be achieved using machine learning techniques relies heavily on the availability of a large amount of high-quality, manually segmented data.","Because hand-labeling individual words and word boundaries is very difficult (Jiao et al., 2006), producing segmented Chinese texts is very time-consuming and expensive.","Although a number of manually segmented datasets have been constructed by various organizations, it is not feasible to combine them into a single complete dataset because of their incompatibility due to the use of various segmenting standards.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Jiao et al., 2006)",D15-1142,"To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years.","These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).","In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Liang et al., 2005)",D15-1142,"To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years.","These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).","In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Sun and Xu, 2011)",D15-1142,"To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years.","These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).","In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Zeng et al., 2013a)",D15-1142,"To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years.","These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).","In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Zeng et al., 2013b)",D15-1142,"These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora).","In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed.","However, because monolingual unlabeled data contain limited natural segmenting information, in most semisupervised methods, the objective function tends to be optimized based on the personal experience and knowledge of the researchers.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Xu et al., 2008)",D15-1142,"Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.","In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).","However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Chang et al., 2008)",D15-1142,"Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.","In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).","However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Ma and Way, 2009)",D15-1142,"Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.","In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).","However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Chung et al., 2009)",D15-1142,"Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.","In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).","However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Xi et al., 2012)",D15-1142,"Figure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus.","In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)).","However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Xue et al., 2003)",D15-1142,Considerable efforts have been made in the NLP community in the study of Chinese word segmentation.,"The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003).","Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Peng et al., 2004)",D15-1142,"The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003).","Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et",1208,http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Zhang and Clark, 2007)",D15-1142,1208,"al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)).","However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Zhao et al., 2010)",D15-1142,1208,"al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)).","However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Sun and Xu, 2011)",D15-1142,"To address this problem, a number of semisupervised CWS approaches have been intensively investigated in recent years.","For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CRF model.","(Zeng et al., 2013a) introduced a graph-based semi-supervised joint model of Chinese word segmentation and part-of-speech tagging and regularized the learning of a linear CRF model based on the label distributions derived from unlabeled data.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Zeng et al., 2013a)",D15-1142,"For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CRF model.","(Zeng et al., 2013a) introduced a graph-based semi-supervised joint model of Chinese word segmentation and part-of-speech tagging and regularized the learning of a linear CRF model based on the label distributions derived from unlabeled data.","However, because monolingual unlabeled data lack natural segmenting information, most previous semi-supervised CWS methods have required certain assumptions to be made regarding their objective functions based on the researchers' personal experiences.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Xu et al., 2008)",D15-1142,2.2 Bilingual Semi-supervised CWS Methods,"Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Chang et al., 2008)",D15-1142,2.2 Bilingual Semi-supervised CWS Methods,"Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Ma and Way, 2009)",D15-1142,2.2 Bilingual Semi-supervised CWS Methods,"Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Chung et al., 2009)",D15-1142,2.2 Bilingual Semi-supervised CWS Methods,"Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Xi et al., 2012)",D15-1142,2.2 Bilingual Semi-supervised CWS Methods,"Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Xu et al., 2004)",D15-1142,"Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).","(Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Peng et al., 2004)",D15-1142,"Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT).","These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).","(Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Zeng et al., 2014)",D15-1142,"These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004).","(Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT.","However, because most of these approaches focus on SMT performance, they emphasize decreasing the perplexity of the bilingual data and word alignment rather than improving the CWS accuracy.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Lafferty et al., 2001)",D15-1142,3.1 Character-level Feature,"The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model's effectiveness in detecting OOV words.","In this paper, we score the character-level feature in the same manner defined by (Xue et al., 2003).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Xue et al., 2003)",D15-1142,3.1 Character-level Feature,"The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model's effectiveness in detecting OOV words.","In this paper, we score the character-level feature in the same manner defined by (Xue et al., 2003).",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Xue et al., 2003)",D15-1142,"The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model's effectiveness in detecting OOV words.","In this paper, we score the character-level feature in the same manner defined by (Xue et al., 2003).","For the jth character cj in the sentence cJ1 = c1...cJ, the score can be calculated as follows:",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Lafferty et al., 2001)",D15-1142,"yj1 and yj represent the tags of the previous and current characters, respectively.","We do not introduce the CRF-based CWS model in detail here, but more information can be obtained from (Lafferty et al., 2001) and (Xue et al., 2003).",3.2 Phrase-level Features,http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Xue et al., 2003)",D15-1142,"yj1 and yj represent the tags of the previous and current characters, respectively.","We do not introduce the CRF-based CWS model in detail here, but more information can be obtained from (Lafferty et al., 2001) and (Xue et al., 2003).",3.2 Phrase-level Features,http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Neubig et al., 2011)",D15-1142,"Then, they are associated with English words using a statistical word aligner.","By representing the English and Chinese sentences as eI1 = e1e2...eI and cJ1 = c1c2...cJ, respectively, where ei and cj represent single elements of the sentences, we define their alignment as aK1 , of which each element is a span ak =< s, t > and represents the alignment of the English word es with the Chinese character ct. Then, the corpus of unlabeled bilingual data can be represented as the set of sentence tuples <eI1, cJ1 , aK1 > To obtain the character-based alignment, we employ an open-source toolkit Pialign1 ((Neubig et al., 2011); (Neubig et al., 2012)) which uses Bayesian learning and inversion transduction grammars.",3.2.2 Features Obtained from the,http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Neubig et al., 2012)",D15-1142,"Then, they are associated with English words using a statistical word aligner.","By representing the English and Chinese sentences as eI1 = e1e2...eI and cJ1 = c1c2...cJ, respectively, where ei and cj represent single elements of the sentences, we define their alignment as aK1 , of which each element is a span ak =< s, t > and represents the alignment of the English word es with the Chinese character ct. Then, the corpus of unlabeled bilingual data can be represented as the set of sentence tuples <eI1, cJ1 , aK1 > To obtain the character-based alignment, we employ an open-source toolkit Pialign1 ((Neubig et al., 2011); (Neubig et al., 2012)) which uses Bayesian learning and inversion transduction grammars.",3.2.2 Features Obtained from the,http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Och and Ney, 2000)",D15-1142,"Specifically, we apply two standard log-linear phrase-based SMT models.","The GIZA++ aligner is adopted to obtain word alignments (Och and Ney, 2000) from the converted NE dictionary.","The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Koehn et al., 2003)",D15-1142,"The GIZA++ aligner is adopted to obtain word alignments (Och and Ney, 2000) from the converted NE dictionary.","The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables.","A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Stolcke et al., 2002)",D15-1142,"The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables.","A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language.","Moses (Koehn et al., 2007) is used as a decoder.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Koehn et al., 2007)",D15-1142,"A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language.","Moses (Koehn et al., 2007) is used as a decoder.","Minimum error rate training (MERT) (Och et al., 2003) is applied to tune the feature parameters on the development dataset.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Och et al., 2003)",D15-1142,"Moses (Koehn et al., 2007) is used as a decoder.","Minimum error rate training (MERT) (Och et al., 2003) is applied to tune the feature parameters on the development dataset.","Given these two phrase-based translation models, we calculate each span < i, jm, jn > in AOne for the Chinese word wn using the following formula: Str(< i, jm, jn >) = Schen(< i, jm, jn >) +Sench(< i, jm, jn >) (6) where Schen(<i,jm,jn>) = DLev(Fletei, PTchen(Fpy(cjn jm))) means that the pronunciation conversion in the Chinese-English direction is performed as follows: First, the English word ei is split into its constituent letters; Second, the sequence of Chinese characters cjn jm is converted into its pronunciation; Third, this pronunciation is input into the Chinese-English phrase-based translation model, and the corresponding translation result is obtained; And finally, the Levenshtein distance between the English letters and the translation result is returned.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Mikolov et al., 2013)",D15-1142,"English-Chinese semantic gap feature To guarantee that the semantic meanings of the Chinese segmentation match those of the corresponding English sentences as closely as possible, we propose to use a feature based on the EnglishChinese semantic gap to ensure the retention of semantic meaning during the segmentation process.","First, we pre-train word embeddings using the open-source toolkit Word2Vec (Mikolov et al., 2013) on the Chinese (segmented using characterlevel features only) and English sentences separately, thereby obtaining the vocabularies Vch and Ven and their corresponding embedding matrixes Lch E Rn|Vch |and Len E Rn|Ven|.","Given a Chinese word wn with an index i in the vocabulary, it is then straightforward to retrieve the word's vector representation via simple multiplication with a binary vector d that is equal to zero at all positions except that with index i:",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Tian et al., 2014)",D15-1142,Table 1: Statistics of training and testing datasets,"Moreover, the bilingual unlabeled data is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014).","There are in total 2,215,000 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Peng et al., 2004)",D15-1142,"In the following, we refer to our methods as ""SLBD"" (segmenter leveraging bilingual data).","Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.","Moreover, we also evaluated the performance of our sub-models by",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Asahara et al., 2005)",D15-1142,"In the following, we refer to our methods as ""SLBD"" (segmenter leveraging bilingual data).","Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.","Moreover, we also evaluated the performance of our sub-models by",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Zhang and Clark, 2007)",D15-1142,"In the following, we refer to our methods as ""SLBD"" (segmenter leveraging bilingual data).","Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.","Moreover, we also evaluated the performance of our sub-models by",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Zhao et al., 2010)",D15-1142,"In the following, we refer to our methods as ""SLBD"" (segmenter leveraging bilingual data).","Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data.","Moreover, we also evaluated the performance of our sub-models by",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Sun et al., 2012)",D15-1142,"Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model.","Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&X); (Zeng et al., 2013b) (Zeng).","To ensure a fair comparison, we performed the evaluation in two steps.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Sun and Xu, 2011)",D15-1142,"Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model.","Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&X); (Zeng et al., 2013b) (Zeng).","To ensure a fair comparison, we performed the evaluation in two steps.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Zeng et al., 2013b)",D15-1142,"Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model.","Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&X); (Zeng et al., 2013b) (Zeng).","To ensure a fair comparison, we performed the evaluation in two steps.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Xu et al., 2008)",D15-1142,"The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.","Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).","The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Ma and Way, 2009)",D15-1142,"The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.","Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).","The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Xi et al., 2012)",D15-1142,"The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.","Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).","The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"(Zeng et al., 2014)",D15-1142,"The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.","Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014).","The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation.",http://www.aclweb.org/anthology/D/D15/D15-1142.pdf
"of India, 2001)",W14-5502,Konkani is an Indian language spoken bv approximatelv 2.5 million people (Gov.,"of India, 2001), mainlv in the Indian state of Goa.",It also has a substantial amount of linguistic minoritv population living in neighboring states of Karnataka and Kerala.,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
"(Arbabi et al., 1994",W14-5502,Machine transliteration is also useful for cross-language information retrieval (CLIR).,"Consequentlv, a significant amount of work has been done in this field (Arbabi et al., 1994; Knight and Graehl, 1998).",Machine transliteration can generallv be classified as rule-based or statistical depending on the approach.,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
"Knight and Graehl, 1998)",W14-5502,Machine transliteration is also useful for cross-language information retrieval (CLIR).,"Consequentlv, a significant amount of work has been done in this field (Arbabi et al., 1994; Knight and Graehl, 1998).",Machine transliteration can generallv be classified as rule-based or statistical depending on the approach.,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
(2008),W14-5502,"Within Indic transliteration, there have been several attempts on rule-based approaches.",Malik et al (2008) implemented a Hindi-Urdu transliteration svstem with finite-state transducers using a universal intermediate transcription (UIT).,It was based on the graphemic equivalence between Perso-Arabic script and Devanagari script.,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
(2011),W14-5502,It was based on the graphemic equivalence between Perso-Arabic script and Devanagari script.,"Similarlv, Kishorjit (2011) developed a rule-based transliteration svstem also based on direct graphemic correspondence between Meetei Mavek and Bengali script.","On the other hand, statistical machine transliteration svstems tvpicallv use procedures familiar from statistical machine translation, including character alignments and subsequent training on the aligned data.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
(2009),W14-5502,"On the other hand, statistical machine transliteration svstems tvpicallv use procedures familiar from statistical machine translation, including character alignments and subsequent training on the aligned data.","Jia et al (2009) also developed a noisv channel model for the English-Chinese language pair using Moses, an SMT tool.",Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses.,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
(2013),W14-5502,"Jia et al (2009) also developed a noisv channel model for the English-Chinese language pair using Moses, an SMT tool.",Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses.,"Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM).",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
(2009),W14-5502,Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses.,"Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM).",Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration.,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
(2012),W14-5502,"Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM).",Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration.,A hvbrid approach combining FSM based techniques with a statistical word language model with better performance was proposed bv Malik et al (2009).,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
(2009),W14-5502,Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration.,A hvbrid approach combining FSM based techniques with a statistical word language model with better performance was proposed bv Malik et al (2009).,3 Initial Attempts,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
"(Allauzen et al., 2007)",W14-5502,4 Architecture,"The implementation of the framework has been done entirelv using OpenFst (Allauzen et al., 2007).","Thrax (Tai et al., 2011) was used to define context-dependent rewrite rules and compile those rules into finite-state transducers compatible with Openfst.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
"(Tai et al., 2011)",W14-5502,"The implementation of the framework has been done entirelv using OpenFst (Allauzen et al., 2007).","Thrax (Tai et al., 2011) was used to define context-dependent rewrite rules and compile those rules into finite-state transducers compatible with Openfst.",Thrax was also used to define finite state acceptors.,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
"(Novak et al., 2011)",W14-5502,We found Thrax to be particularlv robust and flexible in generating various FSTs.,"Character alignments were performed using Phonetisaurus (Novak et al., 2011).","N-gram models were created with the OpenGrm Ngram librarv (Roark et al., 2012), which also generates the models as FSTs.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
"(Roark et al., 2012)",W14-5502,"Character alignments were performed using Phonetisaurus (Novak et al., 2011).","N-gram models were created with the OpenGrm Ngram librarv (Roark et al., 2012), which also generates the models as FSTs.","Below, we describe the detailed architecture for each transliteration pair.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
"(Narasimhan et al., 2004)",W14-5502,The rules below are listed with a corresponding sample case.,"These rules were compiled as finite-state transducers (Narasimhan et al., 2004) We have produced a list of possible suffixes and prefixes from the collected corpus.",Let Pre denote the set of prefixes and 8uf the set of prefixes.,http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
"(Jiampojamarn et al., 2007)",W14-5502,"Romanizing the input verv marginallv improves the character alignment process, since both input and output are then alphabetic scripts (as compared to svllabic to alphabetic alignment).","We initiallv experimented with tools such as GIZA++, but found Phonetisaurus produced better alignments compared to other tools as it uses manv-to-manv alignments developed specificallv for grapheme to phoneme svstems (Jiampojamarn et al., 2007).","A sample alignment sequence from Phonetisaurus is given below: melillem I mellil'lem  m}m e}e 1}lel i}i l}l1' l}l e}e m}m gadvemtlvan I gaddientlean  g}g a}a d}dld v}i e}e m}n t}t l}l v}e a}a n}n bharatasarkva I bharotasarkea  bh}blh a}a r}r a}o t}t a}a s}s a}a r}r k}k v}e a}a bhiveli I bhieli  bh}blh ilv}i e}e l}l i}i Where } denotes individual character alignment, I between characters indicates grapheme chunks, and d}dld implies that the source grapheme ed is mapped to the target graphemic chunk edd.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
"(Galescu and Allen, 2002)",W14-5502,Figure 5: 5 best paths of Lp for Kannada input 26Af aikv,"This alignment lattice was then used to create a joint sequence n-gram model (Galescu and Allen, 2002) Nknrm.","This is then composed with the input word I, whose output projection we use.",http://www.aclweb.org/anthology/W/W14/W14-5502.pdf
"(Jijkoun and de Rijke, 2005)",W07-1431,"A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle.","Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).","Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: No case of indigenously acquired rabies infection has been confirmed in the past 2 years.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Romano et al., 2006)",W07-1431,"A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle.","Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).","Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: No case of indigenously acquired rabies infection has been confirmed in the past 2 years.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Hickl et al., 2006)",W07-1431,"A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle.","Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).","Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: No case of indigenously acquired rabies infection has been confirmed in the past 2 years.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Akhmatova, 2005",W07-1431,"Because it drops important qualifiers in a negative context, the hypothesis does not follow; yet both the lexical content and the predicate-argument structure of the hypothesis closely match the premise.","At the other extreme, textual inference can be approached as deduction, building on work in formal computational semantics to translate sentences into first-order logic (FOL), and then applying a theorem prover or a model builder (Akhmatova, 2005; Fowler et al., 2005).","However, such approaches tend to founder on the difficulty of accurately translating natural language in FOL-tricky issues include idioms, intensionality and propositional attitudes, modalities, temporal and causal relations, certain quantifiers, and so on.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"Fowler et al., 2005)",W07-1431,"Because it drops important qualifiers in a negative context, the hypothesis does not follow; yet both the lexical content and the predicate-argument structure of the hypothesis closely match the premise.","At the other extreme, textual inference can be approached as deduction, building on work in formal computational semantics to translate sentences into first-order logic (FOL), and then applying a theorem prover or a model builder (Akhmatova, 2005; Fowler et al., 2005).","However, such approaches tend to founder on the difficulty of accurately translating natural language in FOL-tricky issues include idioms, intensionality and propositional attitudes, modalities, temporal and causal relations, certain quantifiers, and so on.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Bos and Markert, 2006)",W07-1431,"However, such approaches tend to founder on the difficulty of accurately translating natural language in FOL-tricky issues include idioms, intensionality and propositional attitudes, modalities, temporal and causal relations, certain quantifiers, and so on.","FOL-based systems that have attained high precision (Bos and Markert, 2006) have done so at the cost of very poor recall.","In this work, we explore a different point on the spectrum, by developing a computational model of natural logic, that is, a logic whose vehicle of inference is natural language.'",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Sanchez Valencia, 1991)",W07-1431,"Other elements can only be contracted (not expanded) salva veritate, and thus have negative polarity: meal can be narrowed to dinner.","The monotonicity calculus developed in (Sanchez Valencia, 1991) explains these polarity effects by (1) defining an entailment relation over multifarious expressions of natural language, (2) defining monotonicity properties of semantic functions, and finally (3) specifying how monotonicities combine during Fregean composition of semantic functions.",The entailment relation.,http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Marsi and Krahmer, 2005",W07-1431,3 The NatLog System,"Our natural logic system, dubbed the NatLog system, has a three-stage architecture similar to those in (Marsi and Krahmer, 2005; MacCartney et al., 2006), comprising (1) linguistic pre-preprocessing, (2) alignment, and (3) entailment classification.",3.1 Linguistic pre-processing,http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"MacCartney et al., 2006)",W07-1431,3 The NatLog System,"Our natural logic system, dubbed the NatLog system, has a three-stage architecture similar to those in (Marsi and Krahmer, 2005; MacCartney et al., 2006), comprising (1) linguistic pre-preprocessing, (2) alignment, and (3) entailment classification.",3.1 Linguistic pre-processing,http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Klein and Manning, 2003)",W07-1431,"Relative to other textual inference systems, the NatLog system does comparatively little linguistic preprocessing.","We rely on the Stanford parser (Klein and Manning, 2003), a Treebank-trained statistical parser, for tokenization, part-of-speech tagging, and phrase-structure parsing.","By far the most important analysis performed at this stage is monotonicity marking, in which we compute the effective mono",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Levy and Andrew, 2006)",W07-1431,"Unlike the categorial grammar parses assumed by Sanchez Valencia, the nesting of constituents in phrase-structure parses does not always correspond to the composition of semantic functions, which introduces a number of complications.","We define a list of downward-monotone and non-monotone expressions, and for each item we specify its arity and a Tregex pattern (Levy and Andrew, 2006) which permits us to identify its occurrences.","We also specify, for each argument, both the monotonicity and another Tregex pattern which helps us to determine the sentence span over which the monotonicity is projected.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Cooper et al., 1996)",W07-1431,4 Experiments with the FraCaS test suite,"The FraCaS test suite (Cooper et al., 1996) was developed as part of a collaborative research effort in computational semantics.",It contains 346 inference problems reminiscent of a textbook on formal semantics.,http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Dagan et al., 2005)",W07-1431,5 Experiments with RTE data,"Textual inference problems from the PASCAL RTE Challenge (Dagan et al., 2005) differ from FraCaS problems in several important ways.",(See table 5 for examples.),http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Bos and Markert, 2006)",W07-1431,"Rather, in applying NatLog to RTE, we hope to make reliable predictions on a subset of RTE problems, trading recall for precision.","If we succeed, then we may be able to hybridize with a broad-coverage RTE system to obtain better results than either system individually-the same strategy that was adopted by (Bos and Markert, 2006) for their FOL-based system.","For this purpose, we have chosen to use the Stanford RTE system described in (de Marneffe et al., 2006).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(de Marneffe et al., 2006)",W07-1431,"If we succeed, then we may be able to hybridize with a broad-coverage RTE system to obtain better results than either system individually-the same strategy that was adopted by (Bos and Markert, 2006) for their FOL-based system.","For this purpose, we have chosen to use the Stanford RTE system described in (de Marneffe et al., 2006).","In applying NatLog to RTE problems, we use alignments from the Stanford system as input to our entailment model.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Bos and Markert, 2006)",W07-1431,"Relative to the Stanford RTE system, NatLog achieves high precision on its yes predictions-about 76% on the development set, and 68% on the test set-suggesting that hybridizing may be effective.","For comparison, the FOL-based system reported in (Bos and Markert, 2006) attained a similarly high precision of 76% on RTE2 problems, but was able to make a positive prediction in only about 4% of cases.","NatLog makes positive predictions far more often-at a rate of 18% on the development set, and 24% on the test set.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Lakoff, 1970)",W07-1431,6 Related work,"While the roots of natural logic can be traced back to Aristotle's syllogisms, the modern conception of natural logic began with George Lakoff, who proposed ""a logic for natural language"" which could ""characterize all the valid inferences that can be made in natural language"" (Lakoff, 1970).","The study of natural logic was formalized by Johan van Benthem, who crucially connected it with categorial grammar (van Benthem, 1986), and later was brought to fruition by Victor Sanchez Valencia, who first gave a precise definition of a calculus of mono",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(van Benthem, 1986)",W07-1431,"While the roots of natural logic can be traced back to Aristotle's syllogisms, the modern conception of natural logic began with George Lakoff, who proposed ""a logic for natural language"" which could ""characterize all the valid inferences that can be made in natural language"" (Lakoff, 1970).","The study of natural logic was formalized by Johan van Benthem, who crucially connected it with categorial grammar (van Benthem, 1986), and later was brought to fruition by Victor Sanchez Valencia, who first gave a precise definition of a calculus of mono",199,http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Sanchez Valencia, 1991)",W07-1431,199,"tonicity (Sanchez Valencia, 1991).","A small current of theoretical work has continued up to the present, for example (Zamansky et al., 2006).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Zamansky et al., 2006)",W07-1431,"tonicity (Sanchez Valencia, 1991).","A small current of theoretical work has continued up to the present, for example (Zamansky et al., 2006).",There has been surprisingly little work on building computational models of natural logic.,http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Fyodorov et al., 2003)",W07-1431,There has been surprisingly little work on building computational models of natural logic.,"(Fyodorov et al., 2003) describes a Prolog implementation for a small fragment of English, based on a categorial grammar parser.6 In an unpublished draft, (van Eijck, 2005) describes a preliminary implementation in Haskell.","Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(van Eijck, 2005)",W07-1431,There has been surprisingly little work on building computational models of natural logic.,"(Fyodorov et al., 2003) describes a Prolog implementation for a small fragment of English, based on a categorial grammar parser.6 In an unpublished draft, (van Eijck, 2005) describes a preliminary implementation in Haskell.","Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985).",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Hobbs, 1985)",W07-1431,"(Fyodorov et al., 2003) describes a Prolog implementation for a small fragment of English, based on a categorial grammar parser.6 In an unpublished draft, (van Eijck, 2005) describes a preliminary implementation in Haskell.","Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985).","To our knowledge, the FraCaS results reported here represent the first such evaluation.",http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
"(Sukkarieh, 2003)",W07-1431,"To our knowledge, the FraCaS results reported here represent the first such evaluation.","(Sukkarieh, 2003) describes applying a deductive system to some FraCaS inferences, but does not perform a complete evaluation or report quantitative results.",7 Conclusion,http://www.aclweb.org/anthology/W/W07/W07-1431.pdf
(2003),N04-1032,Email: jurafsky@stanford.edu.,based on the SVM-based algorithm proposed for English by Pradhan et al (2003).,We first describe our creation of a small 1100-sentence Chinese corpus labeled according to principles from the English and (in-progress) Chinese PropBanks.,http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
(1999),N04-1032,"We then introduce the features used by our SVM classifier, and show their performance on semantic parsing for both seen and unseen verbs, given hand-corrected (Chinese TreeBank) syntactic parses.",We then describe our port of the Collins (1999) parser to Chinese.,"Finally, we apply our SVM semantic parser to a matching English corpus, and discuss the differences between English and Chinese that lead to significantly better performance on Chinese.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
"(Xue, 2002)",N04-1032,"Although a project to produce a Chinese PropBank is underway (Xue and Palmer 2003), this data is not expected to be available for another year.","For these experiments, we therefore hand-labeled a small corpus following the Penn Chinese Propbank labeling guidelines (Xue, 2002).","In this section, we first describe the semantic roles we used in the annotation and then introduce the data for our experiments.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
(2002),N04-1032,3.1 Architecture and Classifier,"Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem.","For each (non-aux/non-copula) verb in each sentence, our classifier examines each node in the syntactic parse tree for the sentence and assigns it a semantic role label.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
(2003),N04-1032,"Most constituents are not arguments of the verb, and so the most common label is NULL.","Our architecture is based on a Support Vector Machine classifier, following Pradhan et al (2003).","Since SVMs are binary classifiers, we represent this 1-of-19 classification problem (18 roles plus NULL) by training 19 binary one-versus-all classifiers.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
(2003),N04-1032,"Since SVMs are binary classifiers, we represent this 1-of-19 classification problem (18 roles plus NULL) by training 19 binary one-versus-all classifiers.","Following Pradhan et al (2003), we used tinySVM along with YamCha (Kudo and Matsumoto 2000, 2001) as the SVM training and test software.","The system uses a polynominal kernel with degree 2; the cost per unit violation of the margin, C=1; tolerance of the termination criterion e=0.001.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
(2002),N04-1032,feature is only applicable for NPs.,"In the original formulation for English in Gildea and Jurafsky (2002), it answers the question: Is the NP governed by IP or VP?","An NP governed by an IP is likely to be a subject, while an NP governed by a VP is more likely to be an object.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
(2002),N04-1032,"""the international Olympic Conference held in Paris"" Figure 1 Example of DE construction","Since the governing category information is encoded in the path feature, it may be redundant; indeed this redundancy might explain why the governing category feature was used in Gildea & Jurafsky(2002) but not in Gildea and Palmer(2002).","Since the ""DE"" construction caused us to modify the feature for Chinese, we conducted several experiments to test whether the governing category feature is useful or whether it is redundant with the path and position features.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
(2002),N04-1032,"""the international Olympic Conference held in Paris"" Figure 1 Example of DE construction","Since the governing category information is encoded in the path feature, it may be redundant; indeed this redundancy might explain why the governing category feature was used in Gildea & Jurafsky(2002) but not in Gildea and Palmer(2002).","Since the ""DE"" construction caused us to modify the feature for Chinese, we conducted several experiments to test whether the governing category feature is useful or whether it is redundant with the path and position features.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
(2003),N04-1032,These statistics emphasize the key role of the lexicalized head word feature in capturing the collocation between verbs and their arguments.,"Due to the sparsity of the head word feature, we also use the part-of-speech of the head word, following Surdeanu et al (2003).","For example, ""7 26 /July 26"" may not be seen in the training, but its POS, NT(temporal noun) , is a good indicator that it is a temporal.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
(1999),N04-1032,"In practical use, of course, automatic parses will not be as accurate.","In this section we describe experiments on semantic parsing when given automatic parses produced by an automatic parser, the Collins (1999) parser, ported to Chinese.",We first describe how we ported the Collins parser to Chinese and then present the results of the semantic parser with features drawn from the automatic parses.,http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
"(Collins, 1999)",N04-1032,4.1 The Collins parser for Chinese,"The Collins parser is a state-of-the-art statistical parser that has high performance on English (Collins, 1999) and Czech(Collins et al.",1999).,http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
"(Bikel and Chiang, 2000",N04-1032,1999).,"There have been attempts in applying other algorithms in Chinese parsing (Bikel and Chiang, 2000; Chiang and Bikel 2002; Levy and Manning 2003), but there has been no report on applying the Collins parser on Chinese.",The Collins parser is a lexicalized statistical parser based on a head-driven extended PCFG model; thus the choice of head node is crucial to the success of the parser.,http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
(1999),N04-1032,"Second, the features that we extracted for English semantic parsing worked well when applied to Chinese.",Many of these features required creating an automatic parse; in doing so we showed that the Collins (1999) parser when ported to Chinese achieved the best reported performance on Chinese syntactic parsing.,"Finally, we showed that semantic parsing is significantly easier in Chinese than in English.",http://www.aclweb.org/anthology/N/N04/N04-1032.pdf
"(Kim et al., 2004)",W07-1033,"Such a large variety of vocabulary naturally leads to long names with productive use of general words, making the task difficult to be solved by systems with naive Markov assumption of label sequences, because such systems must perform their prediction without seeing the entire string of the entities.","Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.","After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000).",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Zhou and Su, 2004)",W07-1033,"Such a large variety of vocabulary naturally leads to long names with productive use of general words, making the task difficult to be solved by systems with naive Markov assumption of label sequences, because such systems must perform their prediction without seeing the entire string of the entities.","Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.","After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000).",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Lafferty et al., 2001)",W07-1033,"Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.","After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000).","However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(McCallum et al., 2000)",W07-1033,"Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004), where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels.","After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001), which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000).","However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2006),W07-1033,Another reason may be that the computational complexity of the models prevented the developers to invent effective features for the task.,"We had to wait until Tsai et al (2006), who combine pattern-based postprocessing with CRFs, for CRF-based systems to achieve the same level of performance as Zhou et al.","As such, a key to further improvement of the performance of bio-entity recognition has been to employ global features, which are effective to capture the features of long names appearing in the bio domain.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Collins, 2000",W07-1033,"As such, a key to further improvement of the performance of bio-entity recognition has been to employ global features, which are effective to capture the features of long names appearing in the bio domain.","In this paper, we use reranking architecture, which was successfully applied to the task of natural language parsing (Collins, 2000; Charniak and",209,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"Johnson, 2005)",W07-1033,"BioNLP 2007: Biological, translational, and clinical language processing, pages 209-216, Prague, June 2007. c2007 Association for Computational Linguistics","Johnson, 2005), to address the problem.","Reranking enables us to incorporate truly global features to the model of named entity tagging, and we aim to realize the state-of-the-art performance without depending on rule-based post-processes.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2004),W07-1033,"Such systems may be classified into two kinds, one of them uses a single classifier which is optimized incorporating non-local features, and the other consists of pipeline of more than one classifiers.",The former includes Relational Markov Networks by Bunescu et al (2004) and skip-edge CRFs by Sutton et al (2004).,"A major drawback of this kind of systems may be heavy computational cost of inference both for training and running the systems, because non-local dependency forces such models to use expensive approximate inference instead of dynamic-programming-based exact inference.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2004),W07-1033,"Such systems may be classified into two kinds, one of them uses a single classifier which is optimized incorporating non-local features, and the other consists of pipeline of more than one classifiers.",The former includes Relational Markov Networks by Bunescu et al (2004) and skip-edge CRFs by Sutton et al (2004).,"A major drawback of this kind of systems may be heavy computational cost of inference both for training and running the systems, because non-local dependency forces such models to use expensive approximate inference instead of dynamic-programming-based exact inference.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2006),W07-1033,"A major drawback of this kind of systems may be heavy computational cost of inference both for training and running the systems, because non-local dependency forces such models to use expensive approximate inference instead of dynamic-programming-based exact inference.","The latter, pipelined systems include a recent study by Krishnan et al (2006), as well as our reranking system.","Their method is a two stage model of CRFs, where the second CRF uses the global information of the output of the first CRF.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Ohta et al., 2002)",W07-1033,"This section overviews the task of biomedical named entity recognition as presented in JNLPBA shared task held at COLING 2004, and the systems that were successfully applied to the task.","The training data provided by the shared task consisted of 2000 abstracts of biomedical articles taken from the GENIA corpus version 3 (Ohta et al., 2002), which consists of the MEDLINE abstracts with publication years from 1990 to 1999.",The articles are annotated with named-entity BIO tags as an example shown in Table 1.,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Kim et al., 2004)",W07-1033,The difference of publication years between the training and test sets reflects the organizer's intention to see the entity recognizers' portability with regard to the differences of the articles' publication years.,"Kim et al (Kim et al., 2004) compare the 8 systems participated in the shared task.","The systems use various classification models including CRFs, hidden Markov models (HMMs), support vector machines (SVMs), and MEMMs, with various features and external resources.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2004),W07-1033,"The systems use various classification models including CRFs, hidden Markov models (HMMs), support vector machines (SVMs), and MEMMs, with various features and external resources.","Though it is impossible to observe clear correlation between the performance and classification models or resources used, an important characteristic of the best system by Zhou et al (2004) seems to be extensive use of rule-based post processing they apply to the output of their classifier.","After the shared task, several researchers tackled the problem using the CRFs and their extensions.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2006),W07-1033,"After the shared task, several researchers tackled the problem using the CRFs and their extensions.","Okanohara et al (2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state.",Friedrich et al (2006) used CRFs with features from the external gazetteer.,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Sarawagi and Cohen, 2004)",W07-1033,"After the shared task, several researchers tackled the problem using the CRFs and their extensions.","Okanohara et al (2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state.",Friedrich et al (2006) used CRFs with features from the external gazetteer.,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2006),W07-1033,"Okanohara et al (2006) applied semiCRFs (Sarawagi and Cohen, 2004), which can treat multiple words as corresponding to a single state.",Friedrich et al (2006) used CRFs with features from the external gazetteer.,"Current state-of-the-art for the shared-task is achieved by Tsai et al (2006), whose improvement depends on careful design of features including the normalization of numeric expressions, and use of post-processing by automatically extracted patterns.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2006),W07-1033,Friedrich et al (2006) used CRFs with features from the external gazetteer.,"Current state-of-the-art for the shared-task is achieved by Tsai et al (2006), whose improvement depends on careful design of features including the normalization of numeric expressions, and use of post-processing by automatically extracted patterns.",210,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(McCallum et al., 2000)",W07-1033,3 N-best MEMM tagger,"As our n-best tagger, we use a first order MEMM model (McCallum et al., 2000).","Though CRFs (Lafferty et al., 2001) can be regarded as improved version of MEMMs, we have chosen MEMMs because MEMMs are usually much faster to train compared to CRFs, which enables extensive feature selection.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Lafferty et al., 2001)",W07-1033,"As our n-best tagger, we use a first order MEMM model (McCallum et al., 2000).","Though CRFs (Lafferty et al., 2001) can be regarded as improved version of MEMMs, we have chosen MEMMs because MEMMs are usually much faster to train compared to CRFs, which enables extensive feature selection.","Training a CRF tagger with features selected using an MEMM may result in yet another performance boost, but in this paper we concentrate on the MEMM as our n-best tagger, and consider CRFs as one of our future extensions.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Peshkin and Pfefer, 2003)",W07-1033,Table 1 shows the state transition table of our MEMM model.,"Though existing studies suggest that changing the tag set of the original corpus, such as splitting of O tags, can contribute to the performances of named entity recognizers (Peshkin and Pfefer, 2003), our system uses the original tagset of the training data, except that the 'BOS' label is added to represent the state before the beginning of sentences.",Probability of state transition to the i-th label of a sentence is calculated by the following formula:,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Berger et al., 1996)",W07-1033,Table 2: (Recall/Precision/F-score) of forward and backward tagging.,"where li is the next BIO tag, li1 is the previous BIO tag, S is the target sentence, and fj and lj are feature functions and parameters of a log-linear model (Berger et al., 1996).","As a first order MEMM, the probability of a label li is dependent on the previous label li1, and when we calculate the normalization constant in the right hand side (i.e.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2004),W07-1033,"In biomedical named-entity tagging, right boundaries are usually easier to detect, and it may be the reason of the superiority of the backward tagging.","We could have partially alleviated this effect by employing head-word triggers as done in Zhou et al (2004), but we decided to use backward tagging because the results of a number of preliminary experiments, including the ones shown in Table 2 above, seemed to be showing that the backward tagging is preferable in this task setting.",3.2 Feature set,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Collins, 2000)",W07-1033,"We divided the data into 20 contiguous and equally-sized sections, and used the first 18 sections for training, and the last 2 sections for testing while development (henceforth the training and development sets, respectively).","The training data of the reranker is created by the n-best tagger, and every set of 17 sections from the training set is used to train the n-best tagger for the remaining section (The same technique is used by previous studies to avoid the n-best tagger's 'unrealistically good' performance on the training set (Collins, 2000)).","Among the n-best sequences output by the MEMM tagger, the sequence with the highest F-score is used as the 'correct' sequence for training the reranker.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Chen and Rosenfeld, 1999)",W07-1033,The two log-linear models for the MEMM tagger and reranker are estimated using a limited-memory BFGS algorithm implemented in an open-source software Amis3.,"In both models, Gaussian prior distributions are used to avoid overfitting (Chen and Rosenfeld, 1999), and the standard deviations of the Gaussian distributions are optimized to maximize the performance on the development set.",We also used a thresholding technique which discards features with low frequency.,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Kim et al., 2004",W07-1033,Table 4: Performance of the reranker.,"of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006), features of shallow parsers had a large contribution to the performance.","The information of the previous labels was also quite effective, which indicates that label unigram models (i.e.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"Okanohara et al., 2006",W07-1033,Table 4: Performance of the reranker.,"of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006), features of shallow parsers had a large contribution to the performance.","The information of the previous labels was also quite effective, which indicates that label unigram models (i.e.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"Tzong-Han Tsai et al., 2006)",W07-1033,Table 4: Performance of the reranker.,"of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006), features of shallow parsers had a large contribution to the performance.","The information of the previous labels was also quite effective, which indicates that label unigram models (i.e.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2004),W07-1033,"As naturally expected, our system outperformed the systems that cannot accommodate truly global features (Note that one point of F-score improvement is valuable in this task, because inter-annotator agreement rate of human experts in bio-entity recognition is likely to be about 80%.","For example, Krauthammer et al (2004) report the inter-annotater agreement rate of 77.6% for the three way bio-entity classification task.)",and the performance can be said to be at the same level as the best systems.,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2006),W07-1033,Table 6: Comparison of the F-scores of rerankers trained and evaluated with various 0-best taggers.,"F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF",Table 7: Performance comparison on the test set.,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2004),W07-1033,Table 6: Comparison of the F-scores of rerankers trained and evaluated with various 0-best taggers.,"F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF",Table 7: Performance comparison on the test set.,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2006),W07-1033,Table 6: Comparison of the F-scores of rerankers trained and evaluated with various 0-best taggers.,"F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF",Table 7: Performance comparison on the test set.,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
(2006),W07-1033,Table 6: Comparison of the F-scores of rerankers trained and evaluated with various 0-best taggers.,"F-score Method This paper 71.10 MEMM 72.65 reranking Tsai et al (2006) 72.98 CRF, postprocessing Zhou et al (2004) 72.55 HMM, SVM, postprocessing, gazetteer Friedrich et al (2006) 71.5 CRF,gazetteer Okanohara et al (2006) 71.48 semi-CRF",Table 7: Performance comparison on the test set.,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Zhou and Su, 2004)",W07-1033,"Our system showed clear improvement over many of the machine-learning-based systems reported to date, and also proved comparable to the existing state-ofthe-art systems that use rule-based post-processing.","Our future plans include further sophistication of features, such as the use of external gazetteers which is reported to improve the F-score by 1.0 and 2.7 points in (Zhou and Su, 2004) and (Friedrich et al., 2006), respectively.","We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Friedrich et al., 2006)",W07-1033,"Our system showed clear improvement over many of the machine-learning-based systems reported to date, and also proved comparable to the existing state-ofthe-art systems that use rule-based post-processing.","Our future plans include further sophistication of features, such as the use of external gazetteers which is reported to improve the F-score by 1.0 and 2.7 points in (Zhou and Su, 2004) and (Friedrich et al., 2006), respectively.","We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase.",http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(McClosky et al., 2006)",W07-1033,"We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase.","We also plan to apply self-training of n-best tagger which successfully boosted the performance of one of the best existing English syntactic parser (McClosky et al., 2006).",,http://www.aclweb.org/anthology/W/W07/W07-1033.pdf
"(Gaizauskas et al., 2001",W03-1807,We use an English semantic tagger (USAS) developed at Lancaster University to identify multiword units which depict single semantic concepts.,"The Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) built in Sheffield was used to evaluate our approach.","In our evaluation, this approach extracted a total of 4,195 MWE candidates, of which, after manual checking, 3,792 were accepted as valid MWEs, producing a precision of 90.39% and an estimated recall of 39.38%.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
"Clough et al., 2002)",W03-1807,We use an English semantic tagger (USAS) developed at Lancaster University to identify multiword units which depict single semantic concepts.,"The Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) built in Sheffield was used to evaluate our approach.","In our evaluation, this approach extracted a total of 4,195 MWE candidates, of which, after manual checking, 3,792 were accepted as valid MWEs, producing a precision of 90.39% and an estimated recall of 39.38%.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
(2001b),W03-1807,A number of approaches have been suggested and tested to address this problem.,"However, efficient extraction of MWEs still remains an unsolved issue, to the extent that Sag et al (2001b) call it ""a pain in the neck of NLP"".","In this paper, we present our work in which we approach the issue of MWE extraction by using a semantic field annotator.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
"(Gaizauskas et al., 2001",W03-1807,multiword expressions.,"We have drawn from the Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) a collection of British newspaper reports on court stories to evaluate our approach.","Our experiment shows that it is efficient in identifying MWEs, in particular MWEs of low frequencies.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
"Clough et al., 2002)",W03-1807,multiword expressions.,"We have drawn from the Meter Corpus (Gaizauskas et al., 2001; Clough et al., 2002) a collection of British newspaper reports on court stories to evaluate our approach.","Our experiment shows that it is efficient in identifying MWEs, in particular MWEs of low frequencies.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
"a, 2001b",W03-1807,1997; Michiels and Dufour 1998; Merkel and Andersson 2000; Piao and McEnery 2001; Sag et al.,"2001a, 2001b; Biber et al.",2003).,http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
(1994),W03-1807,"In practice, most statistical approaches use linguistic filters to collect candidate MWEs.",Such approaches include Dagan and Church's (1994) Termight Tool.,"In this tool, they first collect candidate nominal terms with a POS syntactic pattern filter, then use concordances to identify frequently co-occurring multiword units.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
(1993),W03-1807,"In this tool, they first collect candidate nominal terms with a POS syntactic pattern filter, then use concordances to identify frequently co-occurring multiword units.","In his Xtract system, Smadja (1993) first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance, strength and spread, and then examined concordances of the bi-grams to find longer frequent multiword units.","Similarly, Merkel and Andersson (2000) compared frequency-based and entropy based algorithms, each of which was combined with a language filter.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
(2000),W03-1807,"In his Xtract system, Smadja (1993) first extracted significant pairs of words that consistently co-occur within a single syntactic structure using statistical scores called distance, strength and spread, and then examined concordances of the bi-grams to find longer frequent multiword units.","Similarly, Merkel and Andersson (2000) compared frequency-based and entropy based algorithms, each of which was combined with a language filter.",They reported that the entropy-based algorithm produced better results.,http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
(1997),W03-1807,Lexical resources and parsers are used to obtain better coverage of the lexicon in MWE extraction.,"For example, Wu (1997) used an English-Chinese bilingual parser based on stochastic transduction grammars to identify terms, including multiword expressions.","In their DEFI Project, Michiels and Dufour (1998) used dictionaries to identify English and French multiword expressions and their translations in the other language.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
(1998),W03-1807,"For example, Wu (1997) used an English-Chinese bilingual parser based on stochastic transduction grammars to identify terms, including multiword expressions.","In their DEFI Project, Michiels and Dufour (1998) used dictionaries to identify English and French multiword expressions and their translations in the other language.",Wehrli (1998) employed a generative grammar framework to identify compounds and idioms in their ITS-2 MT English-French system.,http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
(1998),W03-1807,"In their DEFI Project, Michiels and Dufour (1998) used dictionaries to identify English and French multiword expressions and their translations in the other language.",Wehrli (1998) employed a generative grammar framework to identify compounds and idioms in their ITS-2 MT English-French system.,Sag et al (2001b) introduced Head-driven Phrase Structure Grammar for analyzing MWEs.,http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
(2001b),W03-1807,Wehrli (1998) employed a generative grammar framework to identify compounds and idioms in their ITS-2 MT English-French system.,Sag et al (2001b) introduced Head-driven Phrase Structure Grammar for analyzing MWEs.,"Like pure statistical approaches, purely knowledgebased symbolic approaches also face problems.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
(2001b),W03-1807,They are language dependent and not flexible enough to cope with complex structures of MWEs.,"As Sag et al (2001b) suggest, it is important to find the right balance between symbolic and statistical approaches.","In this paper, we propose a new approach to MWEs extraction using semantic field information.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
"(Garside and Smith, 1997)",W03-1807,The USAS system has been in development at Lancaster University since 19901.,"Based on POS annotation provided by the CLAWS tagger (Garside and Smith, 1997), USAS assigns a set of semantic tags to each item in running text and then attempts to disambiguate the tags in order to choose the most likely candidate in each context.",Items can be single words or multiword expressions.,http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
"(McArthur, 1981)",W03-1807,The groups include not only synonyms and antonyms but also hypernyms and hyponyms.,"The initial tagset was loosely based on Tom McArthur's Longman Lexicon of Contemporary English (McArthur, 1981) as this appeared to offer the most appropriate thesaurus type classification of word senses for this kind of analysis.",The tagset has since been considerably revised in the light of practical tagging problems met in the course of the research.,http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
"(Garside and Rayson, 1997)",W03-1807,"As in the case of grammatical tagging, the task of semantic tagging subdivides broadly into two phases: Phase I (Tag assignment): attaching a set of potential semantic tags to each lexical unit and Phase II (Tag disambiguation): selecting the contextually appropriate semantic tag from the set provided by Phase I. USAS makes use of seven major techniques or sources of information in phase II.","We will list these only briefly here, since they are described in more detail elsewhere in a sentence, the following heuristics are ap(Garside and Rayson, 1997).",plied in sequence:,http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
"(by Gale et al, 1992)",W03-1807,Text-based disambiguation.,"It has been claimed (by Gale et al, 1992) on the basis of corpus analysis that to a very large extent a word keeps the same meaning throughout a text.",6.,http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
"Clough et al., 2002)",W03-1807,"Finally, we manually checked the results.","The Meter Corpus chosen as the test data is a collection of court reports from the British Press Association (PA) and some leading British newspapers (Gaizauskas 2001; Clough et al., 2002).","In our experiment, we used the newspaper part of the corpus containing 774 articles with more than 250,000 words.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
(1993),W03-1807,We noticed various possible definitions have been suggested for MWE/MWU.,"For example, Smadja (1993) suggests a basic characteristic of collocations and multiword units is recurrent, domain-dependent and cohesive lexical clusters.",Sag et el.,http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
(2001b),W03-1807,Sag et el.,"(2001b) suggest that MWEs can roughly be defined as ""idiosyncratic interpretations that cross word boundaries (or spaces)"".","Biber et al (2003) describe MWEs as lexical bundles, which they go on to define as combinations of words that can be repeated frequently and tend to be used frequently by many different speakers/writers within a register.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf
(2003),W03-1807,"(2001b) suggest that MWEs can roughly be defined as ""idiosyncratic interpretations that cross word boundaries (or spaces)"".","Biber et al (2003) describe MWEs as lexical bundles, which they go on to define as combinations of words that can be repeated frequently and tend to be used frequently by many different speakers/writers within a register.","Although it is not difficult to interpret these deifications in theory, things became much more complicated when we undertook our practical checking of the MWE candidates.",http://www.aclweb.org/anthology/W/W03/W03-1807.pdf

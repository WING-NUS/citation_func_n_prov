_unit_id,_golden,_unit_state,_trusted_judgments,_last_judgment_at,what_is_the_function_of_this_citation,what_is_the_function_of_this_citation:confidence,article,current,link,marker,next,previous,what_is_the_function_of_this_citation_gold
1157166537,false,finalized,3,2/12/2017 17:00:34,Pos,1.0,P96-1026,"Our analysis was carried out within the framework of Systemic-Functional Linguistics (sFL) (Halliday, 1978; Halliday, 1985) which views language as a resource for the creation of meaning.",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf,"(Halliday, 1978",SFL stratifies meaning into context and language.,3 Linguistic Framework: Systemic Functional Linguistics,
1157166538,false,finalized,3,2/12/2017 16:34:53,Neut,0.6575,P96-1026,"It certainly conforms to the principles of good documentation established by current research on technical documentation and on the needs of end-users, e.g., (Carroll, 1994; Hammond, 1994), in that it supplies clear and concise information for the task at hand.",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf,"(Carroll, 1994","Finally, we have been assured by French users of the software that they consider this particular manual to be well written and to bear no unnatural trace of its origins.","Furthermore, we know that Macintosh documentation undergoes thorough local quality control.",
1157166539,false,finalized,3,2/12/2017 16:38:45,Pos,0.6627,P96-1026,"We have looked to the construct of genre (Martin, 1992) to provide this additional control, on two grounds: (1) since genres are distinguished by their communicative purposes, we can view each of the functional sections already identified as a distinct genre; (2) genre is presented as controlling text structure and realisation.",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf,"(Martin, 1992)","In Martin's view, genre is defined as a staged, goal-oriented social process realised through register, the context of situation, which in turn is realised in language to achieve the goals of a text.","In such cases, we must appeal to another source of control over the apparently available choices.",
1157166540,false,finalized,3,2/12/2017 18:16:57,Neut,0.6505,P96-1026,"This is consistent with other accounts of text structure for text generation in technical domains, e.g., (McKeown, 1985; Paris, 1993; Kittredge et al., 1991).",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf,"(McKeown, 1985","For those cases where the realisation remains under-determined by the task element type, we conducted a finer-grained analysis, by overlaying a genre partition on the undifferentiated data.","It will also allow us to determine the text structures appropriate in each genre, a study we are currently undertaking.",
1157166541,false,finalized,3,2/12/2017 16:41:50,CoCo,0.6828,P96-1026,"The results from our linguistic analysis are consistent with other research on sublanguages in the instructions domain, in both French and English, e.g., (Kosseim and Lapalme, 1994; Paris and Scott, 1994).",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf,"(Kosseim and Lapalme, 1994",Our analysis goes beyond previous work by identifying within the discourse context the means for exercising explicit control over a text generator.,9 Related Work,
1157166542,false,finalized,4,2/12/2017 16:19:44,Pos,0.5063,P96-1026,"An interesting difference with respect to previous descriptions is the use of the true (or direct) imperative to express an action in the procedure genre, as results from (Paris and Scott, 1994) seem to indicate that the infinitive-form of the imperative is preferred in French.",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf,"(Paris and Scott, 1994)","These results, however, were obtained from a corpus of instructions mostly for domestic appliances as opposed to software manuals.",Our analysis goes beyond previous work by identifying within the discourse context the means for exercising explicit control over a text generator.,
1157166543,false,finalized,3,2/12/2017 16:37:33,Weak,0.6766,P96-1026,"Furthermore the use of the infinitive-form in instructions in general as observed by (Kocourek, 1982) is declining, as some of the conventions already common in English technical writing are being adopted by French technical writers, e.g., (Timbal-Duclaux, 1990).",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf,"(Kocourek, 1982)","We also note that the patterns of realisations uncovered in our analysis follow the principle of good technical writing practice known as the minimalist approach, e.g., (Carroll, 1994; Hammond, 1994).","These results, however, were obtained from a corpus of instructions mostly for domestic appliances as opposed to software manuals.",
1157166544,false,finalized,3,2/12/2017 16:40:00,CoCo,0.6809,P96-1026,"The methodology we employed is similar to that endorsed by (Biber, 1995).",http://www.aclweb.org/anthology/P/P96/P96-1026.pdf,"(Biber, 1995)",It is summarised as follows:,2 Methodology,
1157166545,false,finalized,4,2/12/2017 16:21:45,Neut,1.0,W06-2204,"TPLEX is a transductive algorithm (Vapnik, 1998), in that the goal is to perform extraction from a given unlabelled corpus, given a labelled corpus.",http://www.aclweb.org/anthology/W/W06/W06-2204.pdf,"(Vapnik, 1998)","This is in contrast to the typical machine learning framework, where the goal is a set of extraction patterns (which can of course then be applied to new unlabelled text).","Rather than relying on redundancy of the fragments, TPLEX exploits redundancy of the learned extraction patterns.",
1157166546,false,finalized,4,2/12/2017 16:19:44,Neut,0.5635,W06-2204,"Another possibility is to explore semi-supervised extensions to boosting (d'Alche Buc et al., 2002).",http://www.aclweb.org/anthology/W/W06/W06-2204.pdf,"Alche Buc et al., 2002)","Boosting is a highly effective ensemble learning technique, and BWI uses boosting to tune the weights of the learned patterns, so if we generalize boosting to handle unlabelled data, then the learned weights may well be more effective than those calculated by TPLEX.",It remains to be seen whether this approach would be effective for information extraction.,
1157166547,false,finalized,3,2/12/2017 16:38:27,CoCo,1.0,W06-2204,"TPLEX's boundary detectors are similar to those learned by BWI (Freitag and Kushmerick, 2000).",http://www.aclweb.org/anthology/W/W06/W06-2204.pdf,"(Freitag and Kushmerick, 2000)","A boundary detector has two parts, a left pattern and a right pattern.","This strategy has previously been employed successfully (Freitag and Kushmerick, 2000; Ciravegna, 2001; Yangarber et al., 2002; Finn and Kushmerick, 2004).",
1157166548,false,finalized,3,2/12/2017 16:43:25,Pos,1.0,W06-2204,"For the CRF results, we used MALLET's SimpleTagger (McCallum, 2002), with each token encoded with a set of binary features (one for each observed literal, as well as the eight token generalizations).",http://www.aclweb.org/anthology/W/W06/W06-2204.pdf,"(McCallum, 2002)",Our results in Fig.,The results for ELIE were generated by the current implementation [http://smi.ucd.ie/aidan/Software.html].,
1157166549,false,finalized,3,2/12/2017 16:41:48,Neut,1.0,W15-2134,Schwartz et al (2012) is a systematic study of how representation choices in dependency annotation schemes affect their learnability for parsing.,http://www.aclweb.org/anthology/W/W15/W15-2134.pdf,(2012),"The choice points investigated, much like in the current paper, relate to the issue of headedness.",2 Related work,
1157166550,false,finalized,3,2/12/2017 16:42:15,CoCo,0.6553,W15-2134,"In terms of LAS, MaltParser (Nivre et al., 2007) performs best of the 3 parsers with SD, and MSTParser (McDonald et al., 2006) performs best with CoNLL.",http://www.aclweb.org/anthology/W/W15/W15-2134.pdf,"(Nivre et al., 2007)","In Nilsson et al (2006), the authors investigate the effects of two types of input transformation on the performance of MaltParser.","For all parsers and in most experiments (which explore several pipelines with different POS-tagging strategies), SD is easier to label (i.e., label accuracy scores are higher) and CoNLL is easier to structure (i.e., unlabeled attachment scores are higher).",
1157166551,false,finalized,4,2/12/2017 16:29:28,Neut,0.7424,W15-2134,"The first version annotated with the UD representation was released in 2015 (Nivre et al., 2015)1.",http://www.aclweb.org/anthology/W/W15/W15-2134.pdf,"(Nivre et al., 2015)",1http://universaldependencies.github.io/docs/,The result is the first human-checked largescale gold standard for syntactic dependency annotation of English text.,
1157166552,false,finalized,5,2/12/2017 16:27:27,Weak,0.6027,W15-2134,This creates an opportunity for losses that did not exist in the experiments of Schwartz et al (2012).,http://www.aclweb.org/anthology/W/W15/W15-2134.pdf,(2012),7 Conclusion,"Here, since the main concern is the design of a parsing representation that is meant simply as an intermediary step, all output has to be evaluated against the same gold standard.",
1157166553,false,finalized,4,2/12/2017 16:21:45,Pos,0.5063,S15-2109,"Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons.",http://www.aclweb.org/anthology/S/S15/S15-2109.pdf,"(Zhu et al., 2014)","In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015).","However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning.",
1157166554,false,finalized,3,2/12/2017 16:41:49,CoCo,0.6671,S15-2109,"Training employed conventional Stochastic Gradient Descent (Rumelhart et al., 1985) with mini-batch size 1 and random uniform initialization similar to (Glorot and Bengio, 2010).",http://www.aclweb.org/anthology/S/S15/S15-2109.pdf,"(Rumelhart et al., 1985)","After some initial experiments, it was determined that a learning rate of 0.01 and selecting the model with the best accuracy on the 20% set after 8 iterations led to the best results.","The parameters of subspace model in Equation 2, S and C were estimated to minimize the negative log-likelihood of the correct class.",
1157166555,false,finalized,4,2/12/2017 16:13:59,Pos,1.0,S15-2109,"We used the corpus of 52 million tweets used in (Owoputi et al., 2013) with the tokenizer described in the same work.",http://www.aclweb.org/anthology/S/S15/S15-2109.pdf,"(Owoputi et al., 2013)",The words that occurred less than 40 times in the data were discarded from the vocabulary.,"The embedding matrix E was trained in unsupervised fashion using the structured skip-gram model, described in Section 2.",
1157166556,false,finalized,3,2/12/2017 16:33:33,Pos,1.0,S15-2109,"To train the model, we used a negative sampling rate of 25 words, sampled from a multinomial of unigram word probabilities over all the vocabulary (Goldberg and Levy, 2014).",http://www.aclweb.org/anthology/S/S15/S15-2109.pdf,"(Goldberg and Levy, 2014)","Embeddings of 50, 200, 400 and 600 dimensions were trained.",The words that occurred less than 40 times in the data were discarded from the vocabulary.,
1157166557,false,finalized,4,2/12/2017 16:29:28,Neut,1.0,E09-1091,"In Cross-Language Information Retrieval (CLIR) systems, they play an even more important role as the accuracy of their transliterations is shown to correlate highly with the performance of the CLIR systems (Mandl and Womser-Hacker, 2005, Xu and Weischedel, 2005).",http://www.aclweb.org/anthology/E/E09/E09-1091.pdf,"(Mandl and Womser-Hacker, 2005, ",Traditional methods for transliterations have not proven to be very effective in CLIR.,Named Entities (NEs) play a critical role in many Natural Language Processing and Information Retrieval (IR) tasks.,
1157166558,false,finalized,3,2/12/2017 16:41:50,CoCo,0.6647,E09-1091,"We show that MINT's performance is significantly better than a state of the art method (Klementiev and Roth, 2006).",http://www.aclweb.org/anthology/E/E09/E09-1091.pdf,"(Klementiev and Roth, 2006)","Proceedings of the 12th Conference of the European Chapter of the ACL, pages 799-807, Athens, Greece, 30 March - 3 April 2009. c2009 Association for Computational Linguistics","We demonstrate MINT's effectiveness on 4 language pairs involving 5 languages (English, Hindi, Kannada, Russian, and Tamil) from 3 different language families, and its scalability on corpora of vastly different sizes (2,000 to 200,000 articles).",
1157166559,false,finalized,3,2/12/2017 16:41:48,Neut,1.0,E09-1091,"News stories are typically rich in NEs and therefore, comparable news corpora can be expected to contain NETEs (Klementiev and Roth, 2006; Tao et al., 2006).",http://www.aclweb.org/anthology/E/E09/E09-1091.pdf,"(Klementiev and Roth, 2006","The large quantity and the perpetual availability of news corpora in many of the world's languages, make mining of NETEs a viable alternative to traditional approaches.","The ubiquitous availability of comparable news corpora in multiple languages suggests a promising alternative to Machine Transliteration, namely, the mining of Named Entity Transliteration Equivalents (NETEs) from such corpora.",
1157166560,false,finalized,5,2/12/2017 16:27:27,CoCo,0.6027,E09-1091,"In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996).",http://www.aclweb.org/anthology/E/E09/E09-1091.pdf,"(Pirkola et al., 2003)","While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs.","The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman & Oard, 2002; Mandl & Womser-hacker, 2005; Xu &Weischedel, 2005).",
1157166561,false,finalized,3,2/12/2017 16:43:36,Neut,0.6607,E09-1091,"NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006), and extended for NETE mining for several languages in (Saravanan and Kumaran, 2007).",http://www.aclweb.org/anthology/E/E09/E09-1091.pdf,"(Klementiev and Roth, 2006)","However, such methods miss vast majority of the NETEs due to their dependency on frequency signatures.","In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences.",
1157166562,false,finalized,3,2/12/2017 16:33:33,Weak,0.6657,E09-1091,"NETE mining from comparable corpora using phonetic mappings was proposed in (Tao et al., 2006), but the need for language specific knowledge restricts its applicability across languages.",http://www.aclweb.org/anthology/E/E09/E09-1091.pdf,"(Tao et al., 2006)","We proposed the idea of mining NETEs from multilingual articles with similar content in (Udupa, et al., 2008).","In addition, (Klementiev and Roth, 2006) may not scale for large corpora, as they examine every word in the target side as a potential transliteration equivalent.",
1157166563,false,finalized,6,2/12/2017 16:27:17,Neut,0.5485,N12-1038,"Georgescul et al (2009) note that while WindowDiff technically penalizes false positives and false negatives equally, false positives are in fact more likely; a false positive error occurs anywhere were there are more computed boundaries than boundaries in the reference, while a false negative error can only occur when a boundary is missed.",http://www.aclweb.org/anthology/N/N12/N12-1038.pdf,(2009),"Consider figure 1, only 3 of the 8 windows contain a boundary; only those 3 windows may have false negatives (a missed boundary), while all other windows may contain false positives (too many boundaries).","Tasks requiring a uniform theme in a segment might tolerate false positives, while tasks requiring complete ideas or complete themes might accept false negatives.",
1157166564,false,finalized,3,2/12/2017 16:42:15,Pos,0.6657,P06-1009,"We use a conditional random field (CRF) sequence model, which allows for globally optimal training and decoding (Lafferty et al., 2001).",http://www.aclweb.org/anthology/P/P06/P06-1009.pdf,"(Lafferty et al., 2001)",The inference algo,This paper presents an alternative discriminative method for word alignment.,
1157166565,false,finalized,3,2/12/2017 16:33:33,Pos,1.0,P06-1009,"We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003).",http://www.aclweb.org/anthology/P/P06/P06-1009.pdf,"(Malouf, 2002",Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters.,"While the log-likelihood cannot be maximised for the parameters, A, in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters.",
1157166566,false,finalized,3,2/12/2017 16:38:27,CoCo,0.6782,P06-1009,"We use a similar graphical structure to the directed hidden Markov model (HMM) from GIZA++ (Och and Ney, 2003).",http://www.aclweb.org/anthology/P/P06/P06-1009.pdf,"(Och and Ney, 2003)","This models one-to-many alignments, where each target word is aligned with zero or more source words.","Furthermore, the model allows regularisation using a prior over the parameters, a very effective and simple method for limiting over-fitting.",
1157166567,false,finalized,5,2/12/2017 16:29:53,Pos,0.8315,P06-1009,We use the refined method from Och and Ney (2003) which starts from the intersection of the two models' predictions and 'grows' the predicted alignments to neighbouring alignments which only appear in the output of one of the models.,http://www.aclweb.org/anthology/P/P06/P06-1009.pdf,(2003),4 Experiments,"In order to produce many-to-many alignments we combine the outputs of two models, one for each translation direction.",
1157166568,false,finalized,3,2/12/2017 16:41:50,Pos,1.0,P06-1009,"For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003).",http://www.aclweb.org/anthology/P/P06/P06-1009.pdf,"(Mihalcea and Pedersen, 2003)","For this task we have used the same test data as the competition entrants, and therefore can directly compare our results.","We used the original 37 sentence trial set for feature used these as our training and test sets, respectively.",
1157166569,false,finalized,3,2/12/2017 16:41:43,Pos,1.0,P06-1009,We follow Taskar et al (2005) by using the first 100 test sentences for training and the remaining 347 for testing.,http://www.aclweb.org/anthology/P/P06/P06-1009.pdf,(2005),"This means that our results should not be directly compared to those entrants, other than in an approximate manner.","Unlike the unsupervised entrants in the 2003 task, we require word-aligned training data, and therefore must cannibalise the test set for this purpose.",
1157166570,false,finalized,3,2/12/2017 16:29:17,Neut,0.7082,J98-3009,"After reviewing those that were employed by the Automatic Language Processing Advisory Committee (1966) and those that were designed in the early 1990s on behalf of ARPA, a major U.S. funding agency, the author concludes that translation quality cannot be defined in the abstract, mainly because there is no such thing as ""the correct translation,"" and pleads for an evaluation that explicitly takes into account the specific purpose for which a translation has been made.",http://www.aclweb.org/anthology/J/J98/J98-3009.pdf,(1966),"Now that I have gone through the individual papers, the obvious question is how much they contribute to the overall goal of the book, i.e., to help build a bridge between MT and translation theory.","251-263) is actually an exercise in metaevaluation, for the object of her evaluation is not any (group of) MT systems, but rather the criteria in terms of which MT systems are commonly evaluated.",
1157166571,false,finalized,3,2/12/2017 16:40:00,Neut,1.0,P15-2128,"Banea et al (2008) follow this approach, translating an annotated corpus via MT.",http://www.aclweb.org/anthology/P/P15/P15-2128.pdf,(2008),Balamurali et al (2012) use linked Wordnets to,The corpus transfer approach consists of transferring a source training corpus into the target language and building a corpus-based classifier in the target language.,
1157166572,false,finalized,3,2/12/2017 17:00:34,Pos,1.0,P15-2128,"We performed the experiments with the weka toolkit (Hall et al., 2009), using a filter to convert strings into word vectors, and two learning algorithms: SVMs and bagging with Fast Decision Tree Learner as base algorithm.",http://www.aclweb.org/anthology/P/P15/P15-2128.pdf,"(Hall et al., 2009)",Figure 4 represents the experiments conducted with the EN test set.,bels.,
1157166573,false,finalized,3,2/12/2017 17:52:36,Pos,0.6346,P15-2128,"However, Balahur and Turchi (2014) conclude that MT systems can be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English.",http://www.aclweb.org/anthology/P/P15/P15-2128.pdf,(2014),All this work was performed at sentence or document level.,"In language pairs in which no high-quality MT systems are available, MT may not be an appropriate transfer method (Popat et al., 2013; Balamurali et al., 2012).",
1157166574,false,finalized,3,2/12/2017 16:35:14,Pos,0.6987,P15-2128,"We use the Moses statistical MT (SMT) toolkit (Koehn et al., 2007) to perform the translation.",http://www.aclweb.org/anthology/P/P15/P15-2128.pdf,"(Koehn et al., 2007)","In Moses, these reordering constraints are implemented with the zone and wall tags, as indicated in Figure 3.","That is, the text between the relevant segment boundaries is not reordered nor mixed with the text outside these boundaries.3 Thus the text in the target language segment comes only from the corresponding source language segment.",
1157166575,false,finalized,4,2/12/2017 16:29:28,Pos,1.0,P15-2128,"We used Moses Experiment Management System (Koehn, 2010) with all default options to build the SMT system.7 Because the common crawl corpus contained English sentences in the Spanish side, we applied an LM-based filter to select only sentence pairs in which the Spanish side was better scored by the Spanish LM than with the English LM, and conversely for the English side.",http://www.aclweb.org/anthology/P/P15/P15-2128.pdf,"(Koehn, 2010)",We conducted supervised sentiment classification experiments for settings a and b of use case I (see Section 2).,The LM was an interpolation of LMs trained with the target part of the parallel corpora and with the rest of the Booking and Trip Advisor data (last 2 rows of Table 2).,
1157166576,false,finalized,5,2/12/2017 16:29:53,Neut,0.6047,W97-0318,"The ability to distinguish states, e.g., ""Mark seems happy,"" from events, e.g., ""Renee ran down the street,"" is a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse (Moens and Steedman, 1988; Dorr, 1992; Klavans, 1994).",http://www.aclweb.org/anthology/W/W97/W97-0318.pdf,"(Moens and Steedman, 1988","Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause.",1 Introduction,
1157166577,false,finalized,3,2/12/2017 18:15:58,Neut,0.6914,W97-0318,"Aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation (Moens and Steedman, 1988; Passonneau, 1988; Dorr, 1992; Klavans, 1994).",http://www.aclweb.org/anthology/W/W97/W97-0318.pdf,"(Moens and Steedman, 1988","Researchers have used empirical analysis of corpora to develop linguistically-based numerical indicators that aid in aspectual classification (Klavans and Chodorow, 1992; Siegel and McKeown, 1996).","Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause.",
1157166578,false,finalized,3,2/12/2017 16:29:17,Neut,1.0,W97-0318,"and for the automatic identification of semantically related groups of words (Pereira, Tishby, and Lee, 1993; Hatzivassiloglou and McKeown, 1993).",http://www.aclweb.org/anthology/W/W97/W97-0318.pdf,"(Pereira, Tishby, and Lee, 1993","For more detail on the machine learning experiments described here, see Siegel (1997).",Table 4: Breakdown of verb occurrences.,
1157166579,false,finalized,3,2/12/2017 16:35:14,Neut,0.6987,W97-0318,"3Similar baselines for comparison have been used for many classification problems (Duda and Hart, 1973), e.g., part-of-speech tagging (Church, 1988; Allen, 1995).",http://www.aclweb.org/anthology/W/W97/W97-0318.pdf,"(Duda and Hart, 1973)",159,2This test was suggested by Judith Klavans (personal communication).,
1157166580,false,finalized,3,2/12/2017 16:38:27,Neut,0.6782,P11-2022,"A well-written document is coherent (Halliday and Hasan, 1976)- it structures information so that each new piece of information is interpretable given the preceding context.",http://www.aclweb.org/anthology/P/P11/P11-2022.pdf,"(Halliday and Hasan, 1976)","Models that distinguish coherent from incoherent documents are widely used in generation, summarization and text evaluation.",1 Introduction,
1157166581,false,finalized,3,2/12/2017 16:38:27,Pos,0.6611,P11-2022,"In addition to its first proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scoring (Burstein et al., 2010).",http://www.aclweb.org/anthology/P/P11/P11-2022.pdf,"(McIntyre and Lapata, 2010)","It also remains a critical component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2008), which typically combine it with other independently-trained models.","Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks.",
1157166582,false,finalized,3,2/12/2017 18:14:57,Weak,1.0,P11-2022,"Although some versions of the grid use an automatic coreference resolver, this often fails to improve results; in Barzilay and Lapata (2005), coreference improves results in only one of their target domains, and actually hurts for readability prediction.",http://www.aclweb.org/anthology/P/P11/P11-2022.pdf,(2005),"Their results, moreover, rely on running coreference on the document in its original order; in a summarization task, the correct order is not known, which will cause even more resolver errors.",We follow the -COREFERENCE setting from Barzilay and Lapata (2005) and perform heuristic coreference resolution by linking mentions which share a head noun.,
1157166583,false,finalized,4,2/12/2017 16:13:59,CoCo,0.4629,P11-2022,"In document discrimination (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences, scoring the system correct if it prefers the original ordering4.",http://www.aclweb.org/anthology/P/P11/P11-2022.pdf,"(Barzilay and Lapata, 2005)","We also evaluate on the more difficult task of sentence insertion (Chen et al., 2007; Elsner and Charniak, 2008).","We test our model on two experimental tasks, both testing its ability to distinguish between correct and incorrect orderings for wSi articles.",
1157166584,false,finalized,3,2/12/2017 18:13:38,Neut,1.0,P11-2022,"Finding these is also necessary to maximize coreference recall (Elsner and Charniak, 2010).",http://www.aclweb.org/anthology/P/P11/P11-2022.pdf,"(Elsner and Charniak, 2010)",We give nonhead mentions the role X.,"This enables the model to pick up premodifiers in phrases like ""a Bush spokesman"", which do not head NPs in the Penn Treebank.",
1157166585,false,finalized,5,2/12/2017 16:29:53,Neut,0.5589,P11-2022,"7We train the regressor using OWLQN (Andrew and Gao, 2007), modified and distributed by Mark Johnson as part of the Charniak-Johnson parse reranker (Charniak and Johnson, 2005).",http://www.aclweb.org/anthology/P/P11/P11-2022.pdf,"(Andrew and Gao, 2007)",127,The standard entity grid estimates that such an entity will be the subject of the next sentence with a probability of about,
1157166586,false,finalized,3,2/12/2017 16:42:15,Pos,0.6852,P11-2022,"The best WSJ results in previous work are those of Elsner and Charniak (2008), who combine the entity grid with models based on pronoun coreference and discourse-new NP detection.",http://www.aclweb.org/anthology/P/P11/P11-2022.pdf,(2008),We report their scores in the table.,This model is significantly better than the standard grid on discrimination (84% versus 80%) and has a higher mean score on insertion (24% versus 21%)8.,
1157166587,false,finalized,3,2/12/2017 17:52:36,Pos,1.0,P11-2022,We follow the -COREFERENCE setting from Barzilay and Lapata (2005) and perform heuristic coreference resolution by linking mentions which share a head noun.,http://www.aclweb.org/anthology/P/P11/P11-2022.pdf,(2005),"Although some versions of the grid use an automatic coreference resolver, this often fails to improve results; in Barzilay and Lapata (2005), coreference improves results in only one of their target domains, and actually hurts for readability prediction.","To construct a grid, we must first decide which textual units are to be considered ""entities"", and how the different mentions of an entity are to be linked.",
1157166588,false,finalized,3,2/12/2017 17:00:34,Pos,0.6416,J96-1003,There has been substantial work on spelling correction (see the excellent review by Kukich [1992]).,http://www.aclweb.org/anthology/J/J96/J96-1003.pdf,[1992],"All methods essentially enumerate plausible candidates that resemble the incorrect word, and use additional heuristics to rank the results.'",Spelling correction is an important application for error-tolerant recognition.,
1157166589,false,finalized,4,2/12/2017 16:21:45,Neut,1.0,J96-1003,"Kukich (1992), citing a number of studies, reports that typically 80% of misspelled words contain a single error of one of the unit operations, although",http://www.aclweb.org/anthology/J/J96/J96-1003.pdf,(1992),84,"For edit distance thresholds 1, 2, and 3, we selected 1,000 words at random from each word list and perturbed them by random insertions, deletions, replacements, and transpositions, so that each misspelled word had the required edit distance from the correct form.",
1157166590,false,finalized,3,2/12/2017 16:41:48,Pos,0.6956,D09-1160,"Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995).",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf,"(Cortes and Vapnik, 1995)","The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008).","However, 'explicit' feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier.",
1157166591,false,finalized,3,2/12/2017 17:52:36,Pos,0.6535,D09-1160,"However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training E1-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification.",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf,(2005),"In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy.","E1-regularized log-linear models (E1-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007).",
1157166592,false,finalized,4,2/12/2017 16:19:44,Neut,0.5635,D09-1160,"Heuristic Kernel Expansion (SVM-HKE) To make the weight vector sparse, Kudo and Matsumoto (2003) proposed a heuristic method that filters out less useful features whose absolute weight values are less than a pre-defined threshold .2 They reported that increased threshold value  resulted in a dramatically sparse feature space Fd, which had the side-effects of accuracy degradation and classifier speed-up.",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf,(2003),3 Proposed Method,"1) is O(|xd|), which is linear with respect to the number of active features in xd within the expanded feature space Fd.",
1157166593,false,finalized,5,2/12/2017 16:27:27,CoCo,0.6187,D09-1160,"This result conforms to the results reported in (Kudo and Matsumoto, 2003).",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf,"(Kudo and Matsumoto, 2003)","The parsing speed reached 14,937 sentences per second with accuracy of 90.91% (SVM-HKE, d = 3, Q = 0.002).","Without fstries, little speed-up of SVM-HKE classifiers versus the SVM-KE classifiers (in Table 3) was obtained due to the mild reduction in the average number of active features |xd |in the classification.",
1157166594,false,finalized,3,2/12/2017 16:34:53,Pos,0.3531,D09-1160,"When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage.",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf,"(Ganchev and Dredze, 2008)","We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al (2009)).","To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs.",
1157166595,false,finalized,3,2/12/2017 16:37:33,Pos,1.0,D09-1160,"We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1).",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf,"(Kudo and Matsumoto, 2002","Note that features listed in the 'Between bunsetsus' row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector.","Due to space limitations, we omit the details of the parsing algorithm.",
1157166596,false,finalized,6,2/12/2017 16:27:17,Pos,0.6769,D09-1160,"For evaluation, we used Kyoto Text Corpus Version 4.0 (Kurohashi and Nagao, 2003), Mainichi news articles in 1995 that have been manually annotated with dependency relations.6 The training, development, and test sets included 24,283, 4833, and 9284 sentences, and 234,685, 47,571, and 89,874 bunsetsus, respectively.",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf,"(Kurohashi and Nagao, 2003)","The training samples generated from the training set included 150,064 positive and 146,712 negative samples.",This task is therefore a better measure of our method than simple sequential labeling such as POS tagging or named-entity recognition.,
1157166597,false,finalized,4,2/12/2017 16:19:44,Neut,0.7181,D13-1114,"A well-known study evaluated usage of Twitter in the aftermath of the 2010 earthquake in Japan (Sakaki et al., 2010).",http://www.aclweb.org/anthology/D/D13/D13-1114.pdf,"(Sakaki et al., 2010)","Another Japanese-oriented study evaluated the impact of television on tweeted content (Akioka et al., 2010).",More work has been done in the latter category: analysis of social phenomena in a non-English context.,
1157166598,false,finalized,5,2/12/2017 16:27:27,Neut,0.7853,D13-1114,"The use of name-gender associations are problematic when non-English content is considered because databases of anglophone name-gender associations are no longer useful (Mislove et al., 2011).",http://www.aclweb.org/anthology/D/D13/D13-1114.pdf,"(Mislove et al., 2011)","We instead used Amazon Mechanical Turk workers to identify the gender of the person shown in the profile picture associated with a user's account (Liu and Ruths, 2013).","(Rao et al., 2010; Pennacchiotti and Popescu, 2011; Zamal et al., 2012), the dominant way of obtaining datasets consisting of Twitter users with highconfidence gender-labels is to use gender-name associations.",
1157166599,false,finalized,3,2/12/2017 16:43:25,Pos,1.0,D13-1114,"For the present study, we adopted an SVM-based classifier, described in (Zamal et al., 2012), that incorporated nearly all features used in prior work and showed comparable (and sometimes better) accuracy than other methods.",http://www.aclweb.org/anthology/D/D13/D13-1114.pdf,"(Zamal et al., 2012)",Parameter values and kernel choices for the SVM are discussed in the source paper.,"We followed prior work in this regard, particularly since our intent here is to evaluate the relevance of existing gender inference machinery on other languages.",
1157166600,false,finalized,3,2/12/2017 16:29:17,Pos,0.6586,D13-1114,"We used Amazon Mechanical Turk to manually label each user with their gender, using a language-agnostic labeling strategy (Liu and Ruths, 2013).",http://www.aclweb.org/anthology/D/D13/D13-1114.pdf,"(Liu and Ruths, 2013)","For classification, we employed a performant support vector machine-based (SVM) technique that has been used in a range of studies, e.g.",Each dataset consisted of approximately 1000 users who tweeted primarily in a given language.,
1157166601,false,finalized,3,2/12/2017 16:38:13,Neut,1.0,W15-2911,"Negation and its scope has been studied extensively (Moilanen and Pulman, 2008; Pang and Lee, 2004; Choi and Cardie, 2009).",http://www.aclweb.org/anthology/W/W15/W15-2911.pdf,"(Moilanen and Pulman, 2008","Polar words can even carry an opposite sentiment in a new domain (Blitzer et al., 2007; Andreevskaia and Bergler, 2006; Schwartz et al., 2013; Wilson et al., 2005).",Dragut et al (2012) examined inconsistency across lexicons.,
1157166602,false,finalized,4,2/12/2017 16:21:45,Neut,0.7181,W15-2911,"Research by Kennedy and Inkpen (2006) dealt with negation and intensity by creating a discrete modifier scale, namely, the occurrence of good might be either good, not good, intensified good, or diminished good.",http://www.aclweb.org/anthology/W/W15/W15-2911.pdf,(2006),A similar approach was taken by Steinberger et al (2012).,"Taboada et al (2011) presented a polarity lexicon with negation words and intensifiers, which they refer to as contextual valence shifters (Polanyi and Zaenen, 2006).",
1157166603,false,finalized,3,2/12/2017 16:25:57,Pos,1.0,W15-2911,"We combine the following freely available data, leading to a large corpus of positive and negative tweets: - 1.6 million automatically labeled tweets from the Sentiment140 data set (Go et al., 2009), collected by searching for positive and negative emoticons;",http://www.aclweb.org/anthology/W/W15/W15-2911.pdf,"(Go et al., 2009)",1An online demo illustrating the score values and distributional term similarities in this Twitter space can be found at the LT website http://maggie.lt.informatik.,"We compute the LMI over a corpus of positive, respectively negative tweets, in order to obtain positive (LMIpos) and negative (LMIneg) bigram scores.",
1157166604,false,finalized,5,2/12/2017 16:29:53,Weak,0.7761,W02-1024,"Nevertheless, search performance remains unsatisfactory at most e-commerce sites (Hagen et al., 2000).",http://www.aclweb.org/anthology/W/W02/W02-1024.pdf,"(Hagen et al., 2000)","Librarians and search professionals have traditionally favored Boolean keyword search systems, which, when successful, return a small set of relevant hits.",Keyword-based search engines have been one of the most highly utilized internet tools in recent years.,
1157166605,false,finalized,3,2/12/2017 18:13:35,CoCo,1.0,W02-1024,"Similar to our approach, Electric Knowledge transforms a natural language question into a series of increasingly more general keyword queries (Bierner, 2001).",http://www.aclweb.org/anthology/W/W02/W02-1024.pdf,"(Bierner, 2001)","However, their query formulation process utilizes hard-crafted regular expressions, while we adopt a more general machine learning approach for transformation rule application.","User questions are compared against those in the database, and links to webpages for the closest matches are returned.",
1157166606,false,finalized,3,2/12/2017 16:25:57,Pos,1.0,W02-1024,"We adopted the Q learning paradigm (Watkins, 1989; Mitchell, 1997) to model our problem as a set of possible states, S, and a set of actions, A, which can be performed to alter the current state.",http://www.aclweb.org/anthology/W/W02/W02-1024.pdf,"(Watkins, 1989","While in state s E S and performing action a E A, the learner receives a reward r(s, a), and advances to state s0 = (s, a).",4.1 Q Learning,
1157166607,false,finalized,3,2/12/2017 16:25:57,Neut,1.0,W14-2715,"Quotes provide additional context that were used by human annotators in a separate task for annotating agreement and disagreement (Misra and Walker, 2013).",http://www.aclweb.org/anthology/W/W14/W14-2715.pdf,"(Misra and Walker, 2013)",Responses can be labeled as either PRO or CON toward the topic.,The annotations for stance were gathered using Amazon's Mechanical Turk service with an interface that allowed annotators to see complete discussions.,
1157166608,false,finalized,3,2/12/2017 16:37:33,Pos,1.0,W14-2715,"We implement our approach using Probabilistic Soft Logic (PSL) (Bach et al., 2013), a recently introduced tool for collective inference in relational data.",http://www.aclweb.org/anthology/W/W14/W14-2715.pdf,"(Bach et al., 2013)","We evaluate our model on data from the 4FORUMS online debate site (Walker et al., 2012b).","Here, we develop a collective classification approach for stance prediction which leverages the sentiment conveyed in a post through its language, and the reply links consisting of agreements or rebuttals between posts in a discussion.",
1157166609,false,finalized,3,2/12/2017 16:43:25,Error,0.7172,W14-2715,"But unlike other statistical relational learning methods, PSL relaxes boolean truth values for atoms in the domain to soft truth values in the interval [0,1].",http://www.aclweb.org/anthology/W/W14/W14-2715.pdf,[0],"In this setting, finding the mostprobable explanation (MPE), a joint assignment of truth values to all random variable ground atoms, can be done efficiently.","Like other statistical relational learning methods, dependencies in the domain are captured by constructing rules with weights that can be learned from data.",
1157166610,false,finalized,3,2/12/2017 16:42:15,Error,0.6852,W14-2715,"We discard observations with annotations in the interval [0,1] because it indicates a very weak signal of agreement, which is already rare on debate sites.",http://www.aclweb.org/anthology/W/W14/W14-2715.pdf,[0],We populate disagreesPost and agreesPost in the same way as described above.,"For disagreement/agreement annotations in the interval [-5, 5], we consider values [-5,0] as evidence for the disagreesAuth relation and values [1, 5] as evidence for the agreesAuth relation.",
1157166611,false,finalized,3,2/12/2017 16:35:14,CoCo,0.6987,W14-2715,"Work by (Somasundaran and Wiebe, 2009) on idealogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics.",http://www.aclweb.org/anthology/W/W14/W14-2715.pdf,"(Somasundaran and Wiebe, 2009)","Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker's stance in a corpus of congressional floor debates.",We predict that this difference makes it more difficult to achieve accuracies as high for 4FORUMS discussions as can be achieved for the congressional debates corpus.,
1157166612,false,finalized,3,2/12/2017 16:35:14,Pos,0.6987,W14-2715,"We use probabilistic soft logic (PSL) (Bach et al., 2013) to model post stance by leveraging both these local linguistic features as well as the observed network structure of the posts to reason over the dataset.",http://www.aclweb.org/anthology/W/W14/W14-2715.pdf,"(Bach et al., 2013)","We evaluate our approach on 4FORUMS (Walker et al., 2012b), a collection of discussions from an online debate site on issues ranging from gun control to gay marriage.",We identify both linguistic features of the posts and features that capture the underlying relationships between posts and users.,
1157166613,false,finalized,3,2/12/2017 18:13:38,CoCo,0.7142,W04-0839,"(Mohammad and Pedersen, 2004) show results to the same conclusions for SENSEVAL-1 and SENSEVAL-2 data that have similar low training data per task, while, the line, hard, serve and interest data which have much larger training data per task are shown to benefit from a larger context.",http://www.aclweb.org/anthology/W/W04/W04-0839.pdf,"(Mohammad and Pedersen, 2004)",Duluth-ELSS (a sister system of SyntaLex) achieves an accuracy of 61.7%.,"We believe this is due to the low training data per task ratio, which usually means that the weak indicators (P and P ) are likely to be overwhelmed by idiosyncrasies of the data.",
1157166614,false,finalized,3,2/12/2017 16:34:53,Pos,1.0,P06-2052,"Collins et al (Collins, 2001a; Collins, 2001b) proposed an efficient method to calculate Tree Kernel by using C(n1, n2) as follows.",http://www.aclweb.org/anthology/P/P06/P06-2052.pdf,"(Collins, 2001a"," If the productions at n1 and n2 are different C(n1, n2) = 0  If the productions at n1 and n2 are the same, and n1 and n2 are pre-terminals, then C(n1, n2) = 1  Else if the productions at n1 and n2 are the same and n1 and n2 are not pre-terminals,",2.2 Algorithm to calculate similarity,
1157166615,false,finalized,4,2/12/2017 16:13:59,Neut,0.8013,S14-2080,"Transition-based models are widely used for dependency tree parsing, and they can be adapted to graph parsing (Sagae and Tsujii, 2008; Titov et al., 2009).",http://www.aclweb.org/anthology/S/S14/S14-2080.pdf,"(Sagae and Tsujii, 2008","Here we implement 5 transitionbased models for dependency graph parsing, each of which is based on different transition system.","We explore two kinds of basic models: One is transition-based, and the other is tree approximation.",
1157166616,false,finalized,3,2/12/2017 16:43:25,Neut,0.7172,S14-2080,"Reversing the order the words of a given sentence is a simple way to yield heterogeneous parsing models, thus improving parsing accuracy of the model ensemble (Sagae, 2007).",http://www.aclweb.org/anthology/S/S14/S14-2080.pdf,"(Sagae, 2007)","In our experiments, one transition system produces two models, one trained on the normal corpus, and the other on the corpus of reversed sentences.",3.3 Sentence Reversal,
1157166617,false,finalized,3,2/12/2017 16:38:13,Pos,1.0,S14-2080,"Inspired by the PCFG approximation idea (Fowler and Penn, 2010; Zhang and Krieger, 2011) for deep parsing, we study tree approximation approaches for graph spanning.",http://www.aclweb.org/anthology/S/S14/S14-2080.pdf,"(Fowler and Penn, 2010",This tree approximation technique can be applied to both transition-based and graph-based parsers.,"In our work, we use a practical idea to indirectly profile the graph-based parsing techniques for dependency graph parsing.",
1157166618,false,finalized,4,2/12/2017 16:13:59,Error,0.7267,S14-2080,noun ARG1 verb ARG1 verb ARG2 Mrs Ward was relieved root noun ARG1-R root verb ARG2 verb ARG1 adj ARG1;[2]verb ARG1 verb ARG2 Mrs Ward was relieved root noun ARG1-R,http://www.aclweb.org/anthology/S/S14/S14-2080.pdf,[2],462,"We have 19 heterogeneous basic models (10 transition-based models, 9 tree approximation models), and use a simple voter to combine their outputs.",
1157166619,false,finalized,3,2/12/2017 16:29:17,Neut,0.6586,J88-3004,"Initial work in recognizing and indexing abstract configurations of planning relations is discussed in Dolan and Dyer (1985, 1986).",http://www.aclweb.org/anthology/J/J88/J88-3004.pdf,"(1985, 1986)",9.3 OTHER CLASSES OF MISCONCEPTIONS,We have left the automatic creation of this taxonomy from advisor experiences to future research.,
1157166620,false,finalized,3,2/12/2017 16:41:49,CoCo,0.6557,Q14-1008,"Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and ""algebraic"" (as opposed to ""statistical"") approaches (Christiansen et al., 1998; Yang, 2004; Lignos and Yang, 2010; Lignos, 2011; Lignos, 2012).",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,"(Christiansen et al., 1998","Bayesian models of word segmentation (Brent, 1999; Goldwater, 2007), however, have until recently completely ignored stress.","tion of its effectiveness in adult speech processing (Cutler et al., 1986).",
1157166621,false,finalized,3,2/12/2017 18:19:49,Weak,0.6437,Q14-1008,"Bayesian models of word segmentation (Brent, 1999; Goldwater, 2007), however, have until recently completely ignored stress.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,"(Brent, 1999","The sole exception in this respect is Doyle and Levy (2013) who added stress cues to the Bigram model (Goldwater et al., 2009), demonstrating that this leads to an improvement in segmentation performance.","Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and ""algebraic"" (as opposed to ""statistical"") approaches (Christiansen et al., 1998; Yang, 2004; Lignos and Yang, 2010; Lignos, 2011; Lignos, 2012).",
1157166622,false,finalized,3,2/12/2017 18:13:38,Neut,0.6287,Q14-1008,"Relatedly and in line with the finding that stress cues are used by infants before phonotactic cues (Jusczyk et al., 1999a), we observe that phonotactic cues require more input than stress cues to be used efficiently.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,"(Jusczyk et al., 1999a)",A closer look at the knowledge acquired by our models shows that the Unique Stress Constraint of Yang (2004) can be acquired jointly with segmenting the input instead,"Overall, we find that stress cues add roughly 6% token f-score to a model that does not account for phonotactics and 4% to a model that already incorporates phonotactics.",
1157166623,false,finalized,3,2/12/2017 16:25:57,Neut,0.7123,Q14-1008,A closer look at the knowledge acquired by our models shows that the Unique Stress Constraint of Yang (2004) can be acquired jointly with segmenting the input instead,http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,(2004),93,"Relatedly and in line with the finding that stress cues are used by infants before phonotactic cues (Jusczyk et al., 1999a), we observe that phonotactic cues require more input than stress cues to be used efficiently.",
1157166624,false,finalized,3,2/12/2017 16:38:13,Pos,0.6956,Q14-1008,"Following Cutler and Carter (1987)'s observation that stressed syllables tend to occur at the beginnings of words in English, Jusczyk et al (1993) investigated whether infants acquiring English take advantage of this fact.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,(1987),"Their study demonstrated that this is indeed the case for 9 month olds, although they found no indication of using stressed syllables as cues for word boundaries in 6 month olds.","Lexical stress is the ""accentuation of syllables within words"" (Cutler, 2005) and has long been argued to play an important role in adult word recognition.",
1157166625,false,finalized,6,2/12/2017 16:27:17,Neut,0.5116,Q14-1008,Yang (2004) introduced a simple incremental algorithm that relies on stress by embodying a Unique Stress Constraint (USC) that allows at most a single stressed syllable per word.,http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,(2004),"On pre-syllabified child directed speech, he reported a word token fscore of 85.6% for a non-statistical algorithm that exploits the USC.","They only reported a word-token f-score of 44% (roughly, segmentation accuracy: see Section 4), which is considerably below the performance of subsequent models, making a direct comparison complicated.",
1157166626,false,finalized,3,2/12/2017 16:41:43,Neut,0.6556,Q14-1008,"More recently, Lignos (2010, 2011, 2012) further explored Yang's original algorithm, taking into account that function words should not be assumed to possess lexical stress cues.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,"(2010, 2011, 2012)","While his scores are in line with those reported by Yang, the importance of stress for this learner were more modest, providing a gain of around 2.5% (Lignos, 2011).","While the USC has been argued to be near-to-universal and follows from the ""culminative function of stress"" (Fromkin, 2001; Cutler, 2005), the high score Yang reported crucially depends on every word token carrying stress, including function words.",
1157166627,false,finalized,3,2/12/2017 16:33:33,Weak,1.0,Q14-1008,"However, Doyle and Levy (2013) do not directly examine the probabilities assigned to the stress-templates; they only report that their model does slightly prefer stress-initial words over the baseline model by calculating the fraction of stress-initial word types in the output segmentations of their models.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,(2013),"They also demonstrate that stress cues do indeed aid segmentation, although their reported gain of 1% in token f-score is even smaller than that reported by Lignos (2011).",This allows the model to acquire knowledge about the stress patterns of its input by assigning different probabilities to the different stress-templates.,
1157166628,false,finalized,3,2/12/2017 16:38:13,Pos,0.3531,Q14-1008,"The models we examine are derived from the collocational model of Johnson and Goldwater (2009) by varying three parameters, resulting in 6 models: two baselines that do not take advantage of stress cues and either do or do not use phonotactics, as described in Section 3.2; and four stress models that differ with respect to the use of phonotactics, and as to whether they embody the Unique Stress Constraint introduced by Yang (2004).",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,(2009),We describe these models in section 3.3.,"We give an intuitive description of the mathematical background of Adaptor Grammars in 3.1, referring the reader to Johnson et al (2007) for technical details.",
1157166629,false,finalized,3,2/12/2017 16:37:33,Neut,0.6707,Q14-1008,"Intuitively, the reason stress helps infants in segmenting English is that a stressed syllable is a reliable indicator of the beginning of a word (Jusczyk et al., 1993).",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,"(Jusczyk et al., 1993)","More generally, if there is a (reasonably) reliable relationship between the position of stressed syllables and beginnings (or","In order for stress cues to be helpful, the model must have some way of associating the position of stress with word-boundaries.",
1157166630,false,finalized,3,2/12/2017 16:41:50,CoCo,0.3505,Q14-1008,"3This is, in essence, also the strategy chosen by Doyle and Levy (2013).",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,(2013),96,This (partly) captures the tendency of English for stress-initial words and thus provide an additional cue for identifying words; and it is exactly the kind of preference infant learners of English seem to acquire (Jusczyk,
1157166631,false,finalized,3,2/12/2017 16:34:53,Neut,0.6469,Q14-1008,"We can also add the Unique Stress Constraint (USC) (Yang, 2004) by excluding all variants of rule (18) that generate two or more stressed syllables.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,"(Yang, 2004)","For example, while the lexical generator for the colloc3-nophon-stress model will include the rule Word  SSyll SSyll, the lexical generator embodying the USC lacks this rule.",This yields the colloc3-phon-stress model.,
1157166632,false,finalized,3,2/12/2017 17:00:34,Pos,1.0,Q14-1008,"We also added stress information to the BrentBernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent, 1999), following the procedure just outlined.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,"(Bernstein-Ratner, 1987",This corpus is a de-facto standard for evaluat,"As function words account for roughly 50% of the tokens but only roughly 5% of the types in our corpora, this means that the type and token distribution of stress patterns differs dramatically in all our corpora, as can be seen from Table 2.",
1157166633,false,finalized,4,2/12/2017 16:29:28,Neut,1.0,Q14-1008,"Relatedly, recent work such as Borschinger et al (2013) has found that artificially created data often masks the complexity exhibited by real speech.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,(2013),"This suggests that future work should use data directly derived from the acoustic signal to account for contextual effects, rather than using dictionary look-up or other heuristics.","For example, languages such as French lack lexical stress; it would be interesting to know whether in such a case, phonotactic (or other) cues are more important.",
1157166634,false,finalized,3,2/12/2017 17:52:36,Pos,1.0,Q14-1008,"Following Christiansen et al (1998) and Doyle and Levy (2013), we use the Korman corpus (Korman, 1984) as one of our corpora.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,(1998),"It comprises childdirected speech for very young infants, aged between 6 and 16 weeks and, like all other corpora used in this paper, is available through the CHILDES database (MacWhinney, 2000).",4.1 Corpora and corpus creation,
1157166635,false,finalized,6,2/12/2017 16:27:17,Pos,0.6769,Q14-1008,"As our third corpus, we use the Alex portion of the Providence corpus (Demuth et al., 2006; Borschinger et al., 2012).",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,"(Demuth et al., 2006",A major benefit of the Providence corpus is that the video-recordings from which the transcripts were produced are available through CHILDES alongside the transcripts.,"ing models of Bayesian word segmentation (Brent, 1999; Goldwater, 2007; Goldwater et al., 2009; Johnson and Goldwater, 2009), comprising in total 9790 utterances.",
1157166636,false,finalized,3,2/12/2017 16:41:48,CoCo,0.3614,Q14-1008,This results 2We follow Johnson and Goldwater (2009) in limiting the length of possible words to four syllables to speed up runtime.,http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,(2009),"In pilot experiments, this choice did not have a noticeable effect on segmentation performance.","Alternatively, one could limit the models ability to capture word-to-word dependencies by removing rules (1) to (3).",
1157166637,false,finalized,3,2/12/2017 18:08:39,Pos,0.3788,Q14-1008,"Unlike Christiansen et al (1998), Yang (2004), and Doyle and Levy (2013), we follow Lignos and Yang (2010) in making the more realistic assumption that the 94 mono-syllabic function words listed by Selkirk (1984) never surface with lexical stress.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,(1998),"As function words account for roughly 50% of the tokens but only roughly 5% of the types in our corpora, this means that the type and token distribution of stress patterns differs dramatically in all our corpora, as can be seen from Table 2.","Our version of the Korman corpus contains, in total, 11413 utterances.",
1157166638,false,finalized,3,2/12/2017 18:19:46,Neut,0.6906,J11-2002,"(For standard, in-depth, introductions to this fascinating field, see, e.g., Nida [1949], Jensen [1990], Spencer and Zwicky [1998], or Haspelmath [2002].)",http://www.aclweb.org/anthology/J/J11/J11-2002.pdf,[1949],"In language technology applications, a morphological component forms a bridge between texts and structured information about the vocabulary of a language.","If a language has suffixing and/or prefixing-sometimes called concatenative morphology-it obviously follows that text words in that language can be segmented into a sequence of morphological elements: a stem and a number of suffixes after the stem and/or prefixes before the stem.1 Morphology is one of the oldest linguistic subdisciplines, and this brief presentation by necessity omits many intricacies and greatly simplifies a vast scholarship.",
1157166639,false,finalized,3,2/12/2017 18:19:49,Neut,0.6774,J11-2002,"In Andreev (1965b) the method is tested extensively on Russian, which is the subject of several papers in the volume, and a number of other languages: Albanian (Persikov 1965), Armenian (Melkumjan 1965), Bulgarian (Fedulova 1965), Czech (Ozigova 1965), English (Malahovskij 1965), Estonian (Hol'm 1965), French (Kordi 1965), German (Fitialova 1965), Hausa (Fihman 1965a), Hungarian (Andreev 1965a), Latvian (Jakubajtis 1965), Serbo-Croatian (Panina 1965), Swahili (Fihman 1965b), Ukrainian (Eliseeva 1965), and Vietnamese (Jakuseva 1965).",http://www.aclweb.org/anthology/J/J11/J11-2002.pdf,(1965b),"As an aside, we may note that only after the turn of the millennium are we again seeing this variety of languages in ULM work.","The initial suffix candidate, -oj, has a high functional load and consequently a high text frequency: It is the most ambiguous of the Russian adjective suffixes, appearing in four different slots in the adjective paradigm, and is also homonymous with a noun suffix.",
1157166640,false,finalized,3,2/12/2017 18:26:36,Error,0.6873,J11-2002,"2005b; Xanthos, Hu, and Goldsmith 2006 Baroni 2000, 2003 C T Child-English/ Affix List English Cho and Han 2002 C T Korean Segmentation Sharma, Kalita, and Das 2002, 2003; C T Assamese Lexicon+ Paradigms Sharma and Das 2002 Baroni, Matiasek, and Trost 2002 C/NC T English/German (I) Related word pairs Bati 2002 C/NC T Amharic Lexicon+ Paradigms Creutz 2003, 2006; Creutz and C T Finnish/Turkish/ Segmentation Lagus 2002, 2004, 2005a, 2005b, English 2005c, 2007; Creutz, Lagus, and Virpioja 2005; Hirsimaki et al.",http://www.aclweb.org/anthology/J/J11/J11-2002.pdf,"a, 2005b, ",2003; Creutz et al.,"Harris 1955, 1968, 1970 C T English Segmentation Andreev 1965a, Andreev 1967, C T Vietnamese to Segmentation Chapter 2, Persikov 1965; Hungarian (I) Melkumjan 1965; Fedulova 1965; Ozigova 1965; Malahovskij 1965; Hol'm 1965; Kordi 1965; Fitialova 1965; Fihman 1965a; Andreev 1965a; Jakubajtis 1965; Panina 1965; Fihman 1965b; Eliseeva 1965; Jakuseva 1965 Gammon 1969 C T English Segmentation Lehmann 1973, pages 71-93 C T French (I) Segmentation de Kock and Bossaert 1969, 1974, C T French/Spanish Lexicon+ Paradigms 1978 Faulk and Gustavson 1990 C T English (I) Segmentation Hafer and Weiss 1974 C T English (IR) Segmentation Klenk and Langer 1989 C T+SP German Segmentation Langer 1991 C T+SP German Segmentation Redlich 1993 C T English (I) Segmentation Klenk 1992, 1991 C T+SP Spanish Segmentation Flenner 1992, 1994, 1995 C T+SP Spanish Segmentation Janen 1992 C T+SP French Segmentation Juola, Hall, and Boggs 1994 C T English Segmentation Brent 1993, 1999; Brent, Murthy, C T English/Child- Segmentation and Lundberg 1995; Snover 2002; English/Polish/ Snover, Jarosz, and Brent 2002; French Snover and Brent 2001, 2003 Deligne and Bimbot 1997; Deligne C T English/French (I) Segmentation 1996 Yvon 1996 C T French (I) Segmentation Kazakov 1997; Kazakov and C T French/English Segmentation Manandhar 1998, 2001 Jacquemin 1997 C T English Segmentation Cromm 1997 C T German Segmentation Gaussier 1999 C T French/English (I) Lexicon+ Paradigms Dejean 1998a, 1998b C T Turkish/English/ Affix Lists Korean/French/ Swahili/ Vietnamese (I) Medina Urrea 2000, 2003, 2006b C T Spanish Affix List Schone and Jurafsky 2000, 2001a; C T English Segmentation Schone 2001 Goldsmith 2000, 2001, 2006; Belkin C T English (I) Lexicon+ Paradigms and Goldsmith 2002; Goldsmith, Higgins, and Soglasnova 2001; Hu et al.",
1157166641,false,finalized,3,2/12/2017 18:21:01,Neut,0.6262,J11-2002,"2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al.",http://www.aclweb.org/anthology/J/J11/J11-2002.pdf,"a, 2002b","2004 Oliver 2004, Chapter 4-5 C T Catalan Paradigms Bordag 2005a, 2005b, 2007, 2008 C T English/German Segmentation Hammarstrom 2005, 2006a, 2006b, C - Maori to Warlpiri Same-stem 2007b, 2009a, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation ur Rehman and Hussain 2005 C T Finnish/Turkish/English Segmentation Jordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101-139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al.",2005; Hu et al.,
1157166642,false,finalized,3,2/12/2017 18:24:25,Neut,1.0,J11-2002,See Goldsmith and Xanthos (2009) for an excellent survey on how to do this (something which falls under learning phonological categories rather than morphology learning).,http://www.aclweb.org/anthology/J/J11/J11-2002.pdf,(2009),"Basically, it is possible only because there are systematic combination constraints between different phonemes (approximated by graphemes); for example, vowels and consonants alternate in a very non-random manner.","Xanthos (2007), on the other hand, starts out only by assuming that there exists a distinction between root and pattern graphemes and subsequently learns which graphemes are which.",
1157166643,false,finalized,3,2/12/2017 18:07:36,CoCo,0.3683,J11-2002,"In the group-and-abstract paradigm, working with feature sets of a word, as in De Pauw and Wagacha (2007), is an ingenious generalization that holds numerous advantages over string edit distances.",http://www.aclweb.org/anthology/J/J11/J11-2002.pdf,(2007),"Feature set comparisons are naturally defined over arbitrary collections, whereas string edit distances work on pairs of strings.",The recent increased interest in Bayesian generative models in general in NLP may possibly serve as a catalyst.,
1157166644,false,finalized,3,2/12/2017 18:07:36,Neut,1.0,P00-1072,"One can prove that Ak is the best approximation to A for any unitray invariant norm(Michael W. Berry and Jessup, 1999) three variant probability estimation methods in dimension-reduced space.",http://www.aclweb.org/anthology/P/P00/P00-1072.pdf,"Berry and Jessup, 1999)",First is estimating p(y Ix) by computing distance between given word x and predicting word y in reduced space.,"Now, we suggest 21n other words, the projection into the reduced space is chosen such that the representations in the original space are changed as little as possible when measured by the sum of the squares of the differences.",
1157166645,false,finalized,3,2/12/2017 18:35:28,Neut,0.3715,P00-1072,"The similarity-based method(Dagan et al., 1999) and dimension reduction technique can be merged into one model 3 .",http://www.aclweb.org/anthology/P/P00/P00-1072.pdf,"(Dagan et al., 1999)",Reduced dimension can be better representation space than the original space for finding similarities between words.,(k = 3) (dim = 2) (6 = 0.1) (0 = 0) P(swig I coffee) 0 0.09 0 0.1627 0.0756 0.11 P(sip I coffee) 1 1 1 0.2425 0.5536 1 P(drink I coffee) 0 0.18 0.17 0.2359 0.1932 0.28 P(devour I coffee) 0 0.18 0.11 0.1271 0.0726 0.11 P(eat I coffee) 0 0.18 0.11 0.1159 0.0526 0 P(swallow I coffee) 0 0.18 0.11 0.1159 0.0526 0 2.3.3 Method 3: Dimension-reduced similarity-based method,
1157166646,false,finalized,3,2/12/2017 18:19:51,CoCo,0.7009,P00-1072,"Data preparation method and error counting scheme are almost similar to that of similarity-based methods (Dagan et al., 1999) (Lee and Pereira, 1999).",http://www.aclweb.org/anthology/P/P00/P00-1072.pdf,"(Dagan et al., 1999)","Performance is measured by the error rate, defined as error rate = T-1(0 of incorrect choices) where T is the size of test set.","Each method is presented with a noun and two verbs, deciding which verb is more likely to have the noun as a direct object.",
1157166647,false,finalized,3,2/12/2017 18:11:01,Pos,0.6906,P00-1072,"For similarity-based method, the parameter tuning is important to improve performance but we use the simplified unweighted average equation as in (Lee and Pereira, 1999) 6 .",http://www.aclweb.org/anthology/P/P00/P00-1072.pdf,"(Lee and Pereira, 1999)","Since this equation is the same as our estimation method in Section 2.3.3, we can say that the comparison is fair.","Pd(Yilxi) frq(xi, yi) > 0 cx(xi) P(yi) frq(xi,yi) = 0",
1157166648,false,finalized,3,2/12/2017 18:11:08,Neut,1.0,W11-2820,"In particular, adaptive hypertext (O'Donnell et al., 2001) adapts the content and form of natural language text.",http://www.aclweb.org/anthology/W/W11/W11-2820.pdf,"Donnell et al., 2001)",Systems like this introduce the need for a good model of the context and how it influences language.,Adaptive interfaces change the style and content of interaction according to the context of use.,
1157166649,false,finalized,3,2/12/2017 18:36:12,Neut,0.6422,W11-2820,"In the VRE, the link between the social network and digital artifacts is established formally, by the integration of the FOAF social networking vocabulary (Brickley and Miller, 2010) with our provenance ontologies.",http://www.aclweb.org/anthology/W/W11/W11-2820.pdf,"(Brickley and Miller, 2010)","FOAF characterises an individual and their social network by defining a vocabulary describing people, the links between them and the things they create and do.",The user's social context.,
1157166650,false,finalized,3,2/12/2017 18:14:20,Pos,1.0,W11-2820,"This service generates text containing a description of the resource using a deep model of the syntactic structure of sentences and their combinations, inspired by the work of Hielkema (2010).",http://www.aclweb.org/anthology/W/W11/W11-2820.pdf,(2010),VRE User Interface User Core Services Social Networking Metadata Repository Policy Policy Manager Text Interface,"In order to generate the text, we have implemented a RESTful service that invokes a Text Generator service based on the RDF ID of the resource being described, passed as a parameter by the Web interface.",
1157166651,false,finalized,3,2/12/2017 18:28:49,Neut,0.3488,W11-2820,"To determine if two policies may conflict, we plan to use a conflict detection mechanism similar to the one proposed by Sensoy et al (2010).",http://www.aclweb.org/anthology/W/W11/W11-2820.pdf,(2010),"Moreover regarding usability, we need to implement a system that would allow users to easily create SPIN rules representing their policies, possibly using a NLG interface.","Using such a strategy, the Policy Manager would be able to determine how to prioritise conflicting policies applying to a particular resource.",
1157166652,false,finalized,3,2/12/2017 18:15:53,Pos,0.7009,W11-2143,"We managed our experiments with LoonyBin (Clark and Lavie, 2010), an open-source tool for defining, modifying, and running complex experimental pipelines.",http://www.aclweb.org/anthology/W/W11/W11-2143.pdf,"(Clark and Lavie, 2010)",We describe our system-building process in more detail in Section 2.,"Decoding was carried out in Joshua (Li et al., 2009), an open-source framework for parsing-based MT.",
1157166653,false,finalized,3,2/12/2017 18:14:12,Pos,0.7009,W11-2143,"We built a 5-gram language model from it with the SRI language modeling toolkit (Stolcke, 2002).",http://www.aclweb.org/anthology/W/W11/W11-2143.pdf,"(Stolcke, 2002)","To match the treatment given to the training data, the language model was also built in mixed case.",The final prepared corpus was made up of approximately 1.8 billion words of running text.,
1157166654,false,finalized,3,2/12/2017 18:19:03,Weak,0.3467,W11-2143,They are also larger than the 0.7- to 1.1-point gains reported by Pino et al (2010) when the full Giga-FrEn was added.,http://www.aclweb.org/anthology/W/W11/W11-2143.pdf,(2010),"The 2011 system also shows a significant reduction in the out-of-vocabulary (OOV) rate on both test sets: 38% and 47% fewer OOV types, and 44% and 45% fewer OOV tokens, when compared to the 2010 system.","Our BLEU score improvements of 1.2 to 1.9 points are statistically significant according to the paired bootstrap resampling method (Koehn, 2004) with n = 1000 and p < 0.01.",
1157166655,false,finalized,3,2/12/2017 18:14:43,CoCo,0.6866,W11-2143,"In design and construction, the system is similar to our submission from last year's workshop (Hanneman et al., 2010), with changes in the methods we employed for training data selection and SCFG filtering.",http://www.aclweb.org/anthology/W/W11/W11-2143.pdf,"(Hanneman et al., 2010)","Continuing WMT's general trend, we worked with more data than in previous years, basing our 2011 system on 13.9 million sentences of parallel French-English training data and an English language model of 1.8 billion words.",This is our fourth yearly submission to the WMT shared translation task.,
1157166656,false,finalized,3,2/12/2017 18:14:12,Pos,0.7009,W11-2143,"For generating parse trees, we used the French and English grammars of the Berkeley statistical parser (Petrov and Klein, 2007).",http://www.aclweb.org/anthology/W/W11/W11-2143.pdf,"(Petrov and Klein, 2007)","Except for minor bug fixes, our method for extracting and scoring a translation grammar remains the same as in our WMT 2010 submission.","Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the grow-diag-final-and heuristic (Koehn et al., 2005).",
1157166657,false,finalized,3,2/12/2017 19:14:18,Neut,0.6557,P10-2063,"The cross-product of the stem name and (open-class) reduced core POS tags, plus the CLOSED tag, yields 24 labels for a CRF classifier in Mallet (McCallum, 2002).",http://www.aclweb.org/anthology/P/P10/P10-2063.pdf,"(McCallum, 2002)",4 Experiments and Evaluation,"The classifier is used only to get solutions for the openclass words, although we wish to give the classifier all the words for the sentence.",
1157166658,false,finalized,3,2/12/2017 17:59:11,Neut,0.6873,P10-2063,"In contrast, other approaches have used a pipelined approach, with separate models to first do tokenization and then part-of-speech tagging (Diab et al., 2007; Diab, 2009).",http://www.aclweb.org/anthology/P/P10/P10-2063.pdf,"(Diab et al., 2007","While these approaches have somewhat lower performance than the joint approach, they have the advantage that they do not rely on the presence of a full-blown morphological analyzer, which may not always be available or appropriate as the data shifts to different genres or Arabic dialects.","1SAMA-v3.1 is an updated version of BAMA, with many significant differences in analysis.",
1157166659,false,finalized,3,2/12/2017 18:24:25,CoCo,0.6512,P10-2063,"The ""MorphPOS"" task in (Roth et al., 2008), 96.4%, is somewhat similar to ours in that it scores on a ""core tag"", but unlike for us there is only one such tag for a source token (easier) but it distinguishes between NOUN and ADJ (harder).",http://www.aclweb.org/anthology/P/P10/P10-2063.pdf,"(Roth et al., 2008)",We would like to do a direct comparison by simply runing the above systems on the exact same data and evaluating them the same way.,"However, both (Habash and Rambow, 2005; Diab et al., 2007) assume gold tokenization for evaluation of POS results, which we do not.",
1157166660,false,finalized,3,2/12/2017 18:14:12,Neut,1.0,W12-3159,"Random restarts and random walks (Moore and Quirk, 2008) are commonly used to combat the fact the search space is highly non-convex, often with multiple minima.",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf,"(Moore and Quirk, 2008)","Several problems still remain with MERT, three of which are addressed by this work.","Recent efforts extended MERT to work on lattices (Macherey et al., 2008) and hypergraphs (Kumar et al., 2009).",
1157166661,false,finalized,3,2/12/2017 19:07:58,Neut,0.6955,W12-3159,"Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an error-prone decoder differs from the one of an exact decoder (Chang and Collins, 2011).",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf,"(Chang and Collins, 2011)","MERT calculates an envelope from candidate translations and assumes all translations on the envelope are reachable by the decoder, but these translations may become unreachable due to search errors.","First, the Nbest error surface explored by MERT is generally not the same as the true error surface, which means that the error rate at an optimum1 of the N-best error surface is not guaranteed to be any close to an optimum of the true error surface.",
1157166662,false,finalized,3,2/12/2017 18:11:08,Pos,1.0,W12-3159,"Since it is impractical to calculate derivatives of this function, we use derivative-free optimization methods such as the downhill simplex method (Helder and Mead, 1965) and Powell's method (Powell, 1964), which generally handle such difficult search conditions relatively well.",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf,"(Helder and Mead, 1965)",This approach confers several benefits over MERT.,"which takes source sentences and a set of weights as inputs, and outputs the evaluation score (e.g., BLEU score) computed on the decoded sentences.",
1157166663,false,finalized,3,2/12/2017 18:19:51,Pos,1.0,W12-3159,"First, we use a model selection acceleration technique called racing (Moore and Lee, 1994) in conjunction with randomization tests (Riezler and Maxwell, 2005) to avoid decoding the entire development set at each function evaluation.",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf,"(Moore and Lee, 1994)",This approach discards the current model whenever performance on the translated subset of the development data is deemed significantly worse in comparison to the current best model.,"In this paper, we make direct search reasonably fast thanks to two speedup techniques.",
1157166664,false,finalized,3,2/12/2017 18:36:12,Neut,1.0,W12-3159,"Racing for model selection (Maron and Moore, 1994; Moore and Lee, 1994) works as follows: we are given a collection of N,,t models and Nd data points, and we must find the  model that minimizes the mean e j = 1 i ej(i),",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf,"(Maron and Moore, 1994",Nd,"Prior to considering the SMT case, we review one of these methods in the case of leave-one-out cross validation (LOOCV).",
1157166665,false,finalized,3,2/12/2017 18:26:33,Neut,0.3519,W12-3159,"As evaluations progress, we eliminate any model that is significantly worse than any other model.3 We also note that the Racing technique first randomizes the order of the data points to ensure that prefixes of the dataset are gen3The details of these statistical tests are not so important here since we use different ones in the case of SMT, but we briefly summarize them as follows: Maron and Moore (1994) use a non-parametric method (Hoeffding bounds (Hoeffding, 1963)) for confidence estimation, and places confidence intervals on the mean value of the random variable representing ej(i).",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf,(1994),A model is discarded if its confidence interval no longer overlaps with the confidence interval of the current best model.,"The models are evaluated concurrently, and at any given step k E [1, Nd], each model Mj is associated with two pieces of information: the current estimate of its mean error rate, and the estimate of its variance.",
1157166666,false,finalized,3,2/12/2017 19:14:18,Pos,1.0,W12-3159,"We use another technique to speed up direct search by storing and re-using search graphs, which consist of lattices in the case of phrase-based decoding (Och et al., 1999) and hypergraphs in the case of hierarchical decoding (Chiang, 2005).",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf,"(Och et al., 1999)","The successive expansion of translation options in order to construct the search graph is generally done from scratch, but this can be wasteful when the same sentences are translated multiple times, as it is the case with direct search.",5.2 Lattice-based decoding,
1157166667,false,finalized,3,2/12/2017 18:26:33,Pos,0.6536,W12-3159,"For our experiments, we use a phrase-based translation system similar to Moses (Koehn et al., 2007).",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf,"(Koehn et al., 2007)","Our decoder uses many of the same features as Moses, including four phrasal and lexicalized translation scores, phrase penalty, word penalty, a language model score, linear distortion, and six lexicalized reordering scores.",6.1 Setup,
1157166668,false,finalized,3,2/12/2017 18:18:11,Weak,0.697,W12-3159,"In this paper, we use the randomization test for discarding unpromising models, since this statistical test was shown to be less likely to cause type-I errors6 than bootstrap methods (Riezler and Maxwell, 2005).",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf,"(Riezler and Maxwell, 2005)","Since both kinds of statistical tests involve a time-consuming sampling step, it 4Since Racing only discards suboptimal models, the current best model M* is one for which we have decoded the entire development set.","In SMT, it is common to use either bootstrap resampling (Efron and Tibshirani, 1993; Och, 2003) or randomization tests (Noreen, 1989).",
1157166669,false,finalized,3,2/12/2017 19:14:18,Neut,1.0,D14-1114,"(Ren et al., 2014) uses an unsupervised heterogeneous clustering.",http://www.aclweb.org/anthology/D/D14/D14-1114.pdf,"(Ren et al., 2014)","(Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics.","(Tan et al., 2012) encode intent in language models, aware of long-lasting interests.",
1157166670,false,finalized,3,2/12/2017 17:59:11,Neut,1.0,D14-1114,"There are also works using sponsered data (Yamamoto et al., 2012) and interactive data (Ruotsalo et al., 2013).",http://www.aclweb.org/anthology/D/D14/D14-1114.pdf,"(Yamamoto et al., 2012)",The new trend of integrating knowledge graph will be discussed next.,"Quite an amount of work leverage query logs (Jiang et al., 2013), including query reformulations (Radlinski et al., 2010), click-through data (Li et al., 2008).",
1157166671,false,finalized,3,2/12/2017 18:38:59,Neut,1.0,D14-1114,"(Ren et al., 2014) utilizes knowledge graph resources in a hetrogeneous view.",http://www.aclweb.org/anthology/D/D14/D14-1114.pdf,"(Ren et al., 2014)","(Lin et al., 2012) also pays attention to refiners, but restricted to limited domains, while our method is more general.","(Pantel et al., 2012) models latent intent to mine entity type distributions.",
1157166672,false,finalized,3,2/12/2017 18:14:43,CoCo,0.6866,D14-1114,"(Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and (Wang et al., 2009) mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics.",http://www.aclweb.org/anthology/D/D14/D14-1114.pdf,"(Yin and Shah, 2010)","We do not split queries into clusters or subtopics relevant to the original query to indicate a intent, but link them in an graph with intent feature similarity, weakly or strongly, in a holistical view.","(Ren et al., 2014) uses an unsupervised heterogeneous clustering.",
1157166673,false,finalized,3,2/12/2017 18:26:33,Neut,0.6481,D14-1114,"As query log induced intent topic graph is of considerable large size, the pair-wise similarity is computationally prohibitive, hence we use Local Sensitive Hash (Indyk and Motwani, 1998) for each similarity metric so as to compute ISim just in candidate set.",http://www.aclweb.org/anthology/D/D14/D14-1114.pdf,"(Indyk and Motwani, 1998)","We use random hyperplane based hash family proposed in (Charikar, 2002) and set the hash code dimension and hash table numbers empirically to ensure the number of nodes falling into each bucket is relatively stable.","To put it in more details, we use jaccard similarity for name shinglings and cosine similarity for domain and topic vector.",
1157166674,false,finalized,3,2/12/2017 18:07:44,Neut,0.6774,D14-1114,"We use random hyperplane based hash family proposed in (Charikar, 2002) and set the hash code dimension and hash table numbers empirically to ensure the number of nodes falling into each bucket is relatively stable.",http://www.aclweb.org/anthology/D/D14/D14-1114.pdf,"(Charikar, 2002)",3.2 Merging nodes,"As query log induced intent topic graph is of considerable large size, the pair-wise similarity is computationally prohibitive, hence we use Local Sensitive Hash (Indyk and Motwani, 1998) for each similarity metric so as to compute ISim just in candidate set.",
1157166675,false,finalized,3,2/12/2017 18:49:12,Pos,0.6779,D14-1114,"We use an annotation tool especially for short text (Ferragina and Scaiella, 2012) called Tagme3 to recognize entities and observe only 16% of all the queries are exactly an entity itself, which means most of queries do have refiner words to convey information need.",http://www.aclweb.org/anthology/D/D14/D14-1114.pdf,"(Ferragina and Scaiella, 2012)","To ensure the precision of recognized entities, we set a significant threshold and bottom line threshold , queries should have at least one recognized entity with a likelihood above significant level, and those below bottom line are ignored.",We then move on to map queries to Freebase and empirically filter sessions that are less entity-centric.,
1157166676,false,finalized,3,2/12/2017 18:28:49,Neut,0.6868,J99-2001,"The appropriate movement and marking of local focus, and the appropriate choice of the form of a noun phrase (NP) based on local focus information, are considered to contribute to the local coherence exhibited by discourse (Sidner [1979], Grosz, Joshi, and Weinstein [1983, 1995], Carter [1987], and others).",http://www.aclweb.org/anthology/J/J99/J99-2001.pdf,[1979],"In addition, local focus information is one source of information that is used by readers and hearers for interpreting pronouns.","By ""local focus,"" we refer to the person, object, property, or concept that a sentence is most centrally about within the discourse context in which it occurs.",
1157166677,false,finalized,3,2/12/2017 18:38:59,CoCo,0.3738,P05-1046,"4While it may be surprising that disallowing reestimation of the transition function is helpful here, the same has been observed in acoustic modeling (Rabiner and Juang, 1993).",http://www.aclweb.org/anthology/P/P05/P05-1046.pdf,"(Rabiner and Juang, 1993)",375,"where P, is the common word distribution, and  is",
1157166678,false,finalized,3,2/12/2017 18:14:43,Neut,0.6866,P05-1046,Pasula et al (2002) performs limited unsupervised segmentation of bibliographic citations as a small part of a larger probabilistic model of identity uncertainty.,http://www.aclweb.org/anthology/P/P05/P05-1046.pdf,(2002),"However, their system does not explicitly learn a field segmentation model for the citations, and encodes a large amount of hand-supplied information about name forms, abbreviation schemes, and so on.",There has also been some previous work on unsupervised learning of field segmentation models in particular domains.,
1157166679,false,finalized,3,2/12/2017 18:07:44,Neut,0.6774,P05-1046,"Finally, Blei and Moreno (2001) use an HMM augmented by an aspect model to automatically segment documents, similar in goal to the system of Hearst (1997), but using techniques more similar to the present work.",http://www.aclweb.org/anthology/P/P05/P05-1046.pdf,(2001),7 Conclusions,Because the structure of the HMMs they learn is similar to ours it seems that their system could benefit from the techniques of this paper.,
1157166680,false,finalized,3,2/12/2017 18:26:36,Neut,1.0,W15-1842,"We select a FrameNet (Baker et al., 1998) frame and write shallowly syntactic pattern-matching rules based on part-of-speech information and morphology from either a morphological automaton or tagged text.",http://www.aclweb.org/anthology/W/W15/W15-1842.pdf,"(Baker et al., 1998)",1 Introduction,"We use hfst-pmatch (Linden et al., 2013), a pattern-matching tool mimicking and extending Xerox fst (Karttunen, 2011), for demonstrating how to develop a semantic frame extractor.",
1157166681,false,finalized,3,2/12/2017 18:35:28,Pos,1.0,W15-1842,"We use hfst-pmatch (Linden et al., 2013), a pattern-matching tool mimicking and extending Xerox fst (Karttunen, 2011), for demonstrating how to develop a semantic frame extractor.",http://www.aclweb.org/anthology/W/W15/W15-1842.pdf,"(Linden et al., 2013)","We select a FrameNet (Baker et al., 1998) frame and write shallowly syntactic pattern-matching rules based on part-of-speech information and morphology from either a morphological automaton or tagged text.",Abstract,
1157166682,false,finalized,3,2/12/2017 18:11:01,Neut,0.6777,W15-1402,"The use of supervised machine learning techniques for metaphor identification has increased manyfold in the recent years (see section 10, Related Work, for a review and references), partially due to the availability of largescale annotated resources for training and evaluating the algorithms, such as the VU Amsterdam corpus (Steen et al., 2010), datasets built as part of a U.S. government-funded initiative to advance the state-of-art in metaphor identification and interpretation (Mohler et al., 2013; Strzalkowski et al., 2013), and recent annotation efforts with other kinds of data (Beigman Klebanov and Flor, 2013; Jang et al., 2014).",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf,"(Steen et al., 2010)","Some of these data are publicly available (Steen et al., 2010), allowing for benchmarking and for measuring incremental improvements, which is the approach taken in this paper.","In this paper, we present a set of experiments aimed at improving on previous work on the task of supervised word-level detection of linguistic metaphor in running text.",
1157166683,false,finalized,3,2/12/2017 18:19:46,Pos,1.0,W15-1402,"We start with a baseline set of features and training regime from Beigman Klebanov et al (2014), and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations.",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf,(2014),"The usage of concreteness features was previously discussed in the literature; to our knowledge, these features have not yet been evaluated for their impact in a comprehensive system for word-level metaphor detection, apart from the concreteness features as used in Beigman Klebanov et al (2014), which we use as a baseline.","Content tokens are nouns, adjectives, adverbs, and verbs.",
1157166684,false,finalized,3,2/12/2017 18:19:03,Pos,0.688,W15-1402,"We use the logistic regression classifier as implemented in the SKLL package (Blanchard et al., 2013), which is based on scikitlearn (Pedregosa et al., 2011), with F1 optimization (""metaphor"" class).",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf,"(Blanchard et al., 2013)","Performance will be evaluated using Precision, Recall, and F-1 score, for the positive (""metaphor"") class.","In this study, each content-word token in a text is an instance that is classified as either a metaphor or not a metaphor.",
1157166685,false,finalized,3,2/12/2017 18:28:49,Neut,0.6375,W15-1402,"Given that the category distribution is generally heavily skewed towards the non-metaphor category (see Table 1), we experimented with cost-sensitive machine learning techniques to try to correct for the imbalanced class distribution (Yang et al., 2014; Muller et al., 2014).",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf,"(Yang et al., 2014","The first technique uses AutoWeight (as implemented in the auto flag in scikitlearn toolkit), where we assign weights that are inversely proportional to the class frequencies.2 Table 2 shows the results.",5 Experiment 1: Re-weighting of Examples,
1157166686,false,finalized,3,2/12/2017 18:49:12,Neut,0.6868,W15-1402,"Given the relevant literature that has put forward concreteness and difference in concreteness as important predictors of metaphoricity (Dunn, 2014; Tsvetkov et al., 2014; Gandy et al., 2013; Assaf et al., 2013; Turney et al., 2011), it is instructive to evaluate the overall contribution of the concreteness features over the UPT baseline (no concreteness features), across the different weighting regimes.",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf,"(Dunn, 2014",Table 9 provides this information.,"The small improvement is perhaps not surprising, since the baseline model itself already contains a version of the concreteness features.",
1157166687,false,finalized,3,2/12/2017 18:14:35,Neut,0.6872,W15-1402,Shutova and Sun (2013) and Shutova et al (2013) explored unsupervised clustering-based approaches.,http://www.aclweb.org/anthology/W/W15/W15-1402.pdf,(2013),"Features used in supervised learning approaches include selectional preferences violation, outlier detection, semantic analysis using topical signatures and ontologies, as well as n-gram features, among others (Tsvetkov et al., 2014; Schulder and Hovy, 2014; Beigman Klebanov et al., 2014; Mohler et al., 2013; Dunn, 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Strzalkowski et al., 2013; Bethard et al., 2009; Pasanek and Sculley, 2008).","The field of automated identification of metaphor has grown dramatically over the last few years, and there exists a plurality of approaches to the task.",
1157166688,false,finalized,3,2/12/2017 17:59:11,Neut,0.6409,W15-1402,"The usage of concreteness features was previously discussed in the literature; to our knowledge, these features have not yet been evaluated for their impact in a comprehensive system for word-level metaphor detection, apart from the concreteness features as used in Beigman Klebanov et al (2014), which we use as a baseline.",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf,(2014),2 Data,"We start with a baseline set of features and training regime from Beigman Klebanov et al (2014), and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations.",
1157166689,false,finalized,3,2/12/2017 18:19:51,CoCo,0.6729,W15-1402,"In order to allow for direct comparison with prior work, we used the same subset of these data as Beigman Klebanov et al (2014), in the same crossvalidation setting.",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf,(2014),"The total of 90 fragments are used in cross-validation: 10-fold on News, 9-fold on Conversation, 11 on Fiction, and 12 on Academic.","The data is annotated according to the MIPVU procedure (Steen et al., 2010) with the interannotator reliability of  > 0.8.",
1157166690,false,finalized,3,2/12/2017 18:19:51,Pos,1.0,W15-1402,"As a baseline, we use the best performing feature set from Beigman Klebanov et al (2014), who investigated supervised word-level identification of metaphors.",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf,(2014),"We investigate the effect of reweighting of examples, as well as the effectiveness of features related to the notion of concreteness.","Performance will be evaluated using Precision, Recall, and F-1 score, for the positive (""metaphor"") class.",
1157166691,false,finalized,3,2/12/2017 17:59:11,Pos,1.0,W15-1402,"As a baseline, we use the best feature set from Beigman Klebanov et al (2014).",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf,(2014),"Specifically, the baseline contains the following families of features:",4 Baseline System,
1157166692,false,finalized,3,2/12/2017 18:36:12,Pos,0.6933,W15-1402,"In this paper, we use mean concreteness scores for words as published in the large-scale norming study by Brysbaert et al (2013).",http://www.aclweb.org/anthology/W/W15/W15-1402.pdf,(2013),"The dataset has a reasonable coverage for our data; thus, 78% of tokens in Set A have a concreteness rating.",6 Experiment 2: Re-representing concreteness information,
1157166693,false,finalized,3,2/12/2017 18:07:44,Neut,0.6774,W06-2717,"This program is a graphical user interface to insert (or correct) alignments between pairs of syntax trees.4 The TreeAligner can be seen in the line of tools such as I*Link (Ahrenberg et al., 2002) or Cairo (Smith and Jahr, 2000) but it is especially tailored to visualize and align full syntax trees.",http://www.aclweb.org/anthology/W/W06/W06-2717.pdf,"(Ahrenberg et al., 2002)",The TreeAligner requires three input files.,"For this purpose we have developed a ""TreeAligner"".",
1157166694,false,finalized,3,2/12/2017 18:18:07,Pos,1.0,W06-2717,"We use the SALSA tool developed at Saarbrucken University (Erk and Pado, 2004) which also assumes TIGER-XML input.",http://www.aclweb.org/anthology/W/W06/W06-2717.pdf,"(Erk and Pado, 2004)",,"For example, we are currently experimenting with the annotation of semantic frames on top of the treebanks.",
1157166695,false,finalized,3,2/12/2017 18:14:12,Neut,1.0,W13-1708,"Standardized tests like GRE and GMAT too use such systems to complement human scorers while evaluating student essays automatically (Burstein, 2003; Rudner et al., 2005).",http://www.aclweb.org/anthology/W/W13/W13-1708.pdf,"(Burstein, 2003","Zhang (2008) discusses proficiency classification for the Examination for the Certificate of Proficiency in English (ECPE) in detail, by comparing procedures based on four types of measurement models.","Forms of text used for assessment include mathematical responses, short answers, essays and spoken responses among others (Williamson et al., 2010).",
1157166696,false,finalized,3,2/12/2017 18:35:28,Neut,0.6966,W13-1708,"Providing a non-error driven model, Crossley et al (2011) studied the impact of various lexical indices in predicting the learner proficiency level.",http://www.aclweb.org/anthology/W/W13/W13-1708.pdf,(2011),"Using a corpus of 100 writing samples by L2 learners of English classified in to three levels (beginner, intermediate, advanced), they built a classification system that analyses language proficiency using the Coh-metrix5 lexical indices.","Although error-rate is a strong indicator of a learner's proficiency in a language, considering other factors like lexical indices or syntactic and morphological complexity would help in providing multiple views about the same data.",
1157166697,false,finalized,3,2/12/2017 18:11:01,Pos,0.6317,W13-1708,Gama and Brazdil (2000) showed that a cascade can outperform other ensemble methods like stacking or boosting.,http://www.aclweb.org/anthology/W/W13/W13-1708.pdf,(2000),Kaynak and Alpaydin (2000) proposed a method to sequentially cascade classifiers and showed that this improves the accuracy without increasing the computational complexity and cost.,Cascade generalization is the process of sequentially using a set of small classifiers to perform an overall classification task.,
1157166698,false,finalized,3,2/12/2017 18:26:36,Neut,0.6409,W13-1708,"For all our classification experiments, we used the WEKA (Hall et al., 2009) toolkit.",http://www.aclweb.org/anthology/W/W13/W13-1708.pdf,"(Hall et al., 2009)",We report the overall classification accuracy as our evaluation metric.,"Hence, we further investigated the problem as a collection of multi-stage twoclass cascades instead of a single stage three class classification.",
1157166699,false,finalized,3,2/12/2017 19:14:18,Pos,1.0,W13-1708,"We used the StackingC implementation of WEKA (Seewald, 2002) to combine the models, with a linear regression model as our meta classifier.",http://www.aclweb.org/anthology/W/W13/W13-1708.pdf,"(Seewald, 2002)",Table 4 shows the classification accuracies for the individual classifiers as well as the ensemble on a 10-fold CV of the training set and on the held out test set.,"We trained three classification models on the entire feature set, using the same train-test sets as explained before and trained an ensemble model with three classifiers.",
1157166700,false,finalized,3,2/12/2017 18:26:36,Neut,1.0,W13-0804,"Cube pruning (Chiang, 2007) is a widely used search strategy in state-of-the-art hierarchical decoders.",http://www.aclweb.org/anthology/W/W13/W13-0804.pdf,"(Chiang, 2007)","Some alternatives and extensions to the classical algorithm as proposed by David Chiang have been presented in the literature since, e.g.",1 Introduction,
1157166701,false,finalized,3,2/12/2017 18:36:12,Neut,1.0,W13-0804,"Chiang also introduced the cube pruning algorithm for hierarchical search (Chiang, 2007).",http://www.aclweb.org/anthology/W/W13/W13-0804.pdf,"(Chiang, 2007)",It is basically an adaptation of one of the k-best parsing algorithms by Huang and Chiang (2005).,Hierarchical phrase-based translation (HPBT) was first proposed by Chiang (2005).,
1157166702,false,finalized,3,2/12/2017 18:49:12,Neut,1.0,W13-0804,Good descriptions of the cube pruning implementation in the Joshua decoder have been provided by Li and Khudanpur (2008) and Li et al (2009b).,http://www.aclweb.org/anthology/W/W13/W13-0804.pdf,(2008),Xu and Koehn (2012) implemented hierarchical search with the cube growing algorithm in Moses and compared its performance to Moses' cube pruning implementation.,It is basically an adaptation of one of the k-best parsing algorithms by Huang and Chiang (2005).,
1157166703,false,finalized,3,2/12/2017 18:11:01,Pos,0.6777,W13-0804,"During decoding, a maximum length constraint of ten is applied to all non-terminals except the initial symbol S. Model weights are optimized with MERT (Och, 2003) on 100-best lists.",http://www.aclweb.org/anthology/W/W13/W13-0804.pdf,"(Och, 2003)",The optimized weights are obtained (separately for deep and for shallow-1 grammars) with a k-best generation size of 1000 for Chinese-*English and of 500 for Arabic-*English and kept for all setups.,"The language models are 4-grams with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) which have been trained with the SRILM toolkit (Stolcke, 2002).",
1157166704,false,finalized,3,2/12/2017 19:07:58,Pos,0.6375,W00-1220,"The most important are Vendler(1967), Bache(1982), and Smith (1985) .",http://www.aclweb.org/anthology/W/W00/W00-1220.pdf,(1967),"They approximately classify the situation as four types: state, activity, accomplishment, and achievement.","In resent years, Western researchers have published a large volume of papers, which present many points of view.",
1157166705,false,finalized,3,2/12/2017 18:14:43,Neut,1.0,W00-1220,"Chen[3] points that some verbs belong to more than one category, and gives a method to distinguish these cases.",http://www.aclweb.org/anthology/W/W00/W00-1220.pdf,[3],"To make the ideal more clear, we use two steps to complete the sentence situation recognition.",(2) Separate classification of the verb situation from classification of the sentence situation.,
1157166706,false,finalized,3,2/12/2017 18:24:25,Pos,0.6512,W00-1220,"According to table 2.1, a classification algorithm was designed, and we use two resources to implement our algorithm: The Contemporary Chinese Cihai [11] (which we will refer to as the Cihai below) dictionary and the Machine Tractable Dictionary of Contemporary Chinese Predicate Verbs [12}(which we will refer to as the predicate dictionary below).",http://www.aclweb.org/anthology/W/W00/W00-1220.pdf,[11],"The Cihai dictionary includes 12,000 entries and 700,000 collocation instances, predicate dictionary includes about 3000 verbs with their semantic information, case relations and detailed collocation information.",2.3 Implementation of the algorithm,
1157166707,false,finalized,3,2/12/2017 18:26:33,Neut,1.0,S15-2120,Davidov et al (2010) examined hashtags that indicated sarcasm to identify if such labelled tweets can be a reliable source of sarcasm.,http://www.aclweb.org/anthology/S/S15/S15-2120.pdf,(2010),They concluded that user-labelled sarcastic tweets can be noisy and constitute the hardest form of sarcasm.,2010).,
1157166708,false,finalized,3,2/12/2017 18:08:39,Neut,0.6818,S15-2120,Riloff et al (2013) identify sarcasm that arises from the contrast between a positive sentiment referring to a negative situation.,http://www.aclweb.org/anthology/S/S15/S15-2120.pdf,(2013),"Reyes et al (2012) involved in their work features that make use of contextual imbalance, natural language concepts, syntactical and morphological aspects of a tweet.",They concluded that user-labelled sarcastic tweets can be noisy and constitute the hardest form of sarcasm.,
1157166709,false,finalized,3,2/12/2017 18:24:25,Neut,0.6711,N13-3003,"those MT outputs that are guaranteed to require less post-editing effort than the best corresponding TM match are presented to the post-editor (He et al., 2010a).",http://www.aclweb.org/anthology/N/N13/N13-3003.pdf,"(He et al., 2010a)","The MT is integrated seamlessly, and established localisation cost estimation models based on TM technologies still apply as upper bounds.",The confidence metric ensures that only Author did this work during his post doctoral research at CNGL.,
1157166710,false,finalized,3,2/12/2017 18:07:36,Neut,1.0,N13-3003,"(Specia, 2011) and (Specia et al., 2009, 2010) propose a confidence estimator that relates specifically to the post-editing effort of translators.",http://www.aclweb.org/anthology/N/N13/N13-3003.pdf,"(Specia, 2011)","This research uses regression on both the automatic scores assigned to the MT and scores assigned by posteditors and aims to model post-editors' judgements of the translation quality between good and bad, or among three levels of post-editing effort.","MT confidence estimation and its relation to existing TM scoring methods, together with how to make the most effective use of both technologies, is an active area of research.",
1157166711,false,finalized,3,2/12/2017 18:19:03,Pos,0.3467,N13-3003,"(Koehn and Haddow, 2009) presents a post-editing environment using information from the phrasebased SMT system Moses.2 (Guerberof, 2009) compares the post-editing effort required for TM and MT output, respectively.",http://www.aclweb.org/anthology/N/N13/N13-3003.pdf,"(Koehn and Haddow, 2009)","(Tatsumi, 2009) studies the correlation between automatic evaluation scores and post-editing effort.","Related research by (Simard and Isabelle., 2009) focuses on combining TM information into an SMT system for improving the performance of the MT when a close match already exists within the TM.",
1157166712,false,finalized,3,2/12/2017 18:11:08,Neut,1.0,N13-3003,"The SVM classifier outputs class labels and the class labels are converted into confidence scores using the techniques given in (Lin et al., 2007).",http://www.aclweb.org/anthology/N/N13/N13-3003.pdf,"(Lin et al., 2007)",Relying on system independent black-box features has allowed us to build,"System-independent features for each translation output are fed as input to the SVM classifier (Cortes and Vapnik, 1995).",
1157166713,false,finalized,3,2/12/2017 18:14:35,Neut,0.6872,D10-1091,"Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004).",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf,"(Germann, 2003)","Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics.",Translating a sentence amounts to finding the best scoring translation hypothesis in the search space.,
1157166714,false,finalized,3,2/12/2017 19:07:58,Pos,0.667,D10-1091,"To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS.",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf,"(Dreyer et al., 2007",We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure.,1.3 Contribution and organization,
1157166715,false,finalized,3,2/12/2017 18:07:36,Neut,1.0,D10-1091,"Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009).",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf,"Koehn, 2007)",We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3).,934,
1157166716,false,finalized,3,2/12/2017 18:20:03,Neut,0.6585,D10-1091,"This measure is closely related to the BLEU-1 evaluation metric and the Meteor (Banerjee and Lavie, 2005) metric (when it is evaluated without considering near-matches and the distortion penalty).",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf,"(Banerjee and Lavie, 2005)",We also believe that hypotheses that maximize the unigram precision and recall at the sentence level yield corpus level BLEU-4 scores close the maximal achievable.,"This amounts to evaluating translation quality in terms of unigram precision and recall, which are highly correlated with human judgements (Lavie et al., ).",
1157166717,false,finalized,3,2/12/2017 18:38:59,Neut,0.7009,D10-1091,"ILP has already been used in SMT to find the optimal translation for word-based (Germann et al., 2001) and to study the complexity of learning phrase alignments (De Nero and Klein, 2008) models.",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf,"(Germann et al., 2001)","Following the latter reference, we introduce the following variables: fi,j (resp.","To solve the combinatorial problem introduced in the previous section, we propose to cast it into an Integer Linear Programming (ILP) problem, for which many generic solvers exist.",
1157166718,false,finalized,3,2/12/2017 18:15:54,Pos,0.7009,D10-1091,"Implementing the ""local"" or MJ-d (Kumar and Byrne, 2005) reordering strategy is also straightforward, and implies using the following constraints:",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf,"(Kumar and Byrne, 2005)","Xai0,j0,k0,l0  k0k","l < m  1, ai,j,k,lai0,j0,l+1,l0  |i0  j  1 |< d",
1157166719,false,finalized,3,2/12/2017 18:08:39,Neut,0.6818,D10-1091,"Oracle decoding has also been used to assess the limitations induced by various reordering constraints in (Dreyer et al., 2007).",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf,"(Dreyer et al., 2007)","To this end, the authors propose to use a beam-search based oracle decoder, which computes lower bounds of the best achievable BLEU-4 using dynamic programming techniques over finite-state (for so-called local and IBM constraints) or hierarchically structured (for ITG constraints) sets of hypotheses.","This is the main motivation of (Tillmann and Zhang, 2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering model.",
1157166720,false,finalized,3,2/12/2017 18:14:35,CoCo,0.6648,D10-1091,"The approach described in (Auli et al., 2009) is very similar to ours: in this study, the authors propose to find and analyze the limits of machine translation systems by studying the reference reachability.",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf,"(Auli et al., 2009)",A reference is reachable for a given system if it can be exactly generated by this system.,"To the best of our knowledge, there are only a few works that try to study the expressive power of phrase-based machine translation systems or to provide tools for analyzing potential causes of failure.",
1157166721,false,finalized,3,2/12/2017 18:18:11,Pos,1.0,D10-1091,"In our experiments, we used the free solver SCIP (Achterberg, 2007).",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf,"(Achterberg, 2007)",An optimal solution was found for all problems we considered.,"Even though ILP problems are NP-hard in general, there exist several off-theshelf ILP solvers able to efficiently find an optimal solution or decide that the problem is infeasible.",
1157166722,false,finalized,3,2/12/2017 18:19:49,Pos,0.6544,D13-1151,"In his comprehensive review of outlier detection methods in textual data, Guthrie (2008) compares a variety of vectorization methods along with a variety of generic outlier methods.",http://www.aclweb.org/anthology/D/D13/D13-1151.pdf,(2008),"The vectorization methods employ a variety of lexical and syntactic stylistic features, while the outlier detection methods use a variety of similarity/distance measures such as cosine and Euclidean distance.","Subsequent work, such as the StahelDonoho Estimator (Stahel, 1981; Donoho, 1982), PCout (Filzmoser et al., 2008), LOF (Breunig and Kriegel, 2000) and ABOD (Kriegel et al., 2008) have generalized univariate methods to highdimensional data points.",
1157166723,false,finalized,3,2/12/2017 18:38:59,Neut,0.7009,D13-1151,"Similar methods have also been used in the field of intrinsic plagiarism detection, which involves segmenting a text and then identifying outlier segments (Stamatatos, 2009; Stein et al., 2010).",http://www.aclweb.org/anthology/D/D13/D13-1151.pdf,"(Stamatatos, 2009",3 Proximity Methods,"The vectorization methods employ a variety of lexical and syntactic stylistic features, while the outlier detection methods use a variety of similarity/distance measures such as cosine and Euclidean distance.",
1157166724,false,finalized,3,2/12/2017 18:08:39,Pos,1.0,N07-1039,"We used an implementation of Brill's (1992) part-of-speech tagger to find adjectives and modifiers; for parsing, we used the Stanford dependency parser (Klein and Manning, 2003).",http://www.aclweb.org/anthology/N/N07/N07-1039.pdf,(1992),6 Evaluating Extraction,"Each document in each corpus was preprocessed into individual sentences, lower-cased, and tokenized.",
1157166725,false,finalized,3,2/12/2017 18:07:44,Neut,0.6774,N07-1039,"In this regard, we will be examining extension of existing methods for automatically building lexicons of positive/negative words (Turney, 2002; Esuli and Sebastiani, 2005) to the more complex task of estimating also attitude type and force.",http://www.aclweb.org/anthology/N/N07/N07-1039.pdf,"(Turney, 2002",,"sions, such as where an attitude is expressed via a noun or a verb.",
1157166726,false,finalized,3,2/12/2017 18:19:03,CoCo,1.0,N07-1039,The idea is similar to Agrawal and Srikant's (1995) notion of generalized association rules.,http://www.aclweb.org/anthology/N/N07/N07-1039.pdf,(1995),"We treat each appraisal expression as a transaction, with the attributes of attitude type, orientation, polarity, force, and thing type, as well as the document attributes product name, product type, and document classification (based on the number of stars the reviewer gave the product).","We generally look for rules that contain attitude type, orientation, thing type, and a product name, when these rules occur more frequently than expected.",
1157166727,false,finalized,3,2/12/2017 18:15:53,Pos,0.697,N07-1039,"For adjectival attitudes, we used the lexicon developed we developed in our previous work (Whitelaw et al., 2005) on appraisal.",http://www.aclweb.org/anthology/N/N07/N07-1039.pdf,"(Whitelaw et al., 2005)",We reviewed the entire lexicon to determine its accuracy and made numerous improvements.,311,
1157166728,false,finalized,3,2/12/2017 18:14:35,Pos,0.6648,N07-1039,"We use CLOSET+ (Wang et al., 2003) to find all of the frequent closed itemsets in the data, with a support greater than or equal to 20 occurrences.",http://www.aclweb.org/anthology/N/N07/N07-1039.pdf,"(Wang et al., 2003)","Let (b, a1, a2,... an) or (b, A) denote the contents of an itemset, and c ((b, A)) denote the support for this itemset.","We treat each appraisal expression as a transaction, with the attributes of attitude type, orientation, polarity, force, and thing type, as well as the document attributes product name, product type, and document classification (based on the number of stars the reviewer gave the product).",
1157166729,false,finalized,3,2/12/2017 18:15:53,Neut,0.6818,W12-2202,"Earlier work already establishes the importance of lexical changes for text simplification (Carroll et al., 1998; Caseli et al., 2009; De Belder et al., 2010).",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf,"(Carroll et al., 1998","Upon examining a parallel corpus consisting of original and manually simplified newspaper articles in Spanish, we have found that by far the most common type of changes applied by human editors are precisely lexical changes, accounting for 17.48% of all annotated operations (Bott and Saggion, 2012a).","The present work, however, deals with lexical simplification and is centred around a corpus analysis, a preparatory stage for the development of a separate lexical module in the future.",
1157166730,false,finalized,3,2/12/2017 18:20:03,Neut,0.6494,W12-2202,"Upon examining a parallel corpus consisting of original and manually simplified newspaper articles in Spanish, we have found that by far the most common type of changes applied by human editors are precisely lexical changes, accounting for 17.48% of all annotated operations (Bott and Saggion, 2012a).",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf,"(Bott and Saggion, 2012a)",Words perceived as more complicated are replaced,"Earlier work already establishes the importance of lexical changes for text simplification (Carroll et al., 1998; Caseli et al., 2009; De Belder et al., 2010).",
1157166731,false,finalized,3,2/12/2017 18:19:49,Neut,0.6774,W12-2202,"The PSET project (Carroll et al., 1998) dealt with simplification of news articles in English for aphasic readers.",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf,"(Carroll et al., 1998)","Together with syntactic analysis and transformations similar to those of Chandrasekar et al (1996), they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency from the Oxford Psycholinguistic Database (Quinlan, 1992).","The earliest simplification systems employed a rule-based approach and focused on syntactic structure of the text (Chandrasekar et al., 1996).",
1157166732,false,finalized,3,2/12/2017 18:49:12,Neut,0.3683,W12-2202,"Bautista et al (2009) also rely on a dictionary of synonyms, but their criterion for choosing the most appropriate one is wordlength rather than frequency.",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf,(2009),"Caseli et al (2009) analyse lexical operations on a parallel corpus of original and manually simplified texts in Portuguese, using lists of simple words and discourse markers as resources.","The above approach to lexical simplification has been repeated in a number of works (Lal and Ruger, 2002; Burstein et al., 2007).",
1157166733,false,finalized,3,2/12/2017 18:11:08,Neut,0.6262,W12-2202,"The automatic alignments have then been manually corrected through a graphical editing tool within the GATE framework (Cunningham et al., 2002).",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf,"(Cunningham et al., 2002)","A total of 570 sentences have been aligned (246 in original and 324 in simple texts), with the following correlations between them: one to one, one to many or many to one, as well as cases where there is no correlation (cases of content reduction through summarisation or information expansion through the introduction of definitions).","aligning algorithm based on Hidden Markov Models (Bott and Saggion, 2011) has been applied to obtain sentence-level alignments.",
1157166735,false,finalized,3,2/12/2017 18:35:28,CoCo,0.6749,W12-2202,"Together with syntactic analysis and transformations similar to those of Chandrasekar et al (1996), they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency from the Oxford Psycholinguistic Database (Quinlan, 1992).",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf,(1996),"Therefore, the most frequent of a set of synonyms for every content word of the input text was chosen to appear in its simplified version.","The PSET project (Carroll et al., 1998) dealt with simplification of news articles in English for aphasic readers.",
1157166736,false,finalized,3,2/12/2017 19:07:58,Neut,1.0,W06-2938,"The treebanks (Hajic et al., 2004; Chen et al., 2003; Bohmova et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair.",http://www.aclweb.org/anthology/W/W06/W06-2938.pdf,"(Hajic et al., 2004",An initial model is built to identify dependency relations between adjacent word pairs using a decision list learning algorithm.,This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques.,
1157166737,false,finalized,3,2/12/2017 18:28:49,Pos,1.0,P00-1059,"For a more detailed discussion of the issue of metrics in evaluation, of the metrics we have used, and of an experiment relating human judgments to these metrics, see (Bangalore et al., 2000).",http://www.aclweb.org/anthology/P/P00/P00-1059.pdf,"(Bangalore et al., 2000)",String accuracy measures the performance of the entire FERGUS system.,"This number is subtracted from and then divided by the length of the sentence, yielding a number between 0 and 1, with 1 the best score.",
1157166738,false,finalized,3,2/13/2017 05:07:17,Neut,0.6292,W00-1004,"A quote from the most recent Switchboard labeling standard (Hamaker et al., 1998) gives the flavor: 20.",http://www.aclweb.org/anthology/W/W00/W00-1004.pdf,"(Hamaker et al., 1998)","Hesitation Sounds: Use ""uh"" or ""oh"" for hesitations consisting of a vowel sound, and ""urn"" or ""hm"" for hesitations with a nasal sound, depending upon which transcription the actual sound is closest to.",An alternative approach is seen in some schemes used for labeling corpora for purposes of training and evaluating speech recognizers.,
1157166739,false,finalized,3,2/13/2017 04:57:38,Neut,1.0,W00-1004,"21: yes/no sounds: Use ""uh-huh"" or ""um-hum"" (yes) and ""huh-uh"" or ""hum-um"" (no) for anything remotely resembling these sounds of assent or denial"" Another scheme (Lander, 1996) lists several ""miscellaneous words"", including:",http://www.aclweb.org/anthology/W/W00/W00-1004.pdf,"(Lander, 1996)",30,"<other speaker responds> urn ok, I see your point.""",
1157166740,false,finalized,3,2/13/2017 05:05:39,Neut,0.6891,W00-1004,"While it is not possible to devise a single transcription scheme which is perfect for all purposes (Barry and Fourcin, 1992), it is clear that the current schemes all have room for improvement.",http://www.aclweb.org/anthology/W/W00/W00-1004.pdf,"(Barry and Fourcin, 1992)",3 Proposal,"2. can represent all observed grunts, and 3. unambiguously represents all meaningful differences in sound.",
1157166741,false,finalized,3,2/13/2017 05:07:04,Neut,0.6766,D13-1021,"Htun et al (2012) extended the many-to-many alignment for the sample-wise transliteration mining, but its noise model only handles the sample-wise noise and cannot distinguish partial noise.",http://www.aclweb.org/anthology/D/D13/D13-1021.pdf,(2012),We model partial noise in the CRP-based joint substring model.,We introduce a noise symbol to handle partial noise in the many-to-many alignment model.,
1157166742,false,finalized,3,2/13/2017 04:51:52,Neut,1.0,D13-1021,"phrases in statistical machine transliteration and improved transliteration performance (Finch and Sumita, 2010).",http://www.aclweb.org/anthology/D/D13/D13-1021.pdf,"(Finch and Sumita, 2010)","We extracted them by: 1) generate many-to-many word alignment, in which all possible word alignment links in many-to-many correspondences (e.g., 0-0 0-1 0-2 1-0 1-1 1-2 for  , c o m), 2) run phrase extraction and scoring same as a standard Moses training.","BASELINE 104,563 899,080 1,372,993 PROPOSED 104,561 893,366 1,317,256",
1157166743,false,finalized,3,2/13/2017 05:00:13,Neut,1.0,P06-2061,"A desired feature of computer-assisted translation (CAT) systems is the integration of the human speech into the system, as skilled human translators are faster at dictating than typing the translations (Brown et al., 1994).",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf,"(Brown et al., 1994)","Additionally, incorporation of a statistical prediction engine, i.e.",1 Introduction,
1157166744,false,finalized,3,2/13/2017 04:54:08,Neut,1.0,P06-2061,"A statistical prediction engine provides the completions to what a human translator types (Foster et al., 1997; Och et al., 2003).",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf,"(Foster et al., 1997","Then, one possible procedure for skilled human translators is to provide the oral translation of a given source text and then to post-edit the recognized text.","a statistical interactive machine translation system, to the CAT system is another useful feature.",
1157166745,false,finalized,3,2/13/2017 05:09:33,Neut,0.676,P06-2061,"In the post-editing step, a prediction engine helps to decrease the amount of human interaction (Och et al., 2003).",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf,"(Och et al., 2003)","In a CAT system with integrated speech, two sources of information are available to recognize the speech input: the target language speech and the given source language text.","Then, one possible procedure for skilled human translators is to provide the oral translation of a given source text and then to post-edit the recognized text.",
1157166746,false,finalized,3,2/13/2017 05:01:10,Neut,0.6811,P06-2061,"For instance, the resulting word graph can be used in the prediction engine of a CAT system (Och et al., 2003).",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf,"(Och et al., 2003)","The remaining part is structured as follows: in Section 2, a general model for an automatic text dictation system in the computer-assisted translation framework will be described.","Thus, the integration of MT models to ASR word graphs can be considered as an N-best rescoring but with very large value for N. Another advantage of working with ASR word graphs is the capability to pass on the word graphs for further processing.",
1157166747,false,finalized,3,2/13/2017 05:03:00,Pos,0.6628,P06-2061,"We rescore the ASR N-best lists with the standard HMM (Vogel et al., 1996) and IBM (Brown et al., 1993) MT models.",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf,"(Vogel et al., 1996)","The development and evaluation sets Nbest lists sizes are sufficiently large to achieve almost the best possible results, on average 1738 hypotheses per each source sentence are extracted from the ASR word graphs.","better MT system, and generating a larger N-best list from the ASR word graphs.",
1157166748,false,finalized,3,2/13/2017 05:05:39,Pos,1.0,P06-2061,"To rescore the N-best lists, we use the method of (Khadivi et al., 2005).",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf,"(Khadivi et al., 2005)","But the results shown here are different from that work due to a better optimization of the overall ASR system, using a",4.2 N-best Rescoring,
1157166749,false,finalized,3,2/13/2017 05:07:17,Pos,1.0,P06-2061,"To limit the permutations, we used an approach as in (Kanthak et al., 2005).",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf,"(Kanthak et al., 2005)",Each of these two acceptors results in different constraints for the generation of the hypotheses.,"a sequence of nodes with one incoming arc and one outgoing arc, the words of source language text are placed consecutively in the arcs of the acceptor, 2. an acceptor containing possible permutations.",
1157166750,false,finalized,3,2/13/2017 04:51:52,CoCo,0.6991,W13-2241,"Specifically, we want to employ our feature set in a multi-task kernel setting, similar to the one proposed by Cohn and Specia (2013).",http://www.aclweb.org/anthology/W/W13/W13-2241.pdf,(2013),"These kernels have the power to model inter-annotator variance and noise, which can lead to better results in the prediction of post-editing time.","In the future, we plan to further investigate these models by devising more advanced kernels and feature selection methods.",
1157166751,false,finalized,3,2/13/2017 05:09:33,Pos,0.6868,W13-2241,"For task 1.3, we used a subset of the 80 most general BB features as in (Shah et al., 2013), for which we had all the necessary resources available for the extraction.",http://www.aclweb.org/anthology/W/W13/W13-2241.pdf,"(Shah et al., 2013)","We selected the top 25 features for both models, based on empirical results found by Shah et al (2013) for a number of datasets, and then retrained the GP using only the selected features.","For task 1.1, we performed this feature selection over all 160 features mentioned in Section 2.",
1157166752,false,finalized,3,2/13/2017 05:36:53,Pos,1.0,W13-2241,"As a measure of informativeness we used Information Density (ID) (Settles and Craven, 2008).",http://www.aclweb.org/anthology/W/W13/W13-2241.pdf,"(Settles and Craven, 2008)",This measure leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is:,"We optimised the Gaussian Process hyperparameters every 20 new instances, for both tasks.",
1157166753,false,finalized,3,2/13/2017 05:29:36,Pos,1.0,W13-2241,"In this work, we used the confidence method proposed by Vlachos (2008).",http://www.aclweb.org/anthology/W/W13/W13-2241.pdf,(2008),This is an method that measures the model's confidence on a held-out non-annotated dataset every time a new instance is added to the training set and stops the AL procedure when this confidence starts to drop.,"In a real annotation setting, it is important to decide when to stop adding new instances to the training set.",
1157166754,false,finalized,3,2/13/2017 04:57:08,Pos,0.6828,W13-2241,"While a variety of kernel functions are available, here we followed previous work on QE using GP (Cohn and Specia, 2013; Shah et al., 2013) and employed a squared exponential (SE) kernel with automatic relevance determination (ARD): where F is the number of features, 2f is the covariance magnitude and li > 0 are the feature length scales.",http://www.aclweb.org/anthology/W/W13/W13-2241.pdf,"(Cohn and Specia, 2013","The resulting model hyperparameters (SE variance 2f, noise variance 2n and SE length scales li) were learned from data by maximising the model likelihood.",The kernel function encodes the covariance (similarity) between each input pair.,
1157166755,false,finalized,3,2/13/2017 04:54:08,Pos,1.0,W13-2241,"To perform feature selection, we followed the approach used in Shah et al (2013) and ranked the features according to their learned length scales (from the lowest to the highest).",http://www.aclweb.org/anthology/W/W13/W13-2241.pdf,(2013),The length scales of a feature can be interpreted as the relevance of such feature for the model.,"To avoid poor hyperparameter values due to this, we performed a two-step procedure where we first optimise a model with all the SE length scales tied to the same value (which is equivalent to an isotropic model) and we used the resulting values as starting point for the ARD optimisation.",
1157166756,false,finalized,3,2/13/2017 05:03:00,Neut,1.0,N04-4019,1 shows a sample Speech Graffiti dialog User interactions with Speech Graffiti (independent of other speech interfaces) have previously been assessed in Rosenfeld et al (2000).,http://www.aclweb.org/anthology/N/N04/N04-4019.pdf,(2000),"Here we consider a head-to-head comparison: given the chance to interact with both types of interfaces, which would people choose?",Fig.,
1157166757,false,finalized,3,2/13/2017 05:05:46,Neut,1.0,N04-4019,"Guindon & Shuldberg, 1987; Ringle & Halstead-Nussloch, 1989; Sidner & Forlines, 2002).",http://www.aclweb.org/anthology/N/N04/N04-4019.pdf,"Guindon & Shuldberg, 1987","As far as we know, no studies have been done comparing constrained, ""universal"" languages and natural language interfaces directly as we have done in this study.",Several studies have previously found that users are able to interact successfully using constrained or subset languages (e.g.,
1157166758,false,finalized,3,2/13/2017 05:04:30,Neut,1.0,N04-4019,General information about the Speech Graffiti project and its motivation can be found in Rosenfeld et al (2001).,http://www.aclweb.org/anthology/N/N04/N04-4019.pdf,(2001),2 Method,"As far as we know, no studies have been done comparing constrained, ""universal"" languages and natural language interfaces directly as we have done in this study.",
1157166759,false,finalized,3,2/13/2017 04:51:09,Neut,0.6494,W15-4923,"MEANT, Lo and Wu (2011)), as we go beyond a ""flat"" n-gram matching but evaluate a meaningful entity, in our case a preposition-noun-verb triple.",http://www.aclweb.org/anthology/W/W15/W15-4923.pdf,(2011),3 Methodology,"Conceptually, this is loosely related to semantically focused metrics (e.g.",
1157166760,false,finalized,3,2/13/2017 05:10:34,CoCo,0.6818,W15-4923,"The presented work is similar to that of Agirre et al (2009), but is applied to a fully statistical MT system.",http://www.aclweb.org/anthology/W/W15/W15-4923.pdf,(2009),"The main difference is that Agirre et al (2009) use linguistic information to select appropriate translation rules, whereas we generate prepositions in a post-processing step.",Weller et al (2014) use noun class information as tree labels in syntactic SMT to model selectional preferences of prepositions.,
1157166761,false,finalized,3,2/13/2017 05:36:53,Pos,0.6371,W15-4923,"Our approach is integrated into an English-German morphology-aware SMT system which first translates into a lemmatized representation with a component to generate fully inflected forms in a second step, an approach similar to the work by Toutanova et al (2008) and Fraser et al (2012).",http://www.aclweb.org/anthology/W/W15/W15-4923.pdf,(2008),"The inflection requires the modeling of the grammatical case of noun phrases (among other features), which corresponds to determining the syntactic function1.",3 Methodology,
1157166762,false,finalized,3,2/13/2017 05:29:36,Neut,0.6811,W14-3611,"(Milne et al., 2007) proposed a system called ""KORU"" for query expansion using Wikipedia's most relevant articles to user's query.",http://www.aclweb.org/anthology/W/W14/W14-3611.pdf,"(Milne et al., 2007)",The system allows the user to refine the set of Wikipedia pages to be used for expansion.,Synonyms from WordNet are used to expand the question in order to extract the most semantically relevant passages to the question.,
1157166763,false,finalized,3,2/13/2017 05:07:21,Neut,1.0,P15-1061,"Like some other ranking approaches that only update two classes/examples at every training round (Weston et al., 2011; Gao et al., 2014), we can efficiently train the network for tasks which have a very large number of classes.",http://www.aclweb.org/anthology/P/P15/P15-1061.pdf,"(Weston et al., 2011",This is an advantage over softmax classifiers.,We use stochastic gradient descent (SGD) to minimize the loss function with respect to .,
1157166764,false,finalized,3,2/13/2017 05:04:44,Neut,1.0,P15-1061,"Over the years, various approaches have been proposed for relation classification (Zhang, 2004; Qian et al., 2009; Hendrickx et al., 2010; Rink and Harabagiu, 2010).",http://www.aclweb.org/anthology/P/P15/P15-1061.pdf,"(Zhang, 2004",Most of them treat it as a multiclass classification problem and apply a variety of machine learning techniques to the task in order to achieve a high accuracy.,5 Related Work,
1157166765,false,finalized,3,2/13/2017 05:30:56,CoCo,0.6816,P15-1061,These features are similar to the position features proposed by Collobert et al (2011) for the Semantic Role Labeling task.,http://www.aclweb.org/anthology/P/P15/P15-1061.pdf,(2011),In this work we also experiment with the word position embeddings (WPE) proposed by Zeng et al (2014).,Zeng et al (2014) propose the use of word position embeddings (position features) which help the CNN by keeping track of how close words are to the target nouns.,
1157166766,false,finalized,3,2/13/2017 05:02:41,CoCo,0.6811,P15-1061,This approach is similar to the one used by Weston et al (2014) to select negative examples.,http://www.aclweb.org/anthology/P/P15/P15-1061.pdf,(2014),We use the backpropagation algorithm to compute gradients of the network.,"For tasks where the number of classes is large, we can fix a number of negative classes to be considered at each example and select the one with the largest score to perform a gradient step.",
1157166767,false,finalized,3,2/13/2017 05:29:36,Pos,1.0,P15-1061,The third line in Table 4 shows the result reported by Zeng et al (2014) when only word embeddings and WPEs are used as input to the network (similar to our CNN+Softmax).,http://www.aclweb.org/anthology/P/P15/P15-1061.pdf,(2014),We believe that the word embeddings employed by them is the main reason their result is much worse than that of CNN+Softmax.,"CR-CNN outperforms CNN+Softmax in both precision and recall, and improves the F1 by 1.6.",
1157166768,false,finalized,3,2/13/2017 05:00:13,Neut,1.0,W08-0912,"Other assessments, such as the TOEFL Practice Online Speaking test, on the other hand, focus on more spontaneous, high-entropy responses (Zechner et al., 2007).",http://www.aclweb.org/anthology/W/W08/W08-0912.pdf,"(Zechner et al., 2007)","In this paper, we describe a spoken language test with heterogeneous task types, ranging from read speech to tasks that require candidates to give their opinions on an issue, whose goal is to assess communicative competence (Bachman, 1990; Bachman & Palmer, 1996); we call this test THT (Test with Heterogeneous Tasks).","Entropy in this context can be seen as a measure for how predictable the language in the expected spoken response is: Some tests, such as SET-10 (Bernstein 1999), are focused mostly on the lower entropy aspects of language, using tasks such as ""reading"" or ""repetition"", where the expected sequence of words is highly predictable.",
1157166769,false,finalized,3,2/13/2017 05:01:20,Pos,0.6164,W08-0912,"It is shown that the SET-10 test scores can predict different levels on the Oral Interaction Scale of the Council of Europe's Framework (North, 2000) for describing oral proficiency of second/foreign language speakers with reasonable accuracy.",http://www.aclweb.org/anthology/W/W08/W08-0912.pdf,"(North, 2000)",This paper further reports on studies done to correlate the SET-10 automated scores with the human scores from two other tests of oral English communication skills.,"In Bernstein et al (2000), an experiment is performed to investigate the performance of the SET-10 test in predicting speakers' oral proficiency.",
1157166770,false,finalized,3,2/13/2017 04:54:08,Neut,1.0,W08-0912,"The nonnative data came from the TOEFL Practice Online system, a web-based practice program for prospective takers of the Test Of English as a Foreign Language (TOEFL) (Zechner et al., 2007).",http://www.aclweb.org/anthology/W/W08/W08-0912.pdf,"(Zechner et al., 2007)","This data is somewhat different from the THT, as there are only high-entropy tasks in TOEFL Speaking and as the speakers are generally more proficient.","For this work, we are using a state-of-the-art gender-independent Hidden Markov Model speech recognizer whose acoustic model was trained on about 30 hours of non-native speech and whose language model was built on several hundred hours of both native and non-native speech.",
1157166771,false,finalized,3,2/13/2017 05:02:41,Pos,0.6884,W08-0912,"We used unsupervised maximum likelihood linear regression (MLLR) AM adaptation on top of the previous adaptation and optimization steps (Tomokiyo and Waibel, 2001; Wang et al., 2003).",http://www.aclweb.org/anthology/W/W08/W08-0912.pdf,"(Tomokiyo and Waibel, 2001","In this step, all words whose confidence score was higher than a pre-set threshold were collected and their acoustic information was used to adapt the acoustic model.",4.4 Unsupervised speaker adaptation,
1157166772,false,finalized,3,2/13/2017 04:54:08,Weak,0.6494,W14-4719,"Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations.",http://www.aclweb.org/anthology/W/W14/W14-4719.pdf,"(Papineni et al., 2002)","Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy.","Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics.",
1157166773,false,finalized,3,2/13/2017 05:16:43,Neut,0.6339,W14-4719,"To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider four alternative approaches to defining the lexical similarities si,pred and si,j, all of which employ a standard context vector model of the individual words/tokens in the multiword expression arguments between the reference and machine translations, as descibed by Lo et al (2012) and Tumuluru et al (2012).",http://www.aclweb.org/anthology/W/W14/W14-4719.pdf,(2012),3.1 Bag of words (geometric mean),3 Comparison of multiword expression association approaches,
1157166774,false,finalized,3,2/13/2017 05:00:13,Pos,1.0,W14-4719,"In the first maximum alignment based approach we will consider, the definitions of sz,pred and sz, are inspired by Mihalcea et al (2006) who normalize phrasal similarities according to the phrase length.",http://www.aclweb.org/anthology/W/W14/W14-4719.pdf,(2006),"1 Si,pred = 2 1precei,pred,fi,pred + recei,pred,fi,pred 1 =2 1precei,j,fi,j + recei,j ,fi,j",3.2 Maximum alignment (precision-recall average),
1157166775,false,finalized,3,2/13/2017 04:57:04,Pos,0.6868,W14-4719,"We computed sentence-level correlations following the benchmark assessment procedure used by WMT and NIST MetricsMaTr (Callison-Burch et al., 2008, 2010, 2011, 2012; and Bojar, 2013), which use T correlation coefficient, to evaluate the correlation of evaluation metrics against human judgment on ranking the translation adequacy of the three output.",http://www.aclweb.org/anthology/W/W14/W14-4719.pdf,"(Callison-Burch et al., 2008, 2010, 2011, 2012",A higher value for T indicates more similarity to the human adequacy rankings by the evaluation metrics.,output.,
1157166776,false,finalized,3,2/13/2017 05:01:20,Neut,0.7146,W14-4719,"The range of possible values of T correlation coefficient is 1], where 1 mean (2011a).",http://www.aclweb.org/anthology/W/W14/W14-4719.pdf,(2011a),"state-of-the-art systems' Machacek Kendall's systems' Kendall's Kendall's [-1, s the",A higher value for T indicates more similarity to the human adequacy rankings by the evaluation metrics.,
1157166777,false,finalized,3,2/13/2017 04:57:04,Neut,1.0,W14-4719,"Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al (2012) showed that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure.",http://www.aclweb.org/anthology/W/W14/W14-4719.pdf,(2012),5 Conclusion,"The ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave tokens unaligned instead of aligning them anyway as the other strategies tend to do.",
1157166778,false,finalized,3,2/13/2017 05:03:40,Pos,1.0,W14-4719,"To calculate the ( ) inside probability (or more accurately, inside score) of a pair of segments, P A   e/f|G , we use the algorithm described in Saers et al (2009).",http://www.aclweb.org/anthology/W/W14/W14-4719.pdf,(2009),"Given this, si,pred and si, now represent the length normalized BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and machine translations.","The rule probability (or more accurately, rule weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is defined by MEANT's lexical similarity measure on English Gigaword context vectors.",
1157166779,false,finalized,3,2/13/2017 05:04:39,Neut,0.6679,W10-1609,"The objective of the Recognizing Textual Entailment Challenge is determining whether the meaning of the Hypothesis (H) can be inferred from a text (T) (Ido Dagan et al., 2006).",http://www.aclweb.org/anthology/W/W10/W10-1609.pdf,"(Ido Dagan et al., 2006)",This challenge has been organized by NIST in recent years.,1 Introduction,
1157166780,false,finalized,3,2/13/2017 05:03:00,Pos,0.6628,W10-1609,"We used four classifiers to learn every development set: (1) Support Vector Machine, (2) Ada Boost, (3) Multilayer Perceptron (MLP) and (4) Decision Tree using the open source WEKA Data Mining Software (Witten & Frank, 2005).",http://www.aclweb.org/anthology/W/W10/W10-1609.pdf,"(Witten & Frank, 2005)",In all the tables results we show only the accuracy of the best classifier.,"The training set SPARTE-Balanced was created by taking all true cases and randomly taking false cases, and then we build a balanced training set containing 1352 pairs, with 676 true and 676 false pairs.",
1157166781,false,finalized,3,2/13/2017 05:03:00,Neut,0.6628,P13-2128,"Syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces (Baroni and Lenci, 2010).",http://www.aclweb.org/anthology/P/P13/P13-2128.pdf,"(Baroni and Lenci, 2010)","However, they are also sparse, with resulting reliability and coverage problems.",Abstract,
1157166782,false,finalized,3,2/13/2017 05:09:33,Pos,1.0,P13-2128,This makes them more versatile; the Distributional Memory framework by Baroni and Lenci (2010) is applicable to a wide range of tasks.,http://www.aclweb.org/anthology/P/P13/P13-2128.pdf,(2010),"It is also able - at least in principle - to capture more fine-grained types of semantic similarity such as predicateargument plausibility (Erk et al., 2010).","The advantage of syntactic models is that they incorporate a richer, structured notion of context.",
1157166783,false,finalized,3,2/13/2017 04:51:09,Neut,1.0,P13-2128,"Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003).",http://www.aclweb.org/anthology/P/P13/P13-2128.pdf,"(Voorhees, 1994","In lexical semantics, smoothing is often achieved by backing","Many of the methods were first applied in Language Modeling to deal with unseen n-grams (Chen and Goodman, 1999; Dagan et al., 1999).",
1157166784,false,finalized,3,2/13/2017 05:07:21,Neut,1.0,P13-2128,"We lemmatized and POS-tagged the German GUR350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict",http://www.aclweb.org/anthology/P/P13/P13-2128.pdf,"(Zesch et al., 2007)",2Downloadable from: http://goo.gl/bFokI,The first task is predicting semantic similarity.,
1157166785,false,finalized,3,2/13/2017 04:57:38,Pos,1.0,P13-2128,"We use version 1.3 of DERIVBASE (Zeller et al., 2013),1 a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 nonsingleton derivational families.",http://www.aclweb.org/anthology/P/P13/P13-2128.pdf,"(Zeller et al., 2013)",It has a precision of 84% and a recall of 71%.,"For German, there are several resources with derivational information.",
1157166786,false,finalized,3,2/13/2017 04:59:12,Pos,0.6628,P13-2128,"More specifically, we use the W x LW matricization of DM.DE, the German version (Pado and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010).",http://www.aclweb.org/anthology/P/P13/P13-2128.pdf,"(Pado and Utt, 2012)","DM.DE was created on the basis of the 884M-token SDEWAC web corpus (Faa et al., 2010), lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010).",The syntactic distributional model that we use represents target words by pairs of dependency relations and context words.,
1157166787,false,finalized,3,2/13/2017 04:57:08,Pos,1.0,P13-2128,"For synonym choice, we follow the method established by Mohammad et al (2007), measuring accuracy over covered items, with partial credit for ties.",http://www.aclweb.org/anthology/P/P13/P13-2128.pdf,(2007),Results for Semantic Similarity.,"We measure quality of the semantic similarity task as the Pearson correlation between the model predictions and the human judgments for covered items (Zesch et al., 2007).",
1157166788,false,finalized,3,2/13/2017 06:26:02,CoCo,0.6869,W15-2610,"Like the SENNA model, the Skip-gram model (Mikolov et al., 2013) is trained to differentiate between the correct central word of a phrase and a random replacement, which they refer to as negative sampling.",http://www.aclweb.org/anthology/W/W15/W15-2610.pdf,"(Mikolov et al., 2013)","Unlike SENNA, however, the Skipgram model tries to make this prediction using only a single one of the surrounding words at a time and ignores the ordering of those words, i.e.",2.3 Skip-gram,
1157166789,false,finalized,3,2/13/2017 05:30:56,Neut,1.0,W15-2610,"Klein and Manning (2003b), for example, show that the performance of an unlexicalised model can be substantially improved by splitting the existing symbols down into finer categories.",http://www.aclweb.org/anthology/W/W15/W15-2610.pdf,(2003b),Their subcategorizations were developed by hand based on linguistic intuitions and a careful error analysis.,"While an unlexicalized parser that uses syntactic categories based solely on the symbols found in the Penn Treebank will generally perform poorly, a number of results show that refining these categories can substantially improve performance.",
1157166790,false,finalized,3,2/13/2017 05:07:21,Pos,0.6317,W15-2610,"The ngram contexts achieved the best F-Scores fairly consistently for all parsers, vindicating our appeal to the psycholinguistic research of Cartwright and Brent (1997), Mintz (2003) and Redington et al (1998).",http://www.aclweb.org/anthology/W/W15/W15-2610.pdf,(1997),"Turning now to each parser individually, the baseline performance of the Berkeley Parser proved difficult to exceed, with only the 2gram distributional contexts giving any improvement.","Surprisingly, the Skip-gram model retrained on biomedical data (SG-bio) fared worse than the original (SG-news), due probably in large part to the fact that the original training data was almost 100 times larger than our 1.2B word corpus.",
1157166791,false,finalized,3,2/13/2017 05:12:35,Pos,0.6918,W15-2610,"KNN algorithms are also commonly used in Graph-Based Semi-Supervised Learning approaches (Das and Petrov, 2011; Altun et al., 2006; Subramanya et al., 2010), with the knearest-neighbour sets determining the edges that structure the graph.",http://www.aclweb.org/anthology/W/W15/W15-2610.pdf,"(Das and Petrov, 2011",POS tags are then propagated through the graph from labelled to unlabelled data.,"Thus, these clusters engender a form of distributional similarity comparable to that used in our KNN algorithm.",
1157166792,false,finalized,3,2/13/2017 05:01:20,Neut,0.7146,W15-2610,"The Berkeley Parser4 (Petrov et al., 2006), in contrast, is based on a method for automatically finding useful subcategorizations during training by splitting and merging the original nodes.",http://www.aclweb.org/anthology/W/W15/W15-2610.pdf,"(Petrov et al., 2006)","The model is an unlexicalized generative PCFG, but the granularity of the terminal and nonterminal categories found in training give it a much greater sensitivity to the syntactic behaviour of words and phrases than is possible using standard POS tags.",Their subcategorizations were developed by hand based on linguistic intuitions and a careful error analysis.,
1157166793,false,finalized,3,2/13/2017 05:05:39,CoCo,0.6818,W15-2610,"Although similarity in these cases is commonly being assessed between token sequences, as opposed to word types, the features used are similar to the ngram templates used here and the bigram distributions used by Koo et al (2008).",http://www.aclweb.org/anthology/W/W15/W15-2610.pdf,(2008),A major difference in our approach is that it does not require retraining the parser or constructing a full model on the unlabelled data.,POS tags are then propagated through the graph from labelled to unlabelled data.,
1157166794,false,finalized,3,2/13/2017 05:10:34,Pos,1.0,W13-0302,"We use a large-coverage syntactic parser for French, FRMG (FRench MetaGrammar) (De La Clergerie et al., 2009).",http://www.aclweb.org/anthology/W/W13/W13-0302.pdf,"(De La Clergerie et al., 2009)",The main syntactic contexts in which semantic clues can occur are as follows:,"In this section, we address the issue of defining the boundaries of E_M segments, using a syntactic parser.",
1157166795,false,finalized,3,2/13/2017 04:51:52,Pos,0.6991,W02-0312,"This has motivated the development of semantic re-rendering algorithms, designed to enrich the ontological specificity of a domain-specific semantic tag set, based on inductive techniques described in (Pustejovsky et al., 2002c).",http://www.aclweb.org/anthology/W/W02/W02-0312.pdf,"(Pustejovsky et al., 2002c)",3.4 Rerendering Semantic Ontologies,"'protein', rather than 'Amino Acid, Peptid, or Protein).",
1157166796,false,finalized,3,2/13/2017 04:57:08,Neut,0.6325,W05-0623,"We find the exact top N consistent' most likely local model labelings using a simple dynamic program described in (Toutanova et al., 2005).",http://www.aclweb.org/anthology/W/W05/W05-0623.pdf,"(Toutanova et al., 2005)",'A labeling is consistent if satisfies the constraint that argument phrases do not overlap.,The model is trained to re-rank a set of N likely labelings according to the local model.,
1157166797,false,finalized,3,2/13/2017 05:16:43,Neut,0.6822,W05-0623,"The improvement achieved by the joint model relative to the local model is about 2 points absolute in F-Measure, similar to the improvement when gold-standard syntactic parses are used (Toutanova et al., 2005).",http://www.aclweb.org/anthology/W/W05/W05-0623.pdf,"(Toutanova et al., 2005)","The relative error reduction is much lower for automatic parses, possibly due to a lower upper bound on performance.","The percentage of perfectly labeled propositions for the three sets is 55.11% (development), 56.52% (test), and 37.06% (Brown test).",
1157166798,false,finalized,3,2/13/2017 05:10:36,Pos,1.0,W05-0623,"Most of the features we use are described in more detail in (Toutanova et al., 2005).",http://www.aclweb.org/anthology/W/W05/W05-0623.pdf,"(Toutanova et al., 2005)",Here we briefly describe these features and introduce several new joint features (denoted by *).,Figure 2: An example tree with semantic role annotations.,
1157166799,false,finalized,3,2/13/2017 05:07:04,Pos,1.0,W05-0623,"To overcome this problem, we adopt a discriminative re-ranking approach reminiscent of (Collins, 2000).",http://www.aclweb.org/anthology/W/W05/W05-0623.pdf,"(Collins, 2000)","We use a local model, which labels arguments independently, to generate a smaller number of likely joint labelings.","Since the space of possible joint labelings is exponential in the number of parse tree nodes, a model cannot exhaustively consider these labelings unless it makes strong independence assumptions.",
1157166800,false,finalized,3,2/13/2017 05:36:53,Pos,0.6371,D15-1154,"A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high.",http://www.aclweb.org/anthology/D/D15/D15-1154.pdf,"(McDonald, 2006)","On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004).","However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a).",
1157166801,false,finalized,3,2/13/2017 05:04:44,Pos,0.6691,D15-1154,"On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004).",http://www.aclweb.org/anthology/D/D15/D15-1154.pdf,"(Nivre and Scholz, 2004)",Previous studies explored the trade-off between computational costs and parsing performance.,"A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high.",
1157166802,false,finalized,3,2/13/2017 05:04:39,Pos,1.0,D15-1154,The system of McDonald et al (2006) achieved the best average parsing performance over 13 languages (excluding English) in CoNLL-2006 shared tasks.,http://www.aclweb.org/anthology/D/D15/D15-1154.pdf,(2006),"Its average UAS and LAS are 87.03% and 80.83%, respectively, while our average UAS and LAS excluding English are 87.79% and 81.29%.",different languages.,
1157166803,false,finalized,3,2/13/2017 05:03:40,Pos,0.6891,D15-1154,"In this paper, we adopt the second-order sibling factorization (Eisner, 1996; McDonald and",http://www.aclweb.org/anthology/D/D15/D15-1154.pdf,"(Eisner, 1996",1322,Here y(x) denotes the set of possible dependency trees for sentence x.,
1157166804,false,finalized,3,2/13/2017 05:10:04,CoCo,0.6305,W15-2518,"While this distinction has long been acknowledged in text classification literature (Lee, 2001; Dewdney et al., 2001; Lee and Myaeng, 2002), most work on domain adaptation in SMT uses in-domain and out-of-domain data that differs on both the topic and the genre level (e.g., Europarl political proceedings (Koehn, 2005) versus EMEA medical text (Tiedemann, 2009)), making it unclear whether the proposed solutions address topic or genre differences.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,"(Lee, 2001","In this work, we follow text classification literature for definitions of the concepts topic and genre.","Second, the commonly used notion of a domain neglects the fact that topic and genre are two distinct properties of text (Stein and Meyer Zu Eissen, 2006).",
1157166805,false,finalized,3,2/13/2017 05:07:17,Neut,0.6891,W15-2518,"First, mixture modeling approaches learn models from different subcorpora and interpolate these linearly (Foster and Kuhn, 2007) or log-linearly (Koehn and Schroeder, 2007).",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,"(Foster and Kuhn, 2007)","Sennrich (2012) enhances the approach by interpolating up to ten models, and Bertoldi and Federico (2009) use in-domain monolingual data to automatically generate in-domain bilingual data.","Most existing domain adaptation approaches can be grouped into two categories, depending on where in the SMT pipeline they adapt the system.",
1157166806,false,finalized,3,2/13/2017 05:16:43,Neut,1.0,W15-2518,"In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011).",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,"(Moore and Lewis, 2010","In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations.","Second, instance weighting methods prioritize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013).",
1157166807,false,finalized,3,2/13/2017 05:10:36,Neut,0.6756,W15-2518,"In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003).",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,"(Eidelman et al., 2012","Surprisingly, genre (or style) adaptation has only been addressed to a limited extent (Bisazza and Federico, 2012; Wang et al., 2012), with methods requiring the availability of clearly separable in-domain and out-of-domain training corpora.","In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations.",
1157166808,false,finalized,3,2/13/2017 05:04:44,Neut,1.0,W15-2518,"Kessler et al (1997) categorize four types of genre-revealing cues: structural cues (e.g., part-of-speech (POS) tag counts), lexical cues (specific words), character-level cues (e.g., punctuation marks), and derivative cues (ratios and variation measures based on other types of cues).",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,(1997),"Dewdney et al (2001) compare a large number of document features and show that these outperform bag-of-words approaches, which are traditionally used in topic-based text classifica","Karlgren and Cutting (1994) were among the first to use simple document statistics, such as common word frequencies, first-person pronoun count, and average sentence length.",
1157166809,false,finalized,3,2/13/2017 05:12:35,Pos,0.6737,W15-2518,"For the task of genre adaptation to the genres newswire (NW) and UG comments or weblogs, we use a flexible translation model adaptation approach based on phrase pair weighting using a vector space model (VSM) inspired by Chen et al (2013).",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,(2013),"The reason we choose an instanceweighting method rather than a mixture modeling approach is twofold: First, mixture modeling approaches intrinsically depend on subcorpus boundaries, which resemble provenance or require manual labeling.",3 Translation model genre adaptation,
1157166810,false,finalized,3,2/13/2017 05:36:53,Neut,0.6868,W15-2518,"Second, Irvine et al (2013) have shown that including relevant training data in a mixture modeling approach solves many coverage errors, but also introduces substantial amounts of new scoring errors.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,(2013),"With phrase-pair weighting we aim to optimize phrase translation selection while keeping our training data fixed, and we can thus compare the impact of several methodological variants on genre adaptation for SMT.","The reason we choose an instanceweighting method rather than a mixture modeling approach is twofold: First, mixture modeling approaches intrinsically depend on subcorpus boundaries, which resemble provenance or require manual labeling.",
1157166811,false,finalized,3,2/13/2017 06:26:02,Pos,0.6909,W15-2518,"Since our aim is to adapt to multiple genres in a test corpus, we follow Chen et al (2013) and manually group our training data into subcorpora that reflect various genres (see Table 3).",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,(2013),"While this definition of the vector space can approximate genres at different levels of granularity, manual subcorpus labels are labor intensive to generate, particularly in the scenario where provenance information is not available, and may not generalize well to new translation tasks.",different subcorpora.,
1157166812,false,finalized,3,2/13/2017 05:10:04,Neut,0.6884,W15-2518,"features potentially generalize across languages (Petrenz and Webber, 2012), we compute the document-level feature values wi(d) on the source as well as the target sides of our bitext, and we examine whether both are equally suitable for translation model genre adaptation.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,"(Petrenz and Webber, 2012)",3.4 Genre adaptation with LDA,"seven features are most discriminative between the genres NW and UG, and are used in the genrespecific VSM approaches.",
1157166813,false,finalized,3,2/13/2017 05:10:04,Neut,1.0,W15-2518,"Another type of feature that does not depend on provenance information is Latent Dirichlet allocation (LDA) (Blei et al., 2003), an unsupervised word-based approach that infers a preset number of latent dimensions in a corpus and represents documents as distributions over those dimensions.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,"(Blei et al., 2003)","Despite its recent successes in topic adaptation for SMT, we expect such a bag-of-words approach to be insufficient to model genre accurately.",3.4 Genre adaptation with LDA,
1157166814,false,finalized,3,2/13/2017 05:07:04,Neut,0.6766,W15-2518,"Of these, LDA with 10 dimensions yields the best translation performance, which is consistent with findings in a related topic adaptation approach by Eidelman et al (2012).",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,(2012),The LDA features in this VSM variant are inferred from the source side of the training data.,"2010), with varying numbers of latent dimensions (5, 10, 20, and 50).",
1157166815,false,finalized,3,2/13/2017 06:26:02,Pos,0.3778,W15-2518,"While our manually grouped subcorpora approximate those used by Chen et al (2013), exact agreement was impossible to obtain, illustrating that it is not trivial to manually generate optimal subcorpus labels.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,(2013),"We tokenize all Arabic data using MADA (Habash and Rambow, 2005), ATB scheme.","Table 3 lists the corpus statistics of the training data, split by manual subcorpus labels as used for the subcorpus VSM variant (see Section 3.2).",
1157166816,false,finalized,3,2/13/2017 06:26:02,Pos,0.6909,W15-2518,"We use approximate randomization (Noreen, 1989) for significance testing (Riezler and Maxwell, 2005).",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,"(Noreen, 1989)","Statistically significant differences are marked by A and  for the p < 0.05 and the p < 0.01 level, respectively.","Translation quality of all experiments is measured with case-insensitive BLEU (Papineni et al., 2002) using the closest-reference brevity penalty.",
1157166817,false,finalized,3,2/13/2017 04:59:12,Pos,0.6628,W15-2518,"We perform our experiments using an inhouse phrase-based SMT system similar to Moses (Koehn et al., 2007).",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,"(Koehn et al., 2007)","All runs use lexicalized reordering, distinguishing between monotone, swap, and discontinuous reordering, with respect to the previous and next phrase (Koehn et al., 2005).","Note that Gen&Topic contains one reference translation per sentence, while NIST has four sets of reference translations.",
1157166818,false,finalized,3,2/13/2017 04:51:09,Pos,0.6514,W10-1824,Our corpus study adopts similar features of annotation used in Botley and McEnery (2001) and provides some linguistic hypotheses on grammatical functions of Korean demonstratives to be further explored.,http://www.aclweb.org/anthology/W/W10/W10-1824.pdf,(2001),1 Introduction,"Through the development of an annotation scheme and the use of spoken and written corpora, we aim to determine different functions of demonstratives and to examine their distributional properties.",
1157166819,false,finalized,3,2/13/2017 04:51:09,Neut,0.6991,W10-1824,"In contrast with deictic usage, previous studies (Chang, 1980; Chang, 1984) assumed that anaphoric demonstratives show only a two-way distinction between proximal forms i and distal forms ku.",http://www.aclweb.org/anthology/W/W10/W10-1824.pdf,"(Chang, 1980","However, it is still controversial as to whether the boundaries between anaphora and deixis are clear cut.","Thus, distinct usage of ce and ku is associated with how the speaker allocates the deictic center and contextual space, i.e., the speakercentered space vs. the speaker- and the hearer- centered space.",
1157166820,false,finalized,3,2/13/2017 05:12:44,Neut,0.6868,W10-2109,"According to Wason and Reich (1979) (as explained in more detail below), sentences such as (2) are actually nonsensical, but people coerce them into a sensible reading by reversing the interpretation.",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf,(1979),One of our goals in this work is to explore whether computational linguistic techniques-specifically automatic corpus analysis drawing on lexical resources-can help to elucidate the factors influencing interpretation of such sentences across a collection of actual usages.,"First, the contradictory nature of the possible meanings has been explained in terms of pragmatic factors concerning the relevant presuppositions of the sentences.",
1157166821,false,finalized,3,2/13/2017 05:04:30,Pos,0.6164,W10-2109,These findings are consistent with those of Snow et al (2008) in showing that AMT judgements can be as reliable as those of expert judges.,http://www.aclweb.org/anthology/W/W10/W10-2109.pdf,(2008),"Finally, we remove a small number of items from the testing dataset which were difficult to paraphrase due to ellipsis of the verb participating in the target construction, or an extra negation in the verb phrase.","The majority judgements of these annotators are the same as those obtained from AMT on the development data, giving us confidence in the reliability of the AMT judgements.",
1157166822,false,finalized,3,2/13/2017 04:52:13,Neut,0.6325,W10-2109,"Moreover, as in the approach of Jurafsky (1996), the phenomenon we investigate here may be best considered within a constructional analysis (e.g., Langacker, 1987), in which both the syntactic construction and the particular lexical items contribute to the determination of the meaning of a usage.",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf,(1996),"We suggest that a clause of the form No Xis too Y to Z might be the (identical) surface expression of two underlying constructions-one with the ""every"" interpretation and one with the ""no"" interpretation-which place differing constraints on the semantics of the verb.","Although not intended as proposing a cognitive model, the work here can be seen as connected to these lines of research, in investigating the extent to which lexical factors can be used as proxies to more ""hidden"" features that underlie the appropriate interpretation of a pragmatically complex construction.",
1157166823,false,finalized,3,2/13/2017 05:29:36,CoCo,0.3695,W10-2109,"In the future we also intend to extract usages of the target construction from datasets of less formal text, such as blogs (e.g., Burton et al., 2009).",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf,"Burton et al., 2009)",Constructions other than No X is too Y to Z exhibit a similar ambiguity.,"While we do not adopt the view of some that usages of the target construction having the ""no"" interpretation are errors, it could be the case that such usages are more frequent in less formal text.",
1157166824,false,finalized,3,2/13/2017 05:04:30,Neut,1.0,W10-2109,"Recall Pullum's (2004) observation that the verb in the ""no"" interpretation involves explicitly not acting.",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf,(2004),"Using this intuition, we have informally observed that it is largely possible to (manually) predict the interpretation of the target construction knowing only the component verb.",We are particularly interested in further exploring the hypothesis that it is the semantics of the component verb that gives rise to the meaning of the target construction.,
1157166825,false,finalized,3,2/13/2017 05:03:40,CoCo,0.6818,W10-2109,"This stands in contrast to the analysis of Wason and Reich (1979), which presumes that people are applying some higher-level reasoning to ""correct"" an ill-formed statement in the case of the ""no"" in",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf,(1979),67,"Note that such a constructional analysis of this phenomenon assumes that both interpretations of these sentences are linguistically valid, given the appropriate lexical instantiation.",
1157166826,false,finalized,3,2/13/2017 05:12:35,Pos,0.6737,W10-2109,"get construction-we use two corpora: the British National Corpus (Burnard, 2000), an approximately one hundred million word corpus of latetwentieth century British English, and The New York Times Annotated Corpus (Sandhaus, 2008), approximately one billion words of non-newswire text from the New York Times from the years 1987-2006.",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf,"(Burnard, 2000)","We extract all sentences in these corpora containing the sequence of strings no, is too, and to separated by one or more words.",62,
1157166827,false,finalized,3,2/13/2017 05:05:39,Pos,1.0,W10-2109,"WordNet To tap into general lexical semantic properties of the words in the construction, we use features that draw on the semantic classes of words in WordNet (Fellbaum, 1998).",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf,"(Fellbaum, 1998)","These binary features each represent a synset in WordNet, and are turned on or off for the component words (the noun, adjective, and verb) in each instance of the target construction.","To test our hypothesis that the interaction of the semantics of the noun, adjective, and verb in the target construction contributes to its pragmatic interpretation, we represent each instance in our dataset as a vector of features that capture aspects of the semantics of its component words.",
1157166828,false,finalized,3,2/13/2017 04:59:12,Neut,0.6724,W10-2109,"We use WordNet version 3.0 accessed using NLTK version 2.0 (Bird et al., 2009).",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf,"(Bird et al., 2009)","Polarity Because of the observation that the verb in the target construction, in particular, has some property of negativity in the ""no"" interpretation, we also use features representing the semantic polarity of the noun, adjective, and verb in each instance.","A synset feature is on for a word if the synset occurs on the path from all senses of the word to the root, and off otherwise.",
1157166829,false,finalized,3,2/13/2017 05:30:56,Neut,0.6311,W99-0707,"The latter two use a transformation-based error-driven learning method [Brill, 1992].",http://www.aclweb.org/anthology/W/W99/W99-0707.pdf,"Brill, 1992]","In [Ramshaw and Marcus, 1995], the method is used for NP chunking, and in [Cardie and Pierce, 1998] the approach is indirectly used to evaluate corpus-extracted NP chunking rules.","We also present the results of [Argamon et at., 1998], [Ramshaw and Marcus, 1995] and [Cardie and Pierce, 1998] in Table 4.",
1157166830,false,finalized,3,2/13/2017 05:01:10,Neut,0.6305,D08-1058,"Generally speaking, there are two unsupervised scenarios for ""borrowing"" English resources for sentiment analysis in other languages: one is to generate resources in a new language by leveraging on the resources available in English via cross-lingual projections, and then perform sentiment analysis in the English language based on the generated resources, which has been investigated by Mihalcea et al (2007); the other is to translate the texts in a new language into English texts, and then perform sentiment analysis in the English language, which has not yet been investigated.",http://www.aclweb.org/anthology/D/D08/D08-1058.pdf,(2007),"In this study, we first translate Chinese reviews into English reviews by using machine translation services, and then identify the sentiment polarity of English reviews by directly leveraging English resources.","Instead of using only the limited Chinese knowledge, this study aims to improve Chinese sentiment analysis by making full use of bilingual knowledge in an unsupervised way, including both Chinese resources and English resources.",
1157166831,false,finalized,3,2/13/2017 04:52:13,Neut,1.0,D08-1058,Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting.,http://www.aclweb.org/anthology/D/D08/D08-1058.pdf,(2008),"Research work focusing on Chinese sentiment analysis includes (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007; Wang et al., 2007).","Blitzer et al (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products.",
1157166832,false,finalized,3,2/13/2017 05:07:21,Pos,0.6317,W10-3805,"Contextual information is extremely useful in making lexical selections of higher quality, as illustrated by the models for Global Lexical Selection (Bangalore et al., 2007; Venkatapathy and",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf,"(Bangalore et al., 2007",34,"In contrast to a phrase-based model, a discriminative model has the power to integrate much richer contextual information into the training model.",
1157166833,false,finalized,3,2/13/2017 04:59:12,Neut,1.0,W10-3805,"In Direct Translation Model (DTM) proposed for statistical machine translation by (Papineni et al., 1998; Och and Ney, 2002), the authors present a discriminative set-up for natural language understanding (and MT).",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf,"(Papineni et al., 1998",They use a slightly modified equation (in comparison to IBM models) as shown in equation 1.,"In this section, we present approaches that are directly related to our approach.",
1157166834,false,finalized,3,2/13/2017 05:07:17,Neut,0.6891,W10-3805,"The DTM2 model differs from other phrasebased SMT models in that it avoids the redundancy present in other systems by extracting from a word aligned parallel corpora a set of minimal phrases such that no two phrases overlap with each other (Hassan et al., 2009).",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf,"(Hassan et al., 2009)","The decoding strategy in DTM2 (Ittycheriah and Roukos, 2007) is similar to a phrase-based decoder except that the score of a particular translation block is obtained from the maximum entropy model using the set of feature functions.","e = arg max Pr(e|f) e M (3) = arg max mm(f, e) e m=1",
1157166835,false,finalized,3,2/13/2017 05:04:39,Pos,0.6756,W10-3805,"The method uses improved Kneser-Ney smoothing algorithm (Chen and Goodman, 1999) to compute sequence probabilities.",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf,"(Chen and Goodman, 1999)",5 Dataset,We trained a language model of order 5 built on the entire EUROPARL corpus using the SRILM package.,
1157166836,false,finalized,3,2/13/2017 04:57:04,CoCo,0.6828,W10-3805,"We show that the performance of our approach (using simple lexical features) is comparable to that of the state-of-art statistical MT system (Koehn et al., 2007).",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf,"(Koehn et al., 2007)","When additional syntactic features (POS tags in this paper) are used, there is a boost in the performance which is likely to improve when richer syntactic features are incorporated in the model.",We used maximum entropy model in our experiments.,
1157166837,false,finalized,3,2/13/2017 05:03:40,CoCo,0.6818,W10-3805,"The decoding strategy in DTM2 (Ittycheriah and Roukos, 2007) is similar to a phrase-based decoder except that the score of a particular translation block is obtained from the maximum entropy model using the set of feature functions.",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf,"(Ittycheriah and Roukos, 2007)","In our approach, instead of providing the complete scoring function ourselves, we compute the parameters needed by a phrase based decoder, which in turn uses these parameters appropriately.","The DTM2 model differs from other phrasebased SMT models in that it avoids the redundancy present in other systems by extracting from a word aligned parallel corpora a set of minimal phrases such that no two phrases overlap with each other (Hassan et al., 2009).",
1157166840,false,finalized,3,2/13/2017 05:01:10,CoCo,0.6811,W10-3805,"This model is similar to the global lexical selection (GLS) model described in (Bangalore et al., 2007; Venkatapathy and Bangalore, 2009) except that in GLS, the predicted target blocks are not associated with any particular source word unlike the case here.",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf,"(Bangalore et al., 2007","For the set of experiments in this paper, we used a context of size 6, containing three words to the left and three words to the right.",These classifiers predict if a particular target block should be present given the source word and its context.,
1157166841,false,finalized,3,2/13/2017 05:13:05,Pos,0.6845,W10-3805,"For training, we use the maximum entropy software library Llama presented in (Haffner, 2006).",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf,"(Haffner, 2006)",3.1.1 Context Dependent Block Translation Model,"Similarly, we call the reordering model, a 'context dependent block distortion model'.",
1157166842,false,finalized,3,2/13/2017 05:10:36,Neut,1.0,W09-3953,"tween an initiating speech act and a responding one-the analog of adjacency pairs (Sacks et al., 1974).",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf,"(Sacks et al., 1974)","The most closely related effort is (Galley et al., 2004), which aims to automatically identify adjacency pairs in the ICSI Meeting corpus, a large corpus of 75 meetings, using a small tagset.",357,
1157166843,false,finalized,3,2/13/2017 05:12:44,Pos,1.0,W09-3953,"Using the switchboard corpus, (Stolcke et al., 2000) achieved good dialogue act labeling accuracy (71% on manual transcriptions) for a set of 42 dialogue act types, and constructed probabilistic models of dialogue act sequencing in order to test the hypothesis that dialogue act sequence information could boost speech recognition performance.",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf,"(Stolcke et al., 2000)",There has been far less work on developing manual and automatic dialogue act annotation schemes for email.,"Their maximum entropy ranking approach achieved 90% accuracy on the 4-way classification into agreement, disagreement, backchannel and other.",
1157166844,false,finalized,3,2/13/2017 05:05:46,Neut,0.689,W09-3953,"For twenty narratives each segmented by the same seven annotators, using Cochran's Q (Cochran, 1950), we found the probabilities associated with the null hypothesis that the observed distributions could have arisen by chance to be at or below p=0.1 x106.",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf,"(Cochran, 1950)",Partitioning Q by number of annotators gave significant results for all values of A ranging over the number of annotators apart from A = 2.,"We have previously shown (Passonneau and Litman, 1997) that intention-based segmentation can be done reliably by multiple annotators.",
1157166845,false,finalized,3,2/13/2017 05:05:46,CoCo,0.6312,W09-3953,"A forward link (Flink) is the analog of a ""first pair-part"" of an adjacency pair (Sacks et al., 1974), and is similarly restricted to specific speech act types.",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf,"(Sacks et al., 1974)",All Request-Information and Request-Action DFUs are assigned Flinks.,"DFU Links, or simply Links, correspond to adjacency pairs, but need not be adjacent.",
1157166846,false,finalized,3,2/13/2017 04:57:04,Pos,0.6304,W09-3953,"However, Structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2005) is also a compelling method which has the potential to handle the interdependence between labeling and sequencing, due to its ability to handle dependencies among features and prediction results within the structure.",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf,"(Taskar et al., 2003","sequence labeling (Tsochantaridis et al., 2005).","Taskar, 2007) such as link prediction.",
1157166847,false,finalized,3,2/13/2017 05:02:41,Neut,0.6884,W09-3953,"The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as the contrast between human-human and human-computer dialogues (Doran et al., 2001).",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf,"(Traum and Heeman, 1996","While there is a large degree of overlap across schemes, the set of DA types will differ due to differences in the nature of the communicative goals; thus information-seeking versus task-oriented dialogues differ in the set of speech acts and their relative frequencies.","A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses.",
1157166848,false,finalized,3,2/13/2017 04:57:08,Neut,0.6325,W09-3953,"We used a version of the corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006).",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf,"(Yeh and Harnly, 2006)",The annotator of the majority of the Loqui corpus also annotated the Enron corpus.,"Most of the emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely social emails.",
1157166849,false,finalized,3,2/13/2017 04:52:13,Neut,1.0,J88-3005,"The work of Sleeman (1982) on inferring defective algebra rules (mal-rules) is based on the observation that the a priori listing of misconceptions is a difficult, if not impossible, task.",http://www.aclweb.org/anthology/J/J88/J88-3005.pdf,(1982),Sleeman proposes on-line inference of mal-rules based on the answer the student has given to a particular problem.,"The major problem with these systems is due to their inability to reason about the misconception itself, they are completely at a loss when faced with a misconception absent from their a priori listing.",
1157166850,false,finalized,3,2/13/2017 04:57:38,CoCo,1.0,J88-3005,This method for correcting misconceptions suggests a model of natural language generation that is similar to that put forth by McKeown (1982) but which differs from McKeown's model in several ways.,http://www.aclweb.org/anthology/J/J88/J88-3005.pdf,(1982),Both McKeown and this work concentrate on determining the content and textual shape of a response.,The highlighting and similarity metric used by ROMPER will be discussed below.,
1157166851,false,finalized,3,2/13/2017 05:01:10,Neut,0.6305,J88-3005,"As shown by Tversky (1977) and others, human judgments of object similarity have been found to shift both when the set of objects under discussion are altered (e.g., a violin and an electric guitar may be judged quite similar when in a group with a clarinet and an oboe, and may be judged quite different when the other members of the group are Computational Linguistics, Volume 14, Number 3, September 1988 57 Kathleen F. McCoy Reasoning on a Highlighted User Model to Respond to Misconceptions a cello and an electric bass), and when the salience of attributes are altered (e.g., in a group containing a red triangle, a blue triangle, and a red square, the red triangle might be judged similar to the blue triangle when attribute shape is stressed, but may be judged similar to the red square when attribute color is stressed).",http://www.aclweb.org/anthology/J/J88/J88-3005.pdf,(1977),One metric that avoids these problems was introduced by Tversky (1977).,A second major problem with a similarity metric based on distance in the generalization hierarchy is that it is context invariant; contextual information has no way of affecting the assessments.,
1157166852,false,finalized,3,2/13/2017 05:13:05,Weak,1.0,P13-1028,"Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed.",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf,(2012),"The main contribution of this paper is a considerable improvement of unsupervised parsing quality by estimating the Pstop probabilities externally using a very large corpus, and employing this prior knowledge in the standard inference of DMV.",verbs become leaves instead of governing the sentences.,
1157166853,false,finalized,3,2/13/2017 05:10:34,Neut,0.6818,P13-1028,It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al (2008).,http://www.aclweb.org/anthology/P/P13/P13-1028.pdf,(2004),Headden III et al (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing.,Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years.,
1157166854,false,finalized,3,2/13/2017 05:09:33,Neut,0.676,P13-1028,"We introduced a simple procedure for recognition of reducible sequences in (Marecek and Zabokrtsky, 2012): The particular sequence of words is removed from the sentence and if the remainder of the sentence exists elsewhere in the corpus, the sequence is considered reducible.",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf,"(Marecek and Zabokrtsky, 2012)",We provide an example in Figure 2.,3.1 Recognition of reducible sequences,
1157166855,false,finalized,3,2/13/2017 05:12:06,Pos,1.0,P13-1028,"We employ the Gibbs sampling algorithm (Gilks et al., 1996).",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf,"(Gilks et al., 1996)","Unlike in (Marecek and Zabokrtsky, 2012), where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming.",5 Inference,
1157166856,false,finalized,3,2/13/2017 04:57:38,CoCo,0.6811,P13-1028,"Unlike in (Marecek and Zabokrtsky, 2012), where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming.",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf,"(Marecek and Zabokrtsky, 2012)",The algorithm works as follows:,"We employ the Gibbs sampling algorithm (Gilks et al., 1996).",
1157166857,false,finalized,3,2/13/2017 05:10:34,Pos,1.0,P13-1028,"We use the standard generative Dependency Model with Valence (Klein and Manning, 2004).",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf,"(Klein and Manning, 2004)","The generative story is the following: First, the head of the sentence is generated.",4 Model,
1157166858,false,finalized,3,2/13/2017 05:04:30,Pos,0.7146,P13-1028,"After the last iteration, we use these collected counts as weights and compute maximum directed spanning trees using the Chu-Liu/Edmonds' algorithm (Chu and Liu, 1965).",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf,"(Chu and Liu, 1965)","Therefore, the resulting trees consist of edges maximizing the sum of individual counts:","After each iteration is finished (all the trees in the corpus are re-sampled), we increment the counter of all directed pairs of nodes which are connected by a dependency edge in the current trees.",
1157166859,false,finalized,3,2/13/2017 04:52:13,Neut,0.6846,P13-1028,"The first type are CoNLL treebanks from the year 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007), which we use for inference and for evaluation.",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf,"(Buchholz and Marsi, 2006)","As is the standard practice in unsupervised parsing evaluation, we removed all punctuation marks from the trees.",We use two types of resources in our experiments.,
1157166860,false,finalized,3,2/13/2017 05:10:04,Pos,1.0,P13-1028,"For estimating the STOP probabilities (Section 3), we use the Wikipedia articles from W2C corpus (Majlis and Zabokrtsky, 2012), which provide sufficient amount of data for our purposes.",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf,"(Majlis and Zabokrtsky, 2012)",Statistics across languages are shown in Table 1.,"In case a punctuation node was not a leaf, its children are attached to the parent of the removed node.",
1157166861,false,finalized,3,2/13/2017 05:13:05,Neut,1.0,N07-1064,"Elming (2006) suggests using tranformation-based learning to automatically acquire error-correcting rules from such data; however, the proposed method only applies to lexical choice errors.",http://www.aclweb.org/anthology/N/N07/N07-1064.pdf,(2006),"Knight and Chander (1994) also argue in favor of using a separate APE module, which is then portable across multiple MT systems and language pairs, and suggest that the post-editing task could be performed using statistical machine translation techniques.","Allen and Hogan (2000) sketch the outline of such an automated post-editing (APE) system, which would automatically learn post-editing rules from a tri-parallel corpus of source, raw MT and post-edited text.",
1157166862,false,finalized,3,2/13/2017 05:01:20,Neut,0.6689,N07-1064,"To improve raw output from decoding, Portage relies on a rescoring strategy: given a list of n-best translations from the decoder, the system reorders this list, this time using a more elaborate loglinear model, incorporating more feature functions, in addition to those of the decoding model: these typically include IBM-1 and IBM-2 model probabilities (Brown et al., 1993) and an IBM-1-based feature function designed to detect whether any word in one language appears to have been left without satisfactory translation in the other language; all of these feature functions can be used in both language directions, i.e.",http://www.aclweb.org/anthology/N/N07/N07-1064.pdf,"(Brown et al., 1993)",source-to-target and target-to-source.,no new translations are produced at the decoding step.,
1157166863,false,finalized,3,2/13/2017 05:04:39,Pos,0.6565,N07-1064,"Portage implements a dynamic programming beam search decoding algorithm similar to that of Koehn (2004), in which translation hypotheses are constructed by combining in various ways the target-language part of phrase pairs whose sourcelanguage part matches the input.",http://www.aclweb.org/anthology/N/N07/N07-1064.pdf,(2004),These phrase pairs come from large phrase tables constructed by collecting matching pairs of contiguous text segments from word-aligned bilingual corpora.,"Decoding is the central phase in SMT, involving a search for the hypotheses t that have highest probabilities of being translations of the current source sentence s according to a model for P(t1s).",
1157166864,false,finalized,3,2/13/2017 05:04:44,CoCo,0.3423,N07-1064,"The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P(s|t) are smoothed using the Good-Turing technique (Foster et al., 2006).",http://www.aclweb.org/anthology/N/N07/N07-1064.pdf,"(Foster et al., 2006)","The distortion model is also very similar to Koehn's, with the exception of a final cost to account for sentence endings.","Portage's model for P(t1s) is a log-linear combination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature.",
1157166865,false,finalized,3,2/13/2017 05:07:04,Pos,0.6856,E06-1005,"We use the IBM Model 1 (Brown et al., 1993) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996)) to estimate the alignment model.",http://www.aclweb.org/anthology/E/E06/E06-1005.pdf,"(Brown et al., 1993)",The lexicon probability of a sentence pair is modelled as a product of single-word based probabilities of the aligned words.,"As in statistical machine translation, we make modelling assumptions.",
1157166866,false,finalized,3,2/13/2017 05:00:13,Pos,0.6514,E06-1005,"Finally, we also computed consensus translation from some of the submissions to the TC-STAR 2005 evaluation campaign (TC-STAR, 2005).",http://www.aclweb.org/anthology/E/E06/E06-1005.pdf,"(TC-STAR, 2005)",The TC-STAR participants had submitted translations of manually transcribed speeches from the European Parliament Plenary Sessions (EPPS).,"Here, the involved MT systems had used about 60K sentence pairs (420K running words) for training.",
1157166867,false,finalized,3,2/13/2017 04:51:52,Neut,1.0,E91-1050,"Kaplan et al., 1989), there are advantages in treating the intermediate, transfer, stage independently.",http://www.aclweb.org/anthology/E/E91/E91-1050.pdf,"Kaplan et al., 1989)","As an example, consider the FSs shown below:6","The first and last of these are to be performed by parsing and generation with natural language grammars, but, while proposals have been made to combine some of the three stages (e.g.",
1157166868,false,finalized,3,2/13/2017 05:05:46,Pos,0.6798,E91-1050,"We have presented what is to our knowledge the first formalization and implementation of a type of rule and control regime intended for use in situations where it is desired to produce the effect of transforming one feature structure into another.9 The formalism described above has been implemented as part of ISSCO's ELUM, an enhanced PATR-II style (Shieber, 1986) unification grammar environment, based on the UD system presented by Johnson and Rosner (1989).",http://www.aclweb.org/anthology/E/E91/E91-1050.pdf,"(Shieber, 1986)","ELU incorporates a parser and generator, and is primarily intended for use as a tool for research in machine translation.",Conclusion,
1157166869,false,finalized,3,2/13/2017 05:30:56,Neut,1.0,W01-0726,"AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate.",http://www.aclweb.org/anthology/W/W01/W01-0726.pdf,"(Freund and Schapire, 1997)","In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions.",2 System Architecture,
1157166870,false,finalized,3,2/13/2017 05:12:35,Pos,1.0,W01-0726,"In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions.",http://www.aclweb.org/anthology/W/W01/W01-0726.pdf,"(Schapire and Singer, 1999)","This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such as: POS tagging, PP-attachment disambiguation, text categorization, and word sense disambiguation.","AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate.",
1157166871,false,finalized,3,2/13/2017 05:12:06,Pos,0.676,W01-0726,"In our particular setting, weak rules are extended to arbitrarily deep decision trees following the suggestion of (Schapire and Singer, 1999) and the definition presented in (Carreras and Marquez, 2001).",http://www.aclweb.org/anthology/W/W01/W01-0726.pdf,"(Schapire and Singer, 1999)","These more complex weak rules allow the algorithm to work in a higher dimensional feature space that contains conjunctions of simple features, and this fact has turned out to be crucial for improving results in the present domain.","This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such as: POS tagging, PP-attachment disambiguation, text categorization, and word sense disambiguation.",
1157166872,false,finalized,3,2/13/2017 05:16:43,Neut,1.0,W05-0618,"Chekuri et al (2001) proposed an integer linear programming (ILP) formulation of the metric labeling problem, with both assignment cost and separation costs being modeled as binary variables of the linear cost function.",http://www.aclweb.org/anthology/W/W05/W05-0618.pdf,(2001),"Recently, Roth & Yih (2004) applied an ILP model to the task of the simultaneous assignment of semantic roles to the entities mentioned in a sentence and recognition of the relations holding between them.",the costs of selecting a pair of labels for two related objects3.,
1157166873,false,finalized,3,2/13/2017 05:10:36,Neut,0.6565,W15-0302,"Particularly, Thompson and Mulac (1991) consider this epistemic phrase has achieved a hedging state through a process of grammaticalization.",http://www.aclweb.org/anthology/W/W15/W15-0302.pdf,(1991),"Their view is that I think is roughly similar to maybe when used to express the degree of speaker commitment, thus comprising a grammatical sub-category of adverbs.",Early discussion about interpreting epistemic phrases as hedges originated in the analysis I think.,
1157166874,false,finalized,3,2/13/2017 05:12:06,Neut,0.6868,W15-0302,"They also describe particular properties of these phrases such as: a) representing the speaker's attitude with respect to the subsequent piece of discourse in contrast to when third person is used, there the piece of discourse is seen as a description (Scheibman, 2001), b) it comprises explicitly subjective claims in contrast to impersonal expressions where the hedging source is obscure (Hyland, 1998) , c) used to express knowledge states, as a boundary marker for turn-taking in conversation, a speaker's perspective marker, and as a way to align the speaker's with the listener's stance (Karkkainen, 2010).",http://www.aclweb.org/anthology/W/W15/W15-0302.pdf,"(Scheibman, 2001)","Besides, Wierzbicka (2006) suggests that this category of phrases merits recognition as a major grammatical and semantic class in modern English.","Scheibman (2001), Karkkainen (2010) and Wierzbicka (2006) showed that first person epistemic phrases used to express personal stance are highly frequent in various registers in contemporary English.",
1157166875,false,finalized,3,2/13/2017 05:12:44,Neut,1.0,N03-2033,"Information Quality Dimensions (Source: Strong, Lee, Wang, 1997, p.39)",http://www.aclweb.org/anthology/N/N03/N03-2033.pdf,"Strong, Lee, Wang, 1997, ",Empirical attempts to assess quality have primarily focused on counting hyperlinks in a networked environment.,Table 1.,
1157166876,false,finalized,3,2/13/2017 05:12:06,Neut,0.3629,W11-2507,The categorical distributional compositional model of meaning of Coecke et al (2010) combines the modularity of formal semantic models with the empirical nature of vector space models of lexical semantics.,http://www.aclweb.org/anthology/W/W11/W11-2507.pdf,(2010),"The meaning of a sentence is defined to be the application of its grammatical structure- represented in a type-logical model-to the kronecker product of the meanings of its words, as computed in a distributional model.",1 Background,
1157166877,false,finalized,3,2/13/2017 05:13:05,CoCo,0.3723,W11-2507,"Dataset The dataset is built using the same guidelines as Mitchell and Lapata (2008), using transitive verbs obtained from CELEX1 paired with subjects and objects.",http://www.aclweb.org/anthology/W/W11/W11-2507.pdf,(2008),We first picked 10 transitive verbs from the most frequent verbs of the BNC.,This degree decreases for the pair 'the child met the house' and 'the child satisfied the house'.,
1157166878,false,finalized,3,2/13/2017 05:02:41,Pos,1.0,W11-2507,Parameters We used the parameters described by Mitchell and Lapata (2008) for the noun and verb vectors.,http://www.aclweb.org/anthology/W/W11/W11-2507.pdf,(2008),All vectors were built from a lemmatised version of the BNC.,"The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011).",
1157169712,true,golden,9,"",Neut,0.6834,D09-1160,"Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995).",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf,"(Cortes and Vapnik, 1995)","The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008).","However, 'explicit' feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier.","Pos
Neut"
1157170472,true,golden,9,"",Neut,0.7984,W09-3953,"A forward link (Flink) is the analog of a ""first pair-part"" of an adjacency pair (Sacks et al., 1974), and is similarly restricted to specific speech act types.",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf,"(Sacks et al., 1974)",All Request-Information and Request-Action DFUs are assigned Flinks.,"DFU Links, or simply Links, correspond to adjacency pairs, but need not be adjacent.","Neut
CoCo"
1157170741,true,golden,13,"",Pos,0.622,W01-0726,"In our particular setting, weak rules are extended to arbitrarily deep decision trees following the suggestion of (Schapire and Singer, 1999) and the definition presented in (Carreras and Marquez, 2001).",http://www.aclweb.org/anthology/W/W01/W01-0726.pdf,"(Schapire and Singer, 1999)","These more complex weak rules allow the algorithm to work in a higher dimensional feature space that contains conjunctions of simple features, and this fact has turned out to be crucial for improving results in the present domain.","This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such as: POS tagging, PP-attachment disambiguation, text categorization, and word sense disambiguation.",Pos
1157170987,true,golden,9,"",Neut,0.8901,W05-0618,"Chekuri et al (2001) proposed an integer linear programming (ILP) formulation of the metric labeling problem, with both assignment cost and separation costs being modeled as binary variables of the linear cost function.",http://www.aclweb.org/anthology/W/W05/W05-0618.pdf,(2001),"Recently, Roth & Yih (2004) applied an ILP model to the task of the simultaneous assignment of semantic roles to the entities mentioned in a sentence and recognition of the relations holding between them.",the costs of selecting a pair of labels for two related objects3.,Neut
1157172438,true,golden,11,"",Pos,0.9138,W09-3953,"Using the switchboard corpus, (Stolcke et al., 2000) achieved good dialogue act labeling accuracy (71% on manual transcriptions) for a set of 42 dialogue act types, and constructed probabilistic models of dialogue act sequencing in order to test the hypothesis that dialogue act sequence information could boost speech recognition performance.",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf,"(Stolcke et al., 2000)",There has been far less work on developing manual and automatic dialogue act annotation schemes for email.,"Their maximum entropy ranking approach achieved 90% accuracy on the 4-way classification into agreement, disagreement, backchannel and other.","Pos
Neut"
1157172629,true,golden,10,"",Pos,0.5128,W01-0726,"AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate.",http://www.aclweb.org/anthology/W/W01/W01-0726.pdf,"(Freund and Schapire, 1997)","In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions.",2 System Architecture,"Pos
Neut"
1157172685,true,golden,17,"",Neut,0.8823,P06-2061,"In the post-editing step, a prediction engine helps to decrease the amount of human interaction (Och et al., 2003).",http://www.aclweb.org/anthology/P/P06/P06-2061.pdf,"(Och et al., 2003)","In a CAT system with integrated speech, two sources of information are available to recognize the speech input: the target language speech and the given source language text.","Then, one possible procedure for skilled human translators is to provide the oral translation of a given source text and then to post-edit the recognized text.",Neut
1157172736,true,golden,13,"",Pos,0.7216,W14-4719,"In the first maximum alignment based approach we will consider, the definitions of sz,pred and sz, are inspired by Mihalcea et al (2006) who normalize phrasal similarities according to the phrase length.",http://www.aclweb.org/anthology/W/W14/W14-4719.pdf,(2006),"1 Si,pred = 2 1precei,pred,fi,pred + recei,pred,fi,pred 1 =2 1precei,j,fi,j + recei,j ,fi,j",3.2 Maximum alignment (precision-recall average),Pos
1157172826,true,golden,10,"",Neut,0.916,W15-0302,"Particularly, Thompson and Mulac (1991) consider this epistemic phrase has achieved a hedging state through a process of grammaticalization.",http://www.aclweb.org/anthology/W/W15/W15-0302.pdf,(1991),"Their view is that I think is roughly similar to maybe when used to express the degree of speaker commitment, thus comprising a grammatical sub-category of adverbs.",Early discussion about interpreting epistemic phrases as hedges originated in the analysis I think.,Neut
1157173026,true,golden,16,"",Pos,0.6847,W09-3953,"However, Structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2005) is also a compelling method which has the potential to handle the interdependence between labeling and sequencing, due to its ability to handle dependencies among features and prediction results within the structure.",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf,"(Taskar et al., 2003","sequence labeling (Tsochantaridis et al., 2005).","Taskar, 2007) such as link prediction.","Pos
Neut"
1157173971,true,golden,8,"",Neut,0.7805,W09-3953,"The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as the contrast between human-human and human-computer dialogues (Doran et al., 2001).",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf,"(Traum and Heeman, 1996","While there is a large degree of overlap across schemes, the set of DA types will differ due to differences in the nature of the communicative goals; thus information-seeking versus task-oriented dialogues differ in the set of speech acts and their relative frequencies.","A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses.",Neut
1157174009,true,golden,9,"",Neut,0.8055,W15-2518,"Kessler et al (1997) categorize four types of genre-revealing cues: structural cues (e.g., part-of-speech (POS) tag counts), lexical cues (specific words), character-level cues (e.g., punctuation marks), and derivative cues (ratios and variation measures based on other types of cues).",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,(1997),"Dewdney et al (2001) compare a large number of document features and show that these outperform bag-of-words approaches, which are traditionally used in topic-based text classifica","Karlgren and Cutting (1994) were among the first to use simple document statistics, such as common word frequencies, first-person pronoun count, and average sentence length.",Neut
1157174143,true,golden,11,"",Pos,0.8211,S15-2109,"Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons.",http://www.aclweb.org/anthology/S/S15/S15-2109.pdf,"(Zhu et al., 2014)","In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015).","However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning.","Pos
Neut"
1157174209,true,golden,8,"",Pos,1.0,W11-2507,Parameters We used the parameters described by Mitchell and Lapata (2008) for the noun and verb vectors.,http://www.aclweb.org/anthology/W/W11/W11-2507.pdf,(2008),All vectors were built from a lemmatised version of the BNC.,"The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011).",Pos
1157174325,true,golden,14,"",Neut,0.5048,D13-1114,"A well-known study evaluated usage of Twitter in the aftermath of the 2010 earthquake in Japan (Sakaki et al., 2010).",http://www.aclweb.org/anthology/D/D13/D13-1114.pdf,"(Sakaki et al., 2010)","Another Japanese-oriented study evaluated the impact of television on tweeted content (Akioka et al., 2010).",More work has been done in the latter category: analysis of social phenomena in a non-English context.,"Pos
Neut"
1157174368,true,golden,11,"",CoCo,0.9016,N07-1064,"The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P(s|t) are smoothed using the Good-Turing technique (Foster et al., 2006).",http://www.aclweb.org/anthology/N/N07/N07-1064.pdf,"(Foster et al., 2006)","The distortion model is also very similar to Koehn's, with the exception of a final cost to account for sentence endings.","Portage's model for P(t1s) is a log-linear combination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature.","Pos
CoCo"
1157174437,true,golden,8,"",Pos,0.8811,W10-2109,"get construction-we use two corpora: the British National Corpus (Burnard, 2000), an approximately one hundred million word corpus of latetwentieth century British English, and The New York Times Annotated Corpus (Sandhaus, 2008), approximately one billion words of non-newswire text from the New York Times from the years 1987-2006.",http://www.aclweb.org/anthology/W/W10/W10-2109.pdf,"(Burnard, 2000)","We extract all sentences in these corpora containing the sequence of strings no, is too, and to separated by one or more words.",62,Pos
1157174583,true,golden,14,"",Pos,0.9316,W12-3159,"Since it is impractical to calculate derivatives of this function, we use derivative-free optimization methods such as the downhill simplex method (Helder and Mead, 1965) and Powell's method (Powell, 1964), which generally handle such difficult search conditions relatively well.",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf,"(Helder and Mead, 1965)",This approach confers several benefits over MERT.,"which takes source sentences and a set of weights as inputs, and outputs the evaluation score (e.g., BLEU score) computed on the decoded sentences.",Pos
1157174903,true,golden,9,"",Pos,0.8855,W01-0726,"In designing our system, a generalized version of the AdaBoost algorithm has been used AdaBoost.MH, (Schapire and Singer, 1999), which works with very simple domain partitioning weak hypotheses (decision stumps) with confidence rated predictions.",http://www.aclweb.org/anthology/W/W01/W01-0726.pdf,"(Schapire and Singer, 1999)","This particular boosting algorithm is able to work efficiently in very high dimensional feature spaces, and has been applied, with significant success, to a number of NLP disambiguation tasks, such as: POS tagging, PP-attachment disambiguation, text categorization, and word sense disambiguation.","AdaBoost (Freund and Schapire, 1997) is a general method for obtaining a highly accurate classification rule by combining many weak classifiers, each of which may be only moderately accurate.",Pos
1157175150,true,golden,13,"",CoCo,0.7704,W02-1024,"Similar to our approach, Electric Knowledge transforms a natural language question into a series of increasingly more general keyword queries (Bierner, 2001).",http://www.aclweb.org/anthology/W/W02/W02-1024.pdf,"(Bierner, 2001)","However, their query formulation process utilizes hard-crafted regular expressions, while we adopt a more general machine learning approach for transformation rule application.","User questions are compared against those in the database, and links to webpages for the closest matches are returned.","Neut
CoCo"
1157175204,true,golden,11,"",Neut,0.9178,W10-1609,"The objective of the Recognizing Textual Entailment Challenge is determining whether the meaning of the Hypothesis (H) can be inferred from a text (T) (Ido Dagan et al., 2006).",http://www.aclweb.org/anthology/W/W10/W10-1609.pdf,"(Ido Dagan et al., 2006)",This challenge has been organized by NIST in recent years.,1 Introduction,Neut
1157175279,true,golden,12,"",Neut,0.595,Q14-1008,"We can also add the Unique Stress Constraint (USC) (Yang, 2004) by excluding all variants of rule (18) that generate two or more stressed syllables.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,"(Yang, 2004)","For example, while the lexical generator for the colloc3-nophon-stress model will include the rule Word  SSyll SSyll, the lexical generator embodying the USC lacks this rule.",This yields the colloc3-phon-stress model.,"Pos
Neut"
1157175380,true,golden,7,"",Neut,0.5607,W12-2202,"Bautista et al (2009) also rely on a dictionary of synonyms, but their criterion for choosing the most appropriate one is wordlength rather than frequency.",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf,(2009),"Caseli et al (2009) analyse lexical operations on a parallel corpus of original and manually simplified texts in Portuguese, using lists of simple words and discourse markers as resources.","The above approach to lexical simplification has been repeated in a number of works (Lal and Ruger, 2002; Burstein et al., 2007).","Neut
CoCo"
1157175423,true,golden,8,"",Neut,0.615,W13-0804,"Cube pruning (Chiang, 2007) is a widely used search strategy in state-of-the-art hierarchical decoders.",http://www.aclweb.org/anthology/W/W13/W13-0804.pdf,"(Chiang, 2007)","Some alternatives and extensions to the classical algorithm as proposed by David Chiang have been presented in the literature since, e.g.",1 Introduction,"Pos
Neut"
1157175463,true,golden,11,"",Pos,0.7612,P13-1028,"The first type are CoNLL treebanks from the year 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007), which we use for inference and for evaluation.",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf,"(Buchholz and Marsi, 2006)","As is the standard practice in unsupervised parsing evaluation, we removed all punctuation marks from the trees.",We use two types of resources in our experiments.,Pos
1157175612,true,golden,5,"",Weak,0.8153,P13-1028,"Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed.",http://www.aclweb.org/anthology/P/P13/P13-1028.pdf,(2012),"The main contribution of this paper is a considerable improvement of unsupervised parsing quality by estimating the Pstop probabilities externally using a very large corpus, and employing this prior knowledge in the standard inference of DMV.",verbs become leaves instead of governing the sentences.,Weak
1157175972,true,golden,13,"",Error,0.5493,J11-2002,"2005a C/NC T Unclear Unclear Bacchin, Ferro, and Melucci 2005, C T Italian/English Segmentation 2002a, 2002b; Nunzio et al.",http://www.aclweb.org/anthology/J/J11/J11-2002.pdf,"a, 2002b","2004 Oliver 2004, Chapter 4-5 C T Catalan Paradigms Bordag 2005a, 2005b, 2007, 2008 C T English/German Segmentation Hammarstrom 2005, 2006a, 2006b, C - Maori to Warlpiri Same-stem 2007b, 2009a, 2009b Bernhard 2005a, 2005b, 2006, 2007, C T Finnish/Turkish/English Segmentation+ 2008 Related sets of words Keshava and Pitler 2005 C T Finnish/Turkish/English Segmentation Johnsen 2005 C T Finnish/Turkish/English Segmentation Atwell and Roberts 2005 C T Finnish/Turkish/English Segmentation Dang and Choudri 2005 C T Finnish/Turkish/English Segmentation ur Rehman and Hussain 2005 C T Finnish/Turkish/English Segmentation Jordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English Segmentation Goldwater, Griffiths, and Johnson C T English/Child-English Segmentation 2005; Goldwater 2007; Naradowsky and Goldwater 2009 Freitag 2005 C T English Segmentation Golcher 2006 C - English/German Lexicon+ Paradigms Arabsorkhi and Shamsfard 2006 C T Persian Segmentation Chan 2006, Chan 2008, C T English Paradigms pages 101-139 Demberg 2007 C/NC T English/German/ Segmentation Finnish/Turkish Dasgupta and Ng 2006, 2007a; C T Bengali Segmentation 2007b; Dasgupta 2007 De Pauw and Wagacha 2007 C/NC T Gikuyu Segmentation Tepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish Analysis Xanthos 2007 NC T Arabic Lexicon+ Paradigms Majumder et al.",2005; Hu et al.,Error
1157176116,true,golden,10,"",Weak,0.5909,W12-3159,"Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an error-prone decoder differs from the one of an exact decoder (Chang and Collins, 2011).",http://www.aclweb.org/anthology/W/W12/W12-3159.pdf,"(Chang and Collins, 2011)","MERT calculates an envelope from candidate translations and assumes all translations on the envelope are reachable by the decoder, but these translations may become unreachable due to search errors.","First, the Nbest error surface explored by MERT is generally not the same as the true error surface, which means that the error rate at an optimum1 of the N-best error surface is not guaranteed to be any close to an optimum of the true error surface.","Weak
Neut"
1157198136,true,golden,5,"",Neut,1.0,J88-3004,"Initial work in recognizing and indexing abstract configurations of planning relations is discussed in Dolan and Dyer (1985, 1986).",http://www.aclweb.org/anthology/J/J88/J88-3004.pdf,"(1985, 1986)",9.3 OTHER CLASSES OF MISCONCEPTIONS,We have left the automatic creation of this taxonomy from advisor experiences to future research.,Neut
1157198201,true,golden,8,"",Pos,0.6352,W12-2202,"The automatic alignments have then been manually corrected through a graphical editing tool within the GATE framework (Cunningham et al., 2002).",http://www.aclweb.org/anthology/W/W12/W12-2202.pdf,"(Cunningham et al., 2002)","A total of 570 sentences have been aligned (246 in original and 324 in simple texts), with the following correlations between them: one to one, one to many or many to one, as well as cases where there is no correlation (cases of content reduction through summarisation or information expansion through the introduction of definitions).","aligning algorithm based on Hidden Markov Models (Bott and Saggion, 2011) has been applied to obtain sentence-level alignments.",Pos
1157199269,true,golden,4,"",Neut,1.0,W97-0318,"Aspectual classification is a necessary component for a system that analyzes temporal constraints, or performs lexical choice and tense selection in machine translation (Moens and Steedman, 1988; Passonneau, 1988; Dorr, 1992; Klavans, 1994).",http://www.aclweb.org/anthology/W/W97/W97-0318.pdf,"(Moens and Steedman, 1988","Researchers have used empirical analysis of corpora to develop linguistically-based numerical indicators that aid in aspectual classification (Klavans and Chodorow, 1992; Siegel and McKeown, 1996).","Furthermore, stativity is the first of three fundamental temporal distinctions that compose the aspectual class of a clause.",Neut
1157204430,true,golden,3,"",Neut,0.6718,W11-2507,The categorical distributional compositional model of meaning of Coecke et al (2010) combines the modularity of formal semantic models with the empirical nature of vector space models of lexical semantics.,http://www.aclweb.org/anthology/W/W11/W11-2507.pdf,(2010),"The meaning of a sentence is defined to be the application of its grammatical structure- represented in a type-logical model-to the kronecker product of the meanings of its words, as computed in a distributional model.",1 Background,Neut
1157209855,true,golden,11,"",Neut,0.901,N12-1038,"Georgescul et al (2009) note that while WindowDiff technically penalizes false positives and false negatives equally, false positives are in fact more likely; a false positive error occurs anywhere were there are more computed boundaries than boundaries in the reference, while a false negative error can only occur when a boundary is missed.",http://www.aclweb.org/anthology/N/N12/N12-1038.pdf,(2009),"Consider figure 1, only 3 of the 8 windows contain a boundary; only those 3 windows may have false negatives (a missed boundary), while all other windows may contain false positives (too many boundaries).","Tasks requiring a uniform theme in a segment might tolerate false positives, while tasks requiring complete ideas or complete themes might accept false negatives.",Neut
1157216188,true,golden,9,"",CoCo,0.8952,D10-1091,"The approach described in (Auli et al., 2009) is very similar to ours: in this study, the authors propose to find and analyze the limits of machine translation systems by studying the reference reachability.",http://www.aclweb.org/anthology/D/D10/D10-1091.pdf,"(Auli et al., 2009)",A reference is reachable for a given system if it can be exactly generated by this system.,"To the best of our knowledge, there are only a few works that try to study the expressive power of phrase-based machine translation systems or to provide tools for analyzing potential causes of failure.","Neut
CoCo"
1157218394,true,golden,5,"",Pos,0.8157,Q14-1008,"We also added stress information to the BrentBernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent, 1999), following the procedure just outlined.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,"(Bernstein-Ratner, 1987",This corpus is a de-facto standard for evaluat,"As function words account for roughly 50% of the tokens but only roughly 5% of the types in our corpora, this means that the type and token distribution of stress patterns differs dramatically in all our corpora, as can be seen from Table 2.",Pos
1157219254,true,golden,6,"",Pos,1.0,W13-1708,"We used the StackingC implementation of WEKA (Seewald, 2002) to combine the models, with a linear regression model as our meta classifier.",http://www.aclweb.org/anthology/W/W13/W13-1708.pdf,"(Seewald, 2002)",Table 4 shows the classification accuracies for the individual classifiers as well as the ensemble on a 10-fold CV of the training set and on the held out test set.,"We trained three classification models on the entire feature set, using the same train-test sets as explained before and trained an ensemble model with three classifiers.",Pos
1157219275,true,golden,13,"",Pos,0.7746,W09-3953,"We used a version of the corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006).",http://www.aclweb.org/anthology/W/W09/W09-3953.pdf,"(Yeh and Harnly, 2006)",The annotator of the majority of the Loqui corpus also annotated the Enron corpus.,"Most of the emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely social emails.",Pos
1157219324,true,golden,5,"",Neut,1.0,W14-3611,"(Milne et al., 2007) proposed a system called ""KORU"" for query expansion using Wikipedia's most relevant articles to user's query.",http://www.aclweb.org/anthology/W/W14/W14-3611.pdf,"(Milne et al., 2007)",The system allows the user to refine the set of Wikipedia pages to be used for expansion.,Synonyms from WordNet are used to expand the question in order to extract the most semantically relevant passages to the question.,Neut
1157219361,true,golden,8,"",Neut,0.6498,P13-1028,It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al (2008).,http://www.aclweb.org/anthology/P/P13/P13-1028.pdf,(2004),Headden III et al (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing.,Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years.,"Pos
Neut"
1157219569,true,golden,5,"",Pos,1.0,W05-0623,"We find the exact top N consistent' most likely local model labelings using a simple dynamic program described in (Toutanova et al., 2005).",http://www.aclweb.org/anthology/W/W05/W05-0623.pdf,"(Toutanova et al., 2005)",'A labeling is consistent if satisfies the constraint that argument phrases do not overlap.,The model is trained to re-rank a set of N likely labelings according to the local model.,Pos
1157219609,true,golden,8,"",Neut,0.7589,N03-2033,"Information Quality Dimensions (Source: Strong, Lee, Wang, 1997, p.39)",http://www.aclweb.org/anthology/N/N03/N03-2033.pdf,"Strong, Lee, Wang, 1997, ",Empirical attempts to assess quality have primarily focused on counting hyperlinks in a networked environment.,Table 1.,Neut
1157219654,true,golden,7,"",CoCo,0.8759,P15-1061,These features are similar to the position features proposed by Collobert et al (2011) for the Semantic Role Labeling task.,http://www.aclweb.org/anthology/P/P15/P15-1061.pdf,(2011),In this work we also experiment with the word position embeddings (WPE) proposed by Zeng et al (2014).,Zeng et al (2014) propose the use of word position embeddings (position features) which help the CNN by keeping track of how close words are to the target nouns.,"Neut
CoCo"
1157219706,true,golden,4,"",Neut,1.0,P10-2063,"The cross-product of the stem name and (open-class) reduced core POS tags, plus the CLOSED tag, yields 24 labels for a CRF classifier in Mallet (McCallum, 2002).",http://www.aclweb.org/anthology/P/P10/P10-2063.pdf,"(McCallum, 2002)",4 Experiments and Evaluation,"The classifier is used only to get solutions for the openclass words, although we wish to give the classifier all the words for the sentence.",Neut
1157219733,true,golden,6,"",Neut,0.843,N04-4019,1 shows a sample Speech Graffiti dialog User interactions with Speech Graffiti (independent of other speech interfaces) have previously been assessed in Rosenfeld et al (2000).,http://www.aclweb.org/anthology/N/N04/N04-4019.pdf,(2000),"Here we consider a head-to-head comparison: given the chance to interact with both types of interfaces, which would people choose?",Fig.,Neut
1157220089,true,golden,7,"",Neut,0.594,P10-2063,"In contrast, other approaches have used a pipelined approach, with separate models to first do tokenization and then part-of-speech tagging (Diab et al., 2007; Diab, 2009).",http://www.aclweb.org/anthology/P/P10/P10-2063.pdf,"(Diab et al., 2007","While these approaches have somewhat lower performance than the joint approach, they have the advantage that they do not rely on the presence of a full-blown morphological analyzer, which may not always be available or appropriate as the data shifts to different genres or Arabic dialects.","1SAMA-v3.1 is an updated version of BAMA, with many significant differences in analysis.","Neut
CoCo"
1157220155,true,golden,7,"",CoCo,0.7177,D09-1160,"This result conforms to the results reported in (Kudo and Matsumoto, 2003).",http://www.aclweb.org/anthology/D/D09/D09-1160.pdf,"(Kudo and Matsumoto, 2003)","The parsing speed reached 14,937 sentences per second with accuracy of 90.91% (SVM-HKE, d = 3, Q = 0.002).","Without fstries, little speed-up of SVM-HKE classifiers versus the SVM-KE classifiers (in Table 3) was obtained due to the mild reduction in the average number of active features |xd |in the classification.",CoCo
1157220936,true,golden,5,"",Neut,0.4066,W15-2518,"Second, Irvine et al (2013) have shown that including relevant training data in a mixture modeling approach solves many coverage errors, but also introduces substantial amounts of new scoring errors.",http://www.aclweb.org/anthology/W/W15/W15-2518.pdf,(2013),"With phrase-pair weighting we aim to optimize phrase translation selection while keeping our training data fixed, and we can thus compare the impact of several methodological variants on genre adaptation for SMT.","The reason we choose an instanceweighting method rather than a mixture modeling approach is twofold: First, mixture modeling approaches intrinsically depend on subcorpus boundaries, which resemble provenance or require manual labeling.","Pos
Neut"
1157220991,true,golden,5,"",CoCo,0.6096,W04-0839,"(Mohammad and Pedersen, 2004) show results to the same conclusions for SENSEVAL-1 and SENSEVAL-2 data that have similar low training data per task, while, the line, hard, serve and interest data which have much larger training data per task are shown to benefit from a larger context.",http://www.aclweb.org/anthology/W/W04/W04-0839.pdf,"(Mohammad and Pedersen, 2004)",Duluth-ELSS (a sister system of SyntaLex) achieves an accuracy of 61.7%.,"We believe this is due to the low training data per task ratio, which usually means that the weak indicators (P and P ) are likely to be overwhelmed by idiosyncrasies of the data.",CoCo
1157221009,true,golden,3,"",Pos,1.0,E06-1005,"We use the IBM Model 1 (Brown et al., 1993) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996)) to estimate the alignment model.",http://www.aclweb.org/anthology/E/E06/E06-1005.pdf,"(Brown et al., 1993)",The lexicon probability of a sentence pair is modelled as a product of single-word based probabilities of the aligned words.,"As in statistical machine translation, we make modelling assumptions.",Pos
1157221024,true,golden,7,"",Pos,0.5846,D15-1154,"A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high.",http://www.aclweb.org/anthology/D/D15/D15-1154.pdf,"(McDonald, 2006)","On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004).","However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a).","Pos
Neut"
1157221059,true,golden,4,"",Weak,0.7636,N07-1064,"Elming (2006) suggests using tranformation-based learning to automatically acquire error-correcting rules from such data; however, the proposed method only applies to lexical choice errors.",http://www.aclweb.org/anthology/N/N07/N07-1064.pdf,(2006),"Knight and Chander (1994) also argue in favor of using a separate APE module, which is then portable across multiple MT systems and language pairs, and suggest that the post-editing task could be performed using statistical machine translation techniques.","Allen and Hogan (2000) sketch the outline of such an automated post-editing (APE) system, which would automatically learn post-editing rules from a tri-parallel corpus of source, raw MT and post-edited text.","Weak
Neut"
1157221080,true,golden,3,"",CoCo,1.0,W10-3805,"We show that the performance of our approach (using simple lexical features) is comparable to that of the state-of-art statistical MT system (Koehn et al., 2007).",http://www.aclweb.org/anthology/W/W10/W10-3805.pdf,"(Koehn et al., 2007)","When additional syntactic features (POS tags in this paper) are used, there is a boost in the performance which is likely to improve when richer syntactic features are incorporated in the model.",We used maximum entropy model in our experiments.,"Pos
CoCo"
1157221159,true,golden,5,"",Neut,1.0,Q14-1008,"More recently, Lignos (2010, 2011, 2012) further explored Yang's original algorithm, taking into account that function words should not be assumed to possess lexical stress cues.",http://www.aclweb.org/anthology/Q/Q14/Q14-1008.pdf,"(2010, 2011, 2012)","While his scores are in line with those reported by Yang, the importance of stress for this learner were more modest, providing a gain of around 2.5% (Lignos, 2011).","While the USC has been argued to be near-to-universal and follows from the ""culminative function of stress"" (Fromkin, 2001; Cutler, 2005), the high score Yang reported crucially depends on every word token carrying stress, including function words.","Neut
CoCo"
